http://kafka.apache.org/documentation.html

GENERAL
-Kafka maintains feeds of messages in categories called Topics
-Processes that publish messages to a kafka topic are called Producers
-Processes that subscribe to topics are called Consumers

So,
Producer ---> [kafka cluster(topic)] ---> Consumer

A "client" refers to either a producer or a consumer. There's lots of
clients out there, using various languages https://cwiki.apache.org/confluence/display/KAFKA/Clients

BROKERS
-A broker is an instance of kafka. It uses zookeeper to know about other brokers in its cluster.
-Each broker can be queried to find out information about the cluster, such as describing a topic, finding out your replication factor, which nodes are leaders for which partitions, etc.
-There is a "Controller" broker in each cluster which handles partition leader elections, updating zookeeper, etc
-Read more in the "CLUSTERING" section

TOPICS
-A topic contains one or more partitions. Partitioning data allows for dataset sizes larger than a single server.
-Ordering guarantees are provided on a per-partition basis. When a topic is split into partitions, the only guarantee on ordering is within the partitions themselves.
--Total ordering guarantees (within kafka, without requiring external client logic) can only be accomplished by using topics with a single partition, and a transactional publishing model
-You can either create a topic (and define the number of partitions you want) with kafka-topics.sh
or some other script, or alternatively you can configure your brokers to auto-create topics when a non-existent topic is published to
-Each published message is added to the end of a partition and assigned a sequential id called the offset (unique)
-The kafka cluster retains all published messages for a configurable period of time (or size limit). Consuming a message does not remove it
-A Consumer can consume any (offset)message it likes, in any order
--Example script: > bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 \
partitions 1 --topic test
--bin/kafka-topics.sh --list --zookeeper localhost:2181
--Note that replication-factor 1 means the data only exists on 1 server (ie: it is not replicated)

PARTITIONS
-Number of partitions is defined upon topic creation, as is number of replicas
-partitions are distributed across servers in a kafka cluster. Kafka does this itself upon topic creation
-You can run bin/kafka-create-topic.sh on one of your brokers, specifying number of replicas and number of partitions. Alternatively, kafka can auto-create topics when a non-existant topic is published to. In this case, it uses defaults for numreplicas and numpartitions.
-Partitions are represented as a set of log files on disk. You can configure how big you want your log files to be before new writes go to a new log.
--When you define a maximum topic size or maximum age, these values apply to your partitions, not the topic as a whole. Once a partition log file hits a certain age, or the maximum partition size is reached (size of all log files added together), the oldest log is deleted from disk.
-Each partition can be replicated across a configurable number of servers. You end up with one "leader" for that partition, and zero or more "followers." If a leader dies, a follower picks up.
-Each server acts as a leader for some of its partitions and a follower for others
-The controller detects broker failures and elects a new leader for each affected partition. The detection is done via an internal broker RPC, not via zookeeper. The controller will however update zookeeper after it has chosen a new leader for an affected partition.
-Read rollout-notes for an idea of how many partitions you should choose for your topics

PRODUCERS
-Producers publish data to the topics of their choice. They query kafka brokers to get metadata about what topics, partitions, and replicas are available. (future producers: just point them at two or three kafka's to grab initial bootstrap data, they'll use that data to get the rest of the brokers in a cluster)
-Kafka comes with an example producer, kafka-console-producer.sh, which accepts stdin(per-line), or raw input (run it, then type stuff in)
-The producer is responsible for choosing which message to assign to which partition within the topic. This can be done round robin, or according to some function (like a hash on a key in the msg)
-The producer can query kafka to see if it has an existing topic, or it can choose to create its own topic. It can pass how many partitions, replicas etc it wants for this new topic, or let kafka use its defaults.
-When writing to Kafka, producers can choose whether they wait for the message to be acknowledged by 0,1 “all” (-1) *in-sync* replicas, or x number of replicas. Note that -1 means all *current in-sync replicas*...if a replica is currently out of sync, publishes with acks=-1 will still succeed. If this is not wanted, you may specify the minimum number of ISR's you'd like a response from (eg:acks=2) before the write is successful. Keep in mind that this will halt all writes if your acks number is higher than the available brokers.

CONSUMERS
-kafka-console-consumer.sh comes with kafka. It'll dump messages to stdout
--> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
-Consumers label themselves with a consumer group name
-Consumers read data from topics. Kafka maintains a consumer group list, and assigns a partition in a topic per consumer in a consumer group.
-Each partition in a topic is delivered to *one* consumer instance (thread) within each subscribing consumer group. 
--This allows for multiple consumers in a consumer group to process a topic in parallel, because only one of them is consuming each partition.
--This means there should not be more consumer instances (in the same consumer group) than there are partitions, extras are left idle
--Multiple consumer groups can subscribe to the same topic. You can put every consumer into its own consumer group if you want.
-One consumer per partition model allows kafka to ensure the ordered delivery of a partition.
--If you need total ordering guarantees (ie: the entire topic, not just a partition in the topic), you have to make a topic with a single partition (and thereby a single consumer)...OR some sort of ordering inside your data itself, which something can re-order once all the consumers pull it together.
-As of 0.8.2, kafka provides an API call that allows consumers to publish their checkpoint offset. 
--Internally, the implementation of offset storage is just a compacted kafka topic (__consumer_offsets) keyed on the consumer's group, topic, and partition
--The offset commit provides acks=-1 durability
--Kafka maintains an in-mem view of this tuple: <consumer group, topic, partition>, so offset fetch reqs can happen very quickly without the need for scanning the compacted __consumer_offsets topic
--Previously, it was the consumers responsibility to push its current offset location to zookeeper

GUARANTEES
-Messages sent to kafka by one producer to a particular topic partition will be appended in the order they are sent, ie: the first sent message will have a lower offset in the log than the second sent message. This is a guarantee built into your producer.
-A consumer sees messages in the order they are stored in the log
-For a topic set to replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log. Eg: replication factor 3 can withstand 2 server failures.

CLUSTERING
-kafka nodes find out about each other via zookeeper (zookeeper.connect= in your server.properties). By default, all kafka nodes who are connected to the same zookeeper cluster will be part of the same cluster.
--To set up multiple kafka clusters connecting to the same zookeeper cluster, you must define a 'chroot' address on your zookeeper.connect line. eg:
zookeeper.connect=hostname1:port1,hostname2:port2,hostname3:port3/chroot/path
--Note that you must create this chroot path yourself prior to starting up your kafka cluster
--this will put that kafka's nodes/leaves under its own path, thereby separating your cluster out from the main path.
--If you are running chroots as above (*and a kafka version less than 0.8.2), you also have to configure your consumers to use the appropriate chroot path. In consumer.properties it's the same zookeeper.connect line.
-After zookeeper discovery, each kafka cluster designates a "controller". It will be responsible for:
--Leadership change of a partition (though each leader can still independently update the ISR list)
--New topics; deleted topics
--Replica re-assignment
--https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Internals
-Broker failure detection is done via direct RPC, internal kafka functions - not zookeeper
-Controller death could be detected by controller.socket.timeout.ms configured on each broker
-Communication between the controller and the rest of the brokers is done through RPC
-The controller is the one that commits changes (new topics, leadership changes etc.) to zookeeper

COMPACTION
http://kafka.apache.org/documentation.html#compaction
-You have the option to either compact or delete log segments once you hit either a certain time limit, or size limit. The idea is to selectively remove records where we have a more recent update with the same primary key.
-can be set per topic at creation, or using alter topic. Default log.cleanup.policy is delete.
-Let's say you're pushing messages to kafka that look like this "uid123 herp=derp", "uid123 herp=lerp", "uid123..." etc etc. You're updating "the same value" over time, perhaps it's a user updating his/her new email address or info. Log compaction will look at a unique key in your logs(messages) and then remove all but the latest update related to that key.
-Compaction allows consumers who are unable to keep up with the log (or crashed) to see the last value a key was set to
-You can set your key's value to 0 to mark that key for deletion (happens 24hrs later by default)
-compaction is not compatable with compressed topics
-your message offset never changes. Some get deleted, the newest stays, and the offset stays the same
-Take note of log.cleaner. options such that performance is not an issue
-Be aware that your topic is going to continue growing until you intervene, so be aware of how many primary keys you have and your primary key growth

API DEETS
Producer-API
The Producer API wraps two low level producers. These are kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer. The goal is to expose all the producer functionality through a single API to the client.
The API provides the ability to batch multiple produce requests (when procer.type=async). As events enter a queue, they are buffered until queue.time or batch.size is reached. Then a background thread kafka.producer.async.ProducerSendThread dequeues the batch of data and lets kafka.producer.EventHandler serialize and send the data to the appropriate broker partition. Monitoring/stats can be done by implementing kafka.producer.async.CallbackHandler to inject callbacks at various stages.
The API also provides software load-balancing between partitions via an optionally user-specified kafka.producer.Partitioner. Otherwise, you can provide a key. If you do not include a key, kafka will assign the message to a random partition. You may also define your own partition.scheme to key off and direct certain data to certain partitions.
Consumer-API
There's two levels. The "simple" API maintains a connection to a single broker and has a close correspondance with the network requests sent to the server. It's stateless and has the offset passed in every request, which allows the user to maintain metadata however they choose.
The "high-level" API hides the details of brokers from the consumer. It allows a consumer to consume off a cluster of machines without concern of the underlying topology. It also maintains state of what has been consumed. It also allows you to subscribe to (or ignore) topics based on a regex.
-SimpleConsumer: Allows you to read a message multiple times
-SimpleConsumer: Allows you to consume only a subset of partitions in a topic in a process
-SimpleConsumer: Allows you fine grained transaction control, allows making sure a message is processed 'just once'
-SimpleConsumer: You must keep track of your own offset to know where you left off consuming
-SimpleConsumer: You must figure out which broker is lead broker for a topic and partition
-SimpleConsumer: You must handle broker leader changes
-HighLevelConsumer: Don't care about handling message offsets. Stores last offset read from a specific partition in Zookeeper, stored under a Consumer Group name
-HighLevelConsumer: Kafka doles out partition assignments per thread connected to it under a certain consumer group. Your High Level Consumer should ideally have as many threads as there are partitions+replications
-HighLevelConsumer: If you have less threads than there are partitions, there will be no guarantee of ordering aside from sequential offset number

CONSUMER OFFSETS
-With 0.8.2 (0.8.3 really) comes the ability to do broker-committed offsets. This means that the consumer api has the option to commit your offsets to a topic called __consumer_offsets on your brokers, rather than having to use zookeeper. There's a bunch of new config options for this - check the docs for offsets.*
-The topic is created as soon as the first consumer using the new API commits an offset to kafka. This topic is created with 50 partitions, repl factor 3, and log compaction on by default
-Commits to __consumer_offsets by the consumer API use groupID, Topic, and PartitionID to key off of in order to determine which partition of __consumer_offsets to save the commit message to.
-The default offsets.topic.num.partitions=50 seems quite high, and 100-200 partitions are "recommended" for production. 
--Keying offset commits across many partitions may mildly improve performance of consumers looking up offset values due to smaller partition sizes, but I'm not sure this justifies the extra 5ms-per-partition, 2ms-per-partition leader initialization time, and replica thread / other used resources. 
-This feature may be buggy. We're currently seeing a random, some brokers but not others, 3300% idle increase in CPU utilization with __consumer_offsets @ 50 partitions and repl 3 on a 3 node cluster. Not sure yet how it's related

MESSAGES
-Messages consist of a fixed-size header and a variable-length opaque byte array payload. The header contains a format version, and a CRC32 checksum to detect corruption or truncation. Put whatever you want in the payload, kafka doesn't care.
-Header has:
  1 byte magic identifier to allow format changes
  {{ONLY if magic byte set}} 1 byte "attributes" identifier (eg: compression enabled, type of codec used)
  4 byte CRC32
  N byte payload

LOG
-A topic like 3p2r (3 partitions, repl factor 2) is going to have three directories on disk - 3p2r_0 3p2r_1 3p2r_2. If you've got three brokers, one partition leader may have 3p2r_0 while another will have _1. A replication partner may also have a 3p2r_0 directory even though it's not the leader for it, because it's a replication partner.
-Two files are saved. An .index file, and a .log file. The .index file contains 64-bit integer offsets giving the byte position of the start of each message in a .log file. It's a mapping of offset to physical location. The .log file holds the actual data.
-The format of the log files saved to disk is a sequence of log entries. The format for a log entry is the same as above, but has a message length integer:
  4 byte message length integer (1+4+n)
  1 byte "magic" value
  4 byte crc
  n byte payload
::Writes
-A log file will grow to a certain size (default 1GB), and then begin writing to separate .index/.log files. Each log file is named with the offset counter of the first message it contains. So, if you're saving 64KB messages and rotating at 1GB, that's around 15625 messages per. It takes a bit for kafka to notice and rotate, so typically you're going to have log files a bit larger than 1GB and having more than your expected # of messages per log file. You may end up with 00000000000000000000.index and then 00000000000000016767.index with 64KB files @ 1GB rotation. So, your first .log file has 16766 messages in it.
::Reads
-Give the offset (ie:counter) and the max chunk size (intended to be larger than any single message could be, but in the event of an abnormally large message, retry the read doubling the buffer size each try until the message is read successfully).

BIG SLOW DISK IS GOOD
http://kafka.apache.org/documentation.html#persistence
-It's common perception that disks are slow and should be avoided. However, because of kafka's sequential nature, it can take great advantage of the operating systems disk read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes.
-Additionally, building on top of JVM means having to use java's ... memory management. The memory overhead of objects is very high, often doubling the size of the data stored (or worse). Additionally, Java garbage collection becomes increasingly fiddly and slow as the in-heap data increases.
-So what to do? Write the stuff out to a file on disk immediately. It just gets written to your pagecache, which is going to take up way less space than in mem java. Data is also stored as a compact bytes structure rather than individual objects. Also, the cache stays warm across process reboots rather than being cleared. And when data needs to be retrieved, you get a direct pointer to the data in memory. You also end up not having to worry about "in memory" vs "on-disk" information, now it's the OS's job.

NOTEABLE CONFIGURATION NOTES
http://kafka.apache.org/documentation.html#configuration
-message.max.bytes (1000000) : max size of a message the broker can receive from a producer. Synch this with the max fetch size your consumers use. You don't want messages too large for your consumers. Also keep in mind possible batched+compressed MessageSets, which may be many times the size of a single normal message!
-num.io.threads (8) : set this to as many threads as you have disks
-queued.max.requests (500) : number of requests that can pile up waiting for disk I/O before the network threads stop reading in new requests
-socket.send.buffer.bytes : socket.receive.buffer.byes (100*1024) : SO_SNDBUFF and SO_RCVBUFF respectively
-log.retention.{minutes,hours} (7d) : retention period. If both this and log.retention.bytes are set, it will do both. Whichever is hit first.
-log.retention.bytes (-1) : how big a single log (ie: partition) can grow before a log segment is deleted/compacted (topics are split into partitions (aka:logs), and logs are split into segments (default 1gig segments). Deletions happen to entire segments at once.
-log.flush.interval.messages (None) : sets your fsync time before messages in memory are sync'd to disk. It is strongly recommended to rely on replication rather than setting an fsync time, as this has a huge performance impact.
-default.replication.factor (1)
-replica.lag.time.max.ms (10000) : If a follower hasn't sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.
-replica.lag.max.messages (4000) : If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.
replica.fetch.max.bytes (1024*1024) : how many bytes (per partition) to try to grab in one fetch to the leader
-num.replica.fetchers (1) : number of replication threads to the leader. Increasing this can increase the degree of I/O parallelism in the follower broker
-controlled.shutdown.enable (false) : This makes the broker attempt to transfer partition leadership to other brokers prior to shutting down. Speeds up unavailability window during shutdown. *WARN* Ensure your init script properly waits and terminates all child java processes prior to returning success, else restart will fail */WARN*
-auto.leader.rebalance.enable (true) : If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the "preferred" replica for each partition if it is available.
--leader.imbalance.per.broker.percentage (10) : The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.


DEPLOYMENT NOTES
-Upon node failure, kafka doesn't know whether it'll be permanent or not. Maybe you want to replace it with a new broker, or make another existing broker part of the replication pool for that topic. The preferred method upon broker failure is to bring up a new broker (if it's a total replacement) with the same broker.id as your failed box. This way when you start it up it'll join, notice that it's way off, and start automatically replicating/recovering.
--Other cluster resizing or rebalancing operations must be done manually, using bin/kafka-preferred-replica-election.sh or kafcat (not kafkacat, kafkat. Easier to use than that stupid script)
-kafka includes a bin/kafka-{producer,consumer}-perf-test.sh. This client utilizes batch processing, and can run multiple threads. It automatically counts for you, can push a specified # of messages, etc. Using this, we can easily max a gigabit link on each node of a 3-node kafka cluster using 64KB message sizes. We can also get around 1.2million 100byte publishes/sec out of this hardware from 3 2008 crappy blades.
-The java producer that comes with kafka has a compression.codec option. Enabling compression will cause the producer to buffer up a set of messages, wrap them in an eventMsg, compress that whole eventMsg, then send that message as a whole to your broker. This message's header will have its magic byte set along with a flag saying that this message is compressed. On the other side, the java kafka consumer has an iterator that can automatically detect these compressed eventMsgs, decompress the eventMsg, and then spit out each individual message.
-There's a nice little statically linked C client called kafkacat that is extremely useful for quick producing/consuming/troubleshooting. It accepts input from stdin if acting as a producer, or outputs to stdout if acting as a consumer. It also supports compression, and will automatically decompress MessageSets when consuming
--~210k messages/sec from a printf loop over 100mbit switch. Seeing ~12-15mbit/sec on each broker.


:::::::STUFF TO WATCH::::::::
This file describes a list of kafka related tools and such to keep an eye on.


---PRODUCERS/CONSUMERS---
https://github.com/edenhill/kafkacat :: 'netcat' for kafka. Lightweight statically-linked producer/consumer client. Takes stdin and consumes to stdout. Extremely simple, versatile, I use it almost exclusively.
https://github.com/travel-intelligence/flasfka :: Expose kafka over python's simple web framework flask(http). Pretty neat-o. Limitations: utf-8 input only. Encode to base64 if you need to push arbitrary data to it. We use this for twittershitter
https://github.com/tagged/bruce :: C client that sits on a host and listens to a unix dgram socket. Your app then only has to send info in a simple binary format to a /dev/kafka_bruce. Saves devs from having to implement kafka API client code in their codebase.
https://github.com/stealthly/go_kafka_client/tree/master/syslog :: syslog->kafka producer, per SRD-29. Works fine
http://www.rsyslog.com/doc/master/configuration/modules/omkafka.html :: omkafka, builtin module for rsyslog to publish to kafka. Likely our best bet for syslog->kafka

Choice of a client API will ultimately be up to our devs:
https://cwiki.apache.org/confluence/display/KAFKA/Clients


---MONITORING RELATED---
https://github.com/jmxtrans/jmxtrans :: Crappy JMX monitoring tool, but has built in writer for openTSDB and many others. Will periodically poll for stats from mbeans exposed via JMX
(collectd)
https://github.com/m-szalik/tjconsole :: Text jconsole! Discover what metrics are available to you over your JMX interface.
https://github.com/dropwizard/metrics :: The big main java metrics monitoring suite. Kafka uses this package to expose its stats
https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jmx.html :: Logstash has a jmx poller. This is much preferable to jmxtrans so long as it is able to scale
https://github.com/claudemamo/kafka-web-console	:: Web interface for monitoring kafka. Might be worth a look.
https://github.com/shunfei/DCMonitor :: Web interface for monitoring kafka. Might be worth a look.

---ADMIN TOOLS---
https://github.com/airbnb/kafkat :: simplified command line administration for kafka. Anything is better than those shit awful json-script-using kafka builtin tools...so check this out


~~GRAPHITE~~
These show uncompressed data coming into the journalers in each site:
https://graphite.rp-core.com/render/?width=1600&height=1066&areaMode=stacked&title=Journaller%20Multicast%20Bytes%20Read%20(Uncompressed)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*ams2*.*.Uncompressed_Recorded_Bytes))%2C%22AMS2%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*fra1*.*.Uncompressed_Recorded_Bytes))%2C%22FRA1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*iad1*.*.Uncompressed_Recorded_Bytes))%2C%22IAD1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*sjc1*.*.Uncompressed_Recorded_Bytes))%2C%22SJC1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*hkg1*.*.Uncompressed_Recorded_Bytes))%2C%22HKG1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*nrt1*.*.Uncompressed_Recorded_Bytes))%2C%22NRT1%22)%2C%22last%22)

https://graphite.rp-core.com/render/?width=1600&height=1066&title=Journaller%20Multicast%20Bytes%20Read%20(Uncompressed)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*ams2*.*.Uncompressed_Recorded_Bytes))%2C%22AMS2%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*fra1*.*.Uncompressed_Recorded_Bytes))%2C%22FRA1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*iad1*.*.Uncompressed_Recorded_Bytes))%2C%22IAD1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*sjc1*.*.Uncompressed_Recorded_Bytes))%2C%22SJC1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*hkg1*.*.Uncompressed_Recorded_Bytes))%2C%22HKG1%22)%2C%22last%22)&target=legendValue(alias(integral(sumSeries(metrics.Data-Pipeline.prod.Collection.Journaller.*nrt1*.*.Uncompressed_Recorded_Bytes))%2C%22NRT1%22)%2C%22last%22)

Num journalers per site according to DNS (may include .dev+.staging...):
ams2: 18
fra1: 14
iad1: 21
sjc1: 22
hkg1: 12
nrt1: 4

avg hdd size: 500GB

