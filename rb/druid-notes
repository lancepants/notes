http://druid.io/blog/2012/10/24/introducing-druid.html
https://metamarkets.com/what-we-do/technology/
http://druid.io/docs/0.8.1/design/index.html

###
# What is it
"A distributed, real-time datastore"

starfire(ingestion) -> Druid(inmem-datastore) -> Visualization(web-interface)

OLAP - OnLine Analytical Processing
OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing. Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region’s sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints. These viewpoints are sometimes called dimensions (such as looking at the same sales by salesman or by date or by customer or by product or by region, etc.)


###
# Featureset

-Distributed Architecture: swappable read-only data segments using an MVCC swapping protocol. Per-segment replication relieves load on hot segments. Supports both in-memory and memory-mapped versions.
-Real-time ingestion: coupled with broker servers to query across real-time and historical data. Automated migration of real-time to historical as it ages.
-Column-oriented: Data is laid out in columns so that scans are limited to the specific data begin searched ?find out what they mean by this?. Compression is used as well.
-Fast filtering: bitmap indices with CONCISE compression
-Good for Ops: fault tolerance due to replication, supports rolling deployments and restarts, simple scale up or scale down by adding or removing nodes

From a query perspective, Druid supports arbitrary Boolean filters as well as Group By, time series roll-ups, aggregation functions and regular expression searches.

###
# Perf
**2011 CLAIMED**
-33M rows per second per core
-Can ingest up to 10k records per second per node
-They have horizontally scaled driod tp supoort scan speeds of 26B records per second (http://druid.io/blog/2012/01/19/scaling-the-druid-data-store.html)
**NOW CLAIMED**
-"starfire" streaming ingestion processes millions of events per second ?how many boxes?


###
# Condensed Docs
THE DATA
Let's say we have this data:
timestamp             publisher          advertiser  gender  country  click  price
2011-01-01T01:01:35Z  bieberfever.com    google.com  Male    USA      0      0.65
2011-01-01T01:03:63Z  bieberfever.com    google.com  Male    USA      0      0.62
2011-01-01T01:04:51Z  bieberfever.com    google.com  Male    USA      1      0.45
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Female  UK       0      0.87
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK       0      0.99
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK       1      1.53

This data set is composed of three distinct components. If you are acquainted with OLAP terminology, the following concepts should be familiar.

Timestamp column: We treat timestamp separately because all of our queries center around the time axis.
Dimension columns: Dimensions are string attributes of an event, and the columns most commonly used in filtering the data. We have four dimensions in our example data set: publisher, advertiser, gender, and country. They each represent an axis of the data that we’ve chosen to slice across.
Metric columns: Metrics are columns used in aggregations and computations. In our example, the metrics are clicks and price. Metrics are usually numeric values, and computations include operations such as count, sum, and mean. Also known as measures in standard OLAP terminology.

ROLL-UP
Roll-up is a first-level aggregation operation over a selected set of dimensions, equivalent to (in pseudocode):
GROUP BY timestamp, publisher, advertiser, gender, country
  :: impressions = COUNT(1),  clicks = SUM(click),  revenue = SUM(price)

If our queryGranularity (roll-up granularity) is set to 1 hour, then our data turns into this:
timestamp             publisher          advertiser  gender country impressions clicks revenue
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Male   USA     1800        25     15.70
2011-01-01T01:00:00Z  bieberfever.com    google.com  Male   USA     2912        42     29.18
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Male   UK      1953        17     17.31
2011-01-01T02:00:00Z  bieberfever.com    google.com  Male   UK      3194        170    34.01

The lowest supported queryGranularity is 1ms.

We can see how much space we save by rolling up the data. The drawback here is that the more we roll-up, the more granularity we lose.


SHARDING
Druid shards are called segments. Druid always first shards data by time. So from our compacted (rolled-up) dataset, we can create two segments (one for each hour):
Segment sampleData_2011-01-01T01:00:00:00Z_2011-01-01T02:00:00:00Z_v1_0 contains
 2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Male   USA     1800        25     15.70
 2011-01-01T01:00:00Z  bieberfever.com    google.com  Male   USA     2912        42     29.18

Segment sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_0 contains
 2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Male   UK      1953        17     17.31
 2011-01-01T02:00:00Z  bieberfever.com    google.com  Male   UK      3194        170    34.01

Segments are self-contained containers for the time interval of data they hold. Segments contain data stored in compressed column orientations, along with the indexes for those columns. Druid queries only understand how to scan segments.

Segments are uniquely identified by a datasource, interval, version, and an optional partition number. In our example:
sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_0  ==  dataSource_iterval_version_partitionNumber

??partition number? what do they mean by that? where are partitions stored? which node??

INDEXING
Druid creates immutable snapshots of data, stored in data structures which are optimized for analytic queries.
Druid is a column store, which means each individual column is stored separately. Using these immutable snapshots of data, druid is able to pretty reliably only scan what it needs for a query. Different columns can also employ different compression methods. 
Each column has a different index associated with them. Druid indexes data on a per-shard (segment) level. ??Get clarification here. Is it a unique column index per segment? or globally??

LOADING DATA
Druid can either ingest in real-time, or in batch. Real-time is best effort, and exactly-once semantics are not guaranteed (although it's on the roadmap to be supported).
Batch does provide exactly once guarantees, and segments created via batch processing will accurately reflect the ingested data. One common approach to operating Druid is to have a real-time pipeline for recent insights, and a batch pipeline for the accurate copy of the data.

QUERYING
Native is JSON over HTTP. The community has contributed many others, including SQL.
Druid does not support joins, it is designed to perform single table operations. Many setups do joins at ETL because data must be denormalized(redundant data added back, such that now your table has many more columns rather than _id keys pointing to other table.column) before loading into druid.
Side:
  Extract, Transform and Load (ETL) refers to a process in database usage and especially in data warehousing that: Extracts data from homogeneous or heterogeneous data sources. Transforms the data for storing it in proper format or structure for querying and analysis purpose.

THE DRUID CLUSTER
A Druid Cluster is composed of several different types of nodes. Each node is designed to do a small set of things very well.

::Historical Nodes::
These commonly form the backbone of a Druid cluster. Historical nodes download immutable segments locally and serve queries over those segments. The nodes have a shared nothing architecture and know how to load segments, drop segments, and serve queries on segments.
::Broker Nodes::
These are what clients and applications query to get data from Druid. Broker nodes are responsible for scattering queries and gathering and merging results. Broker nodes know what segments live where.
::Coordinator Nodes::
These nodes manage segments on historical nodes in a cluster. Coordinator nodes tell historical nodes to load new segments, drop old segments, and move segments to load balance.
::Real-time Processing::
Real-time processing in Druid can currently be done using standalone realtime nodes or using the indexing service. The real-time logic is common between these two services. Real-time processing involves ingesting data, indexing the data (creating segments), and handing segments off to historical nodes. Data is queryable as soon as it is ingested by the realtime processing logic. The hand-off process is also lossless; data remains queryable throughout the entire process.

??Explain data's path through druid - which nodes it hits first, what the nodes do with that data, etc??

EXTERNAL DEPS
Zookeeper - druid relies on zookeeper for intra-cluster communication
Metadata store - Typically MySQL or PostgreSQL. Druid services that create segments write new entires to the metadata store. The coordinator nodes monitor the metadata store to know when new data needs to be loaded or old data needs to be dropped. The metadata store is not involved in the query path.
Deep Storage - Typically S3 or HDFS. Deep storage acts as a permanent backup of segments. Services that create segments will upload them to deep storage, and historical nodes download segments from deep storage. It's also not involved with the query path.

HA
Run at least two nodes of each type. There's no quorum or cluster leaders, just dependencies on those external services for metadata info, and zookeeper for communication between services.



###
# More Specific
REAL TIME NODES
-Events indexed via these nodes are immediately available for querying. The nodes are only concerned with events for some small time range, and periodically hand off immutable batches of events to other druid nodes in the cluster.
-Real-time nodes use zookeeper to coordinate with the rest of the druid cluster. The nodes announce their online state and the data they serve in zookeeper.
-They maintain an in-memory index buffer for all incoming events. The index is incrementally populated as events are ingested.
-This index exists as a row-store, in JVM's memory. In order to avoid heap overflow problems, real-time nodes persist their in-mem indexes to disk either periodically or after a configured maximum row limit is reached.
-The persist to disk process converts the in-memory row store to a column-oriented storage format on disk. Real time nodes will load persisted indexes into off-heap memory such that they can still be queried.
-Periodically, each real-time node will schedule a background task that searches for all locally persisted indexes. The task then merges these indexes together based on a span of time and creates an immutable block of data. This immutable block is called a segment.
-During the handoff stage (??when is this??), the RTN will upload this segment to a permanent backup storage (typically S3 or HDFS), which druid refers to as "deep storage." 

