Algorithms
==========

.. _algorithms:

Big O Notation
--------------
https://justin.abrah.ms/computer-science/big-o-notation-explained.html

O(n), O(n^2), and O(1)
^^^^^^^^^^^^^^^^^^^^^^

.. image:: media/algorithms-bigograph.png
   :alt: Figure 1: Comparison of O(n^2) vs O(n) vs O(1) functions
   :align: center

   **Figure 1: Comparison of O(n^2) vs O(n) vs O(1) functions**

A function(algorithm)'s Big-O notation is determined by how it responds to different inputs. How much slower is it if we give it a list of 1000 things to work on instead of a list of 1 thing?

Let's give an example:

    def is_myitem_in_list(myitem, the_list):
      for item in the_list:
        if myitem == item:
	  return True
      return False

So if we call this function like is_myitem_in_list(1, [1,2,3]), we loop over our list looking for 1. Should be pretty fast. If we change to is_myitem_in_list("potato", [1,2,3]) then we have our worst possible runtime - it has to go through all list values before it returns False. 

From the looks of it, in the worst case scenario, the for loop will run a maximum of len(the_list) times. Since Big-O notation measures the *worst cast* run time of an algorithm (function), the Big-O notation of this function is *O(n)*, roughly meaning that the number of inputs has a linear relationship with how long it's going to take to run. If you graphed it out where x=num_inputs and y=time_taken, you'd get a nice linear graph. The assumption here is that every item in your input list takes the same amount of time to process.

So what about this function:

    def is_none(item):
      return item is None

This is a bit contrived...but it serves as a good example of an O(1) function, also called *constant time*. What it means is no matter how big our input, it always takes the same amount of time to compute things. You could pass it a million integers and it will take the same amount of time to process as if you passed a single integer. Constant time is the best case scenario for a function.

Another example:

    def all_combinations(the_list):
      results = []
      for item in the_list:
        for item_again in the_list:
	  results.append((item, item_again))
      return results

This matches every item in the list with every other item in the list. For example, if we passed in [1,2,3], we'd get back [(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)]. You could use this idea to brute force a PIN number. This is part of the field of combinatorics. The above algorithm is considered O(n^2) because for every item in the list (aka **n** for input size), we have to do **n** more operations. So **n * n == n^2**.

Referring back to Figure 1, as we add input items, we can see quadratic growth.

O(log n)
^^^^^^^^
O(log n) basically means that time goes up linearly while the **n** goes up exponentially. So if it takes **1** second to compute **10** elements, it will take **2** seconds to compute **100** elements, **3** seconds to compute **1000** elements, and so on. Remember again that big-O is stating *worst case*, so it's more accurate to say that the running time of an O(log n) (or any other big-O) grows *at most* proportional to "log n".

Divide-and-conquer type algorithms are typically O(log n) - for example binary search and part of quick sort. More detail about these algorithms later, but a quick example of an O(log n) operation would be looking up someone in a phone book. You first open up the middle of the book, and since it is sorted alphabetically, you can immediately discard half the book from your search since you may be at letter "K" and the person you are looking up has a last name starting with the letter "R". You've just narrowed your search to the final half of the book. You then open halfway the remaining half of the book, and continue doing this until you get to the R's, then the Ri's, then the Ric's, and so on. With each guess, your search range is cut in half (or more, if you are predicting the number of pages between letters). Read about binary search later on this page for a better understanding.

Calculating Big-O
^^^^^^^^^^^^^^^^^
Just follow your code!

  def count_ones(a_list):
    total = 0
    for element in a_list:
      if element == 1:
        total += 1
    return total

- First, we're setting total to 0. We're writing out a chunk of memory, passing in a value and not operating on that value in any way. This is an O(1) operation.
- Next, we're doing a loop. Each item in a_list is done once (worst case). As we add more input values, the time it takes to get through the loop increases linearly. We use a variable to represent the size of the input, which everyone calls **n**. So, the "loop over a list" function is O(n) where **n** represents the size of a_list.
- Next, we check whether an element is equal to 1. This is a binary comparison - it happens once. "element" could be 8, [1,2,3,4,9], or a binary blob, it makes no difference to the comparison, it happens once. This is an O(1) operation.
- Next we add 1 to total. This is the same as setting total to zero, except a read happens first and addition happens. Addition of one, like equality, is constant time. O(1)
- So now we have **O(1)+O(n) * (O(1)+O(1))**. This reduces to 1+n*2, or O(1)+O(2n). In big O, we only care about the biggest "term", where "term" is a portion of the algebraic statement, so we're left with **O(2n)**.
- Since big O only cares about approximation, 2n and n are not fundamentally different - they are simply greater or lesser grades on a linear graph. For example, (1,1),(2,2),(3,3) and (1,2),(2,4),(3,6) is still a linear increase in runtime. Because of this, we can say O(2n) is O(n)
- To sum it up, the answer is that this function has an O(n) runtime (or a linear runtime). It runs slower the more things you give it, but should grow at a predictable rate.

.. _algorithms-sorting:

Sorting Algorithms
------------------

Explanations of common sorting algorithms: https://www.hackerearth.com/notes/sorting-code-monk/

Python's default, timsort, is an implementation which combines quicksort and mergesort. It's O(n) best case and O(n log n) worst. ref: http://corte.si//posts/code/timsort/index.html

Quicksort
^^^^^^^^^
https://en.wikipedia.org/wiki/Quicksort

Quicksort is a comparison sort, meaning that it can sort items of any type so long as there is a "less-than" relationship relationship between items. It may also be considered a "divide and conquer" algorithm. When implemented well, quicksort can be about two or three times faster than merge sort or heapsort. On average, quicksort is an O(n log n), and in the worst case (rare) it is an O(n^2).

The general quicksort steps are:
* Pick an element, called a *pivot*, from the array
* **Partitioning**: reorder the array so that all elements with values less than the pivot point come before the pivot, while all elements with values grater than the pivot come after it (equal values go either way). After this partitioning, the pivot is in its final position. This is called the **partition** operation
* Recursively apply the above steps to the sub-array of elements with smaller values, and separately do the same to the sub-array with the higher values

The pivot selection point and the partitioning steps can be done in several different ways, and which method you choose greatly affects the algorithm's performance.



Searching Algorithms
--------------------

Binary Search
^^^^^^^^^^^^^

.. image:: /media/algorithms-binary-tree.png
   :alt: Depiction of a binary tree with a height of 4
   :align: center

   **A binary tree with a height of 4***

Binary search is a good example of an algorithm which is O(log n). As **n** (number of elements) increases exponentially, time to process only increases linearly.

.. image:: /media/algorithms-ologn-graph.png
   :alt: O(log n) graph plot. The rise of the curve decelerates as n increases** 
   :align: center


Other Comp Sci
--------------
B-tree
^^^^^^

.. image:: /media/algorithms-btree.png
   :alt: A B-tree of order 2 or order 5
   :align: center

- A B-tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in *logarithmic time*
- The B-tree is a generalization of a Binary Search Tree in that a node can have more than two children
- The B-tree is optimized for systems that read and write large blocks of data. As such, they are commonly used in *databases and filesystems*.
Boot Process
============

Legacy Boot
-----------
- Bootstrap loader on MB is loaded. It looks for MBR in first 512bytes. 
- Bootstrap loader starts stage1 bootloader, first sector of disc
- /boot partition sector location is in mbr. stage1 uses the location to launch stage 1.5(if disk is 2TB+) or 2 bootloader
- stage2 (eg:grub) loads the decompressed kernel image and initrd into memory and then invokes the kernel process
- Kernel initializes some hardware and sets up initrd in memory
- init script in initrd sets up /proc, /sys, /dev and other essentials, as well as provides some basic needed tools like insmod
- some basic modules are loaded (such as scsi & sata drivers, ext3 module) and more hardware is initialized and added to /sys
- root is mounted read only
- Kernel runs init
- SYSVINIT....kernel runs rc.sysinit
- rc.sysinit makes sure modprobe (an intelligent wrapper for insmod) and starts udev
- udev and modprobe combine forces to read available modules, stuff in /sys, and use that info to create /dev nodes
- rc.sysinit then goes on to set kernel params, other crap, and then remounts root to rw using info in /proc/mounts. Then mtab is generated, then swap is initialized.
- after init is done with rc.sysinit, it calls inittab
- inittab loads your various services based on your runlevel
- inittab then makes some virtual tty's by running mingetty for each runlevel and asks you for a username
- mingetty passes your username to login. login verifies, then passes to (probably)bash
- bash starts, bootup done.


UEFI
----
In essence, UEFI does away with stage 1 and 1.5 bootloaders.

https://www.happyassassin.net/2014/01/25/uefi-boot-how-does-that-actually-work-then/

tl;dr
^^^^^
- UEFI is firmware. Its config is in NVRAM on your motherboard. It most resembles a stage2 bootloader. Think grub/lilo+menu.cfg built into firmware, with its entries pointed to other "stage 2" bootloaders on various disks/devices.
- In the case of booting a live disk or install media from a storage device, UEFI reads GPT formatted disks. It gets disk GUID and partition GUID, as well as which partitions are marked "0" (system, aka efi FAT system partition) from GPT info at the start or end of the GPT-labelled storage device. It uses this info to look for bootloaders.efi to load.
- OS's run efibootmgr when installing. They generate their own entry in UEFI by taking info from GPT and running efibootmgr -c ...stuff, which generates an entry in your EUFI firmware's config like (diskGUID,partitionGUID)(/efi/grub.efi).
- Use efibootmgr to read/write configs. You can determine what boots first, boot once, blah blah "man efibootmgr"
- UEFI can also detect non storage devices to boot from, such as network cards. It scans ACPI and other paths to find valid resources, based on the EFI_DEVICE_PATH_PROTOCOL section of its spec.
- OS's are now expected to manage themselves and their own bootloaders only, stay out of everyone else's business. In practice, this is not really the case, as UEFI spec is not strict enough.
- Your UEFI firmware can ship with a set of signatures. It can refuse to run any EFI executable which is not signed with one of those signatures. This is what Secure Boot is.

Forward
^^^^^^^

UEFI and BIOS are two types of firmware. Saying a computer has a UEFI BIOS is nonsensical.
As we know, BIOS will look at a disks' MBR for a magic bit and then load the stage1 bootloader. It has no idea what a partition is, or what an OS is - it doesn't care. Stage1 bootloader loads stage2 (or possibly stage1.5), stage2 has the OS list and which partition to load them from, blah blah, normal BIOS boot procedure. UEFI is different...it removes some of the steps in this awkward chain-loading process, and moves a lot of "stage 2" bootloader type functionality into the UEFI firmware itself. Read on.

When you use UEFI "legacy/compatability" boot mode, all UEFI is doing is loading the stage1 bootloader off of a drive's MBR. It's kicking off the old-style BIOS bootloading.

This document describes native UEFI bootloading. It also assumes the use of disks with GPT partition table and EFI FAT32 EFI system partitions (EFI FAT32 ESPs). Crapintosh's may not conform to this assumption, but 99% of other stuff will.

The GPT (GUID partition table) is very much tied into the UEFI spec, and in fact, is necessary to understand in order to know how UEFI figures out a lot of its information. This document includes a foray into GPT structure.

UEFI Can:
-Read a partition table (GPT)
-Access files in some specific filesystems (FAT32, others)
-Execute code in a particular format
-Boot multiple *targets* - not just disks

##
How does UEFI find various possible boot targets? How do I configure boot targets?
##
The UEFI spec defines something called the UEFI boot manager. In Linux, there's a tool called
  efibootmgr
which is used to manipulate the config of the UEFI boot manager. The UEFI Boot Manager is a firmware policy engine that can be configured by modifying global NVRAM variables. The boot manager will attempt to load UEFI drivers and UEFI applications (including UEFI OS bootloaders) in an order defined by these NVRAM variables. In a sense the UEFI Boot Manager is kind of like a stage2 bootloader (ie: grub/lilo menu.cfg). You can add and remove stuff from it, and change the order in which stuff boots. The values though, are saved to NVRAM on your motherboard rather than to an MBR or something on an attached device.

Take a look at your current UEFI settings: 
  efibootmgr -v
    BootCurrent: 0000
    Timeout: 5 seconds
    BootOrder: 0000,0080
    Boot0000* Fedora    HD(4,797c000,64000,60ffa404-df04-48dc-825e-13ae506e0db5)File(\EFI\fedora\shim.efi)
    Boot0004* Hard Drive    BIOS(2,0,00)P0: ST1500DM003-9YN16G
    Boot0080* Mac OS X  HD(1,28,64000,f9a29f55-388b-46e5-b5ba-3afa21d1fa57)File(\EFI\refind\refind_x64.efi)
    Boot0081* Recovery OS   ACPI(a0341d0,0)PCI(1c,5)PCI(0,0)SATA(0,0,0)HD(3,7845e20,135f20,88bc09aa-aafb-4874-be13-ec21dabd7070)File(\com.apple.recovery.boot\boot.efi)
    Boot0082*   ACPI(a0341d0,0)PCI(1c,5)PCI(0,0)SATA(0,0,0)HD(3,7845e20,135f20,88bc09aa-aafb-4874-be13-ec21dabd7070)
    BootFFFF*   ACPI(a0341d0,0)PCI(1c,5)PCI(0,0)SATA(0,0,0)HD(3,1d250438,135f20,88bc09aa-aafb-4874-be13-ec21dabd7070)File(\System\Library\CoreServices\boot.efi)


It is encouraged to man efibootmgr. It has an examples section which is quite useful. eg:
  efibootmgr [-n | --bootnext] 0080  #Boot into 0080 next boot instead of default (just once)
    BootNext: 0080
    BootCurrent: 0000
    ...

When you installed your OS, it likely has already populated your UEFI with its efi bootloader information. For other entries, UEFI is finding boot targets itself.

So how does EFI find these boot targets? Well...crack open the EFI_DEVICE_PATH_PROTOCOL section of the spec if you really want to get into how UEFI defines what a possible boot target could be. This is a generic protocol that’s used for other things than booting – it’s UEFI’s Official Way Of Identifying Devices For All Purposes, used for boot manager entries but also for all sorts of other purposes. Not every possible EFI device path makes sense as a UEFI boot manager entry, for obvious reasons (you’re probably not going to get too far trying to boot from your video adapter). But you can certainly have an entry that points to, say, a PXE server. The spec has lots of bits defining valid non-disk boot targets that can be added to the UEFI boot manager configuration.

In the case of finding bootable attached storage, UEFI needs that disk to either have an MBR(legacy) or be formatted with a GPT. From a disks' GPT, UEFI can generate a target path and look for efi bootloaders. Read on.

##
A quick foray into GPT
##
GPT allocates 64 bits for logical block addresses (LBA = a pointer to a location on disk). So with 64 bits of space, we can address up to 2^64 locations on disk. Now this is where sectors come in. Each of these LBA's points to a sector on disk. A typical sector size is 512 bytes, so given this assumption we are able to address 2^64 locations of 512 byte sectors. 2^64*512b=9.4ZB

HDD manufacturers are starting to transition to 4096byte sectors. Unfortunately, many of these drives are often still presenting 512byte sectors to the OS. This can cause misaligned writes, causing two disk IO operations each time a single misaligned 4KB write operation occurs. Such a misalignment occurs by default if the first partition is placed immediately after the GPT, as the first usable block after (default) GPT is LBA 34, whereas the next 4 KB boundary begins with LBA 40. Keep this in mind when choosing hdds and when configuring sector sizes for your LUNs in your RAID config (you want your presented sector size to match your actual disk sector size).

LBA 0 : At the start of your storage device, the first logical block address, exists a "protective MBR" which allows a BIOS-based computer to recognize the disk and boot from it. Note that the bootloader (ie: grub/other) has to support GPT, and so does your OS
LBA 1 : GPT header. Has: your disk GUID, number and size of partition entries that make up partition table (minimum 16KB reserved for partition table array, which translates to 128 partition entries @ 128bytes each), size and location of secondary GPT header table (the backup GPT at the end of the disk), and a checksum! (this means manually modifying GPT with a hex editor makes the checksum invalid, resulting in other firmwares thinking your GPT is corrupt and possibly triggering automated recovery from your secondary GPT).
LBA 2 : First 4 entries of your partition table, or "Partition Entry Array". Values: partition GUID, first LBA, last LBA, attributes (0-system partition, 1-EFI ignore me, 2-Legacy BIOS bootable ..), etc. 128bytes total per entry.
LBA 3 : Partition Entry Array entries 5 through 128
LBA 34 : First usable sector. Why LBA 34? On a disk having 512-byte sectors, a partition entry array size of 16,384 bytes and the minimum size of 128 bytes for each partition entry, LBA 34 is the first usable sector on the disk.
LBA -33 : Backup partition table entries (first four)
LBA -32 to -2 : Backup partition table entries (remaining)
LBA -1 : Secondary GPT header


Back to UEFI
^^^^^^^^^^^^
When talking about booting from storage devices where there is not already a target configured in UEFI (live images, OS install media)
So from the above, we find that UEFI reads an attached storage device's GPT and looks through its Partition Entry Array for partitions with attribute "0" (efi system partition). From the GPT it also gets your disk GUID and your partition GUIDs. So you have the disk and the partition path, but what .efi bootloader should be ran if booting from that target? Perhaps you are trying to boot from a live image. In this instance, UEFI has default locations that it looks in. Namely, for x86_64, /EFI/BOOT/BOOTx64.EFI  (where x64 is actually a "machine type short-name"). It could also look for BOOTIA32.EFI, BOOTARM.EFI, etc etc..

How does an OS configure UEFI?
When you install an OS that has native UEFI support, at some point in the setup process it will look for an existing EFI system partition (and create one if it doesn't exist), drop its EFI bootloader onto it, then call efibootmgr -c {some options here}. By default just running "efibootmgr -c" assumes that /boot/efi is your EFI system partition, and is mounted at /dev/sda1. It'll drop in elilo.efi and make a new entry called "Linux" at the top of the boot order.
You can have as many EFI system partitions as you want wherever you want...remember the UEFI entries (which exist in NVRAM) reference both a disk GUID and a partition GUID.


A quick recap
^^^^^^^^^^^^^
* Your UEFI firmware contains something very like what you think of as a boot menu.
* You can query its configuration with efibootmgr -v, from any UEFI-native boot of a Linux OS, and also change its configuration with efibootmgr (see the man page for details).
* This ‘boot menu’ can contain entries that say ‘boot this disk in BIOS compatibility mode’, ‘boot this disk in UEFI native mode via the fallback path’ (which will use the ‘look for BOOT(something).EFI’ method described above), or ‘boot the specific EFI format executable at this specific location (almost always on an EFI system partition)’.
* The nice, clean design that the UEFI spec is trying to imply is that all operating systems should install a bootloader of their own to an EFI system partition, add entries to this ‘boot menu’ that point to themselves, and butt out from trying to take control of booting anything else.
* Your firmware UI has free rein to represent this mechanism to you in whatever way it wants, and it may do this well, or it may do this poorly.

An important caveat
^^^^^^^^^^^^^^^^^^^
If you boot the installation medium in ‘UEFI native’ mode, it will do a UEFI native install of the operating system: it will try to write an EFI-format bootloader to an EFI system partition, and attempt to add an entry to the UEFI boot manager ‘boot menu’ which loads that bootloader.
If you boot the installation medium in ‘BIOS compatibility’ mode, it will do a BIOS compatible install of the operating system: it will try to write an MBR-type bootloader to the magic MBR space on a disk.

This is an important distinction! Be aware of which mode you are using when you do your installs. If you want UEFI, use it all the way through. If you're not sure which mode you've booted into in your installer, ctrl-alt-f2 and run efibootmgr -v. It shouldn't error out. If it does, complaining about sysfs/procfs entries for EFI variables not there blah blah then you're in legacy boot mode.
Remember when creating livecd iso-to-disk's that your media needs to comply with UEFI if you want it to boot as UEFI. This means it’s got to have a GPT partition table, and an EFI system partition with a bootloader in the correct ‘fallback’ path – \EFI\BOOT\BOOTx64.EFI (or the other names for the other platforms). Tools like livecd-iso-to-disk will do this for you if you pass --efi to it.

You've also got to worry about booting an installer in UEFI mode, and (without formatting) installing to a disk that is already MBR. Don't do this, you're asking for failure. parted /dev/sdx will show you your Partition Table type.

Secure Boot in a nutshell
^^^^^^^^^^^^^^^^^^^^^^^^^
Your UEFI firmware can ship with a set of signatures. It can refuse to run any EFI executable which is not signed with one of those signatures. That's basically it! So why are people so pissed off about it? Because assholes (microsoft, shipping windows 8 or newer pre-installed) can ship their firmware with secure boot enabled by default, and even though UEFI spec *requires* that a user must be able to disable secure boot, microsoft doesn't care! Microsoft x86 spec says "Allow a physically present person to disable Secure Boot", while their ARM computers shipped from microsoft spec must NOT allow secure boot to be turned off! Now you end up working through a bunch of BS (or not being able) to install another OS because your fedora install doesn't have a signed bootloader.efi.

And it's not just microsoft (as mentioned above, MS actually explicitly protect your right to determine what you can boot on your system *on x86 systems*). All iDevices and some Android devices implement secure boot - that's how they enforce a locked bootloader. This is why people have to come up with exploits in order to patch the system firmware to disable secure boot and allow other builds to be installed on the device. Don't like it? Don't buy it!


Crapintosh
^^^^^^^^^^
Apple ships at least some Macs with their bootloaders in an HFS+ partition. The spec says a UEFI-compliant firmware must support UEFI FAT partitions with the specific GPT partition type that identifies them as an “EFI system partition”, but it doesn’t say the firmware can’t also recognize some other filesystem type and load a bootloader from that. Now everyon else has to deal with it. Additionally, Apple also goes quite a long way beyond the spec in its boot process design, and if you want your alternative OS to show up on its graphical boot menu with a nice icon and things, you have to do more than what the UEFI spec would suggest. 

PXE Booting
^^^^^^^^^^^
!! Add me !!

.. _cassandra:

Cassandra
=========

Intro
-----

Cassandra uses consistent hashing to distribute data and transfers request and data between nodes directly (peer to peer), much the same way as :ref:`dynamo`. A client may choose, upon each request (SELECT, UPDATE, INSERT, DELETE...), how much consistency they desire (ie: how many nodes to respond before a confirmed operation). Interaction with Cassandra uses *Cassandra Query Language* (CQL), which is nearly identical to SQL. Cassandra allows you to apply replication configurations to each of your databases (ie: keyspaces) which take into account running a keyspace across multiple datacenters and physical racks.

In contrast to Dynamo's preference list, any Cassandra node in a cluster can become coordinator for any read/write request. Similar to Dynamo's "hinted handoff," if one or more nodes responsible for a particular set of data are down, data is simply written to another node which temporarily holds the data until the downed node comes back online.

When data is written to Cassandra, it is first written to a commit log. It is also written to an in-memory structure called a memtable, which is eventually flushed to a disk structure called a *sorted strings table* (sstable). When a read request comes in, it is sent out to the node(s) containing the data, who then perform their work in parallel. If a node is down, the request is forwarded to one of its replicas.

Regarding performance, in 2011 Netflix was able to achieve 3.3million writes per second (1.1million with N=3) across three amazon availability zones using 288 medium size instances (96 instances per AZ). Cassandra is touted to have a linear performance increase as you add more nodes.

With regards to data structure, Cassandra can store structured, semi structured, and unstructured data. In contrast to RDBMS's, you can't do JOINs, so data tends to be pretty denormalized (ie: lots of columns). This is no big deal - Cassandra operates very quickly on objects with many thousands of columns. Within a keyspace are one or more column families, which are like relational tables. These families have one to many thousands of columns, with both primary and secondary indexes on columns being supported. More on this later.

Datastax is the main "enterprise support" for Cassandra company. They employ some Cassandra contributors and have employees who sit on Apache's board.


Data Model
----------
A table in Cassandra is a distributed multi dimensional map, indexed by a key. The value is an object which is highly structured. The row key in a table is a string with no size restrictions, although typically 16 to 36 bytes long. Every operation under a single row key is atomic per replica no matter how many columns are being read or written into.

Columns are grouped together into sets called column families very similar to what happens in BigTable. Cassandra exposes two kinds of column families, Simple and Super column families. Super column families can be visualized as a column family within a column family. Furthermore, applications can specify the sort order of columns within a super column or simple column, sorted either by time or by name. Time sorting is good for things like inbox search, where results are always in time sorted order.

Any column within a column family is accessed using the convention *column_family : column* and any column within a column family that is of type super is accessed using hte convention *column_family : super_column : column*. More on this later.

Typically, applications use a dedicated Cassandra cluster and manage them as part of their service. Although the system supports the notion of multiple tables, all deployments (at facebook) have only one table in their schema. In other deployments, users may have multiple tables which are each designed to be efficient in serving certain types of queries (eg: grouping by an attribute, ordering by an attribute, filtering based on some set of conditions...etc). Using separate tables and (most likely necessarily) having duplicate copies of data between them (but with different column layouts) is necessary and recommended in order to gain maximum read optimization. In short, design your tables around what high-level queries you have.

API
---
The Cassandra API consists of the following three simple methods:

* *insert(table, key, rowMutation)*
* *get(table, key, columnName)*
* *delete(table, key, columnName)*

*columnName* can refer to a specific column within a column family, a column family, a super column family, or a column within a super column.

System Architecture
-------------------

General
^^^^^^^
Typically a read/write request for a key gets routed to any node in the cluster. The node then determines the replicas for this particular key. For writes, the system routes the requests to the replicas and waits for a quorum of replicas to acknowledge the completion of writes. For reads, based on the consistency guarantees required by the client, the system either routes the requests to the closest replica or routes the requests to all replicas and waits for a quorum of responses.

Partitioning
^^^^^^^^^^^^
Much like Dynamo, Cassandra partitions data across the cluster using consistent hashing - but uses an order preserving hash function to do so (TODO: how does this order preserving hash function affect key placement? what are they talking about here?).

*Basic consistent hashing:* In consistent hashing, the output *range* of a hash function is treated as a fixed circular space or "ring" (ie: the largest hash value wraps around to the smallest hash value). Each node in this system is assigned a random value within this sapcve which represents its position on the ring. Each data item identified by a key is assigned to a node by hashing the key of the data item, getting that value, and then walking the ring from that value clockwise until it hits the first node with a position larger than the item's position. That node then becomes coordinator for that piece of data. So, each node when walking clockwise on the ring is responsible for the range between itself and its predecessor node. The advantage of this system is that when a node is added or removed (and gets a new position on the ring or is removed from its position), it only affects two neighbouring nodes.

Disadvantages of this method include the random-node-position-assignment and the system being oblivious to node hardware differences. The random node assignment around the ring leads to some nodes being responsible for larger keyspaces than others, leading to uneven load distribution. The lack of heterogenity awareness leads to some higher powered nodes not taking on larger key ranges and load than lower power nodes.

To get around this problem, you could assign the same node to multiple positions along the ring, and/or you can analyze the load along the ring and move lightly loaded nodes into keyspace owned by heavily loaded nodes in order to distribute work more efficiently. Both of these approaches incorporate the usage of virtual nodes, or *tokens*, which sit in positions along the ring in place of actual physical nodes. Physical nodes can then be assigned or removed more or less tokens.

.. image:: media/cassandra-tokenarch.png
   :alt: Single vs Virtual token architecture
   :align: center

   **Figure 1: Single token vs Virtual token node assignment**


Replication
^^^^^^^^^^^
As with Dynamo, each data item (ie: **row**) is replicated to N hosts where N is a given replication factor. Each key is assigned to a coordinator node and that coordinator node is responsible for replicating these keys to N-1 other nodes in the ring.

Cassandra provides the client with various options for how data needs to be replicated, such as "Rack Unaware", "Rack Aware (within a datacenter)" and "Datacenter Aware." If a client chooses rack unaware, the data is simply replicated to the next N-1 successors of the coordinator on the ring.

For *rack aware* and *datacenter aware* the system is a bit more complicated. Cassandra first elects a leader amongst its nodes and stores that info in zookeeper. All nodes upon joining a cluster read zookeeper and then contact the leader, who tells them for what ranges they are replicas for. The leader makes a best effort to ensure that no node is responsible for more than N-1 ranges in the ring. The metadata about the ranges a node is responsible for is cached locally at each node as well as in your zookeeper cluster. As such, a node can crash and lose its disk, and still come back up and know what it is responsible for. Cassandra borrows the *preference list* parlance from Dynamo by calling the nodes that are responsible for a given range the *preference list* for that range.

Cassandra is configured to replicate each *row*. It can be configured such that each *row* is replicated across multiple data centers. In this configuration, the preference list for a given key is constructed such that the member storage nodes are spread across multiple datacenters.


Cluster Membership
^^^^^^^^^^^^^^^^^^
Cluster membership in Cassandra is based off of Scuttlebutt, an efficient anti-entropy gossip based algorithm. This gossip based system is used for membership as well as other system related control state data.

TODO: More about scuttlebutt


Failure Detection
^^^^^^^^^^^^^^^^^
Φ aka PHI
A modified version of ΦAccrual Failure Detector is used by each node to determine whether any other node in the system is up or down. The idea of an Accrual Failure Detector is that you are not working with a boolean stating whether a node is up or down. Instead, the value is more of a sliding scale or a "suspicion level." This value is defined as Φ, and its value can be dynamically adjusted to reflect network and load conditions at the monitored nodes.

Φ has the following meaning: given some threshold Φ, and assuming that we decide to "suspect" that node A is down when Φ = 1, then the likelihood that we will make a mistake (ie: the decision will be contradicted in the future by the reception of a late heartbeat) is about 10%. When Φ = 2 that chance of error decreases to 1%, and when Φ = 3 the chance further decreases to 0.1%, and so on.

Every node in the system maintains a sliding window of inter-arrival times of gossip messages from other nodes in the cluster. The distribution of these inter-arrival times is determined and Φ is calculated. In general, accrual failure detectors have been found to be very good in both their accuracy, speed, and in their ability to adjust well to network conditions and server load conditions.

Facebook found that a slightly conservative PHI value of 5 was able to detect failures in a 100 node cluster on average in about 15 seconds.


Bootstrapping
^^^^^^^^^^^^^
When a node starts for the first time, it chooses a random token for its position in the ring (this contradicts other stated info: "when a new node is added to the system, it gets assigned a token such that it can alleviate a heavily loaded node"). For fault tolerance, this position is saved both locally to disk and also to zookeeper. This info is then gossip'd around the cluster, which is how each node knows about all other nodes and their respective positions in the ring. When a node is bootstrapped for the first time, it reads its config (or zookeeper) for a listing of seed nodes - initial contact points to gain information of the cluster from.

TODO: Does the initial gossip advertisement require knowing about one or more seed nodes (likely answered after reading more about scuttlebutt)?


Scaling
^^^^^^^
When a new node is added to the system, it gets assigned a token such that it can alleviate a heavily loaded node (note: contradicts "randomly selected" info above...). This results in the new node splitting a range off the heavily loaded node for itself. The node giving up the data will stream data over to the new node using kernel-kernel copy techniques (TODO: expand). The Cassandra bootstrap algorithm can be initiated by any other node in the cluster, either via command line utility or Cassandra web dashboard.

Operational experience at facebook has shown that data can be transferred at a rate of 40MB/sec from a single node. They are currently working on improving this transfer rate by having multiple replicas take part in the bootstrap transfer, thereby parallelizing the effort, using a method similar to bittorrent.


Local Persistence
^^^^^^^^^^^^^^^^^
Cassandra saves data to disk using a format that lends itself well to efficient data retrieval. A typical write operation involes a write into a commit log, after which (if a successful write to commit log occurs) an update into an in-memory data structure occurs. Facebook uses a dedicated LUN/disk for their commit log. Writes to the commit log are sequential. 

As for the in-memory data structure, once it crosses a certain threshold (calculated based on data size and number of objects) it dumps itself to disk. Along with each data structure, an index is generated which allows efficient lookups on the associated data structure based on row key. Over time, you end up with a lot of files. As such, a background merge process will occaisionally collate all these different files into a single file. This process is very similar to what happens in Bigtable.

A typical read operation will first query the in-memory structure before looking into the files on disk. If a disk hit is needed, the files are looked at in order of newest to oldest. For some reads, a disk lookup could occur which looks up a key in multiple files on disk. In order to prevent looking into files tha tdo not contain the key, a bloom filter which summarizes the keys in the file is also stored in each data file and as well as kept in memory. This bloom filter is first consuletd to check if the key being looked up does indeed exist in a given file.

A key in a column family could have many columns. In order to prevent scanning of every column on disk, we maintain column indexes which allow us to jump to the right chunk on disk for column retrieval. This is done by generating indeces at every 256K chunk boundary as the columns for a given key are being serialized and written out to disk. This boundary is configurable, but facebook has found that 256K works well in their production workloads.

Implementation Details
^^^^^^^^^^^^^^^^^^^^^^
Cassandra is mainly made up of three abstractions: the partitioning module, cluster membership & failure detection module, and the storage engine module. Each of these modules follow something along the lines of *staged event-driven architecture (SEDA)*, which decomposes a complex, event driven application into a set of stages connected by queues. Message processing pipeline and task pipeline are used to refer to the queues and inter-module data flow in this system.

The cluster membership & failure detection module is built on top of a network layer which uses non-blocking I/O. The system control messages rely on UDP, while the application related messages for replication and request routing rely on TCP.

The request routing modules (ie: the other two?) are implemented using a certain state machine. When a read/write request arrives at any node in the cluster, the state machine morphs through the following states (excluding failure scenarios for now):

1) identify the node(s) that own the data for the key
2) route the requests to the nodes and wait ont he responses to arrive
3) if the replies do not arrive within a configured timeout value, fail the request and return to the client
4) figure out the latest response based on timestamp
5) schedule a repair of the data at any replica if they do not have the latest piece of data.

The system can be configured to perform either synchronous or asynchronous writes. While asynchronous writes allow a very high write throughput, synchronous writes require a quorum of responses before a response is passed back to the client.

As for the commit log, it is rolled over to a new file every 128MB. The write operation into the commit log can either be in normal mode or in fast sync mode. In the fast sync mode, the writes to the commit log are buffered and we also dump the in-memory data structure to disk in a buffered fashion. As such, this mode has the potential for data loss upon a machine crash.

Cassandra morphs all writes to disk into a sequential format, and the files written to disk are never mutated. This means no locks need to be taken when reading the files. TODO: how is old data removed? during compaction process?

The Cassandra system indexes all data based on primary key. The data file on disk is broken down into a sequence of blocks. Each block contains at most 128 keys and is demarcated by a block index. The block index captures the relative offset of a key within the block and the size of its data. When an in-memery data structure is dumped to disk, a block index is generated and their offsets written out to disk as indices. This index is also maintained in memory for fast access.

As stated prior, the number of data files on disk will increase over time. The compaction process, very much like the Bigtable system, will merge multiple files into one; essentially merge sort on a bunch of sorted data files. The system will always compact files that are close to each other with respect to their sizes (ie: a 100GB file will never be compacted together with a sub-50GB files). Periodically a major compaction process is run to compact all related data files into one big file. This compaction process is disk I/O intensive and can be optimized so as not to affect incoming read requests.


Practical Noteable
------------------
* Setting a value of PHI to 5 on a 100 node cluster resulted in an average node failure detection time of 15 seconds
* Cassandra is well integrated with Ganglia
* Facebook's inbox was holding around 50+TB on a 150 node east/west cost cluster in 2009
.. _dynamo:

Dynamo
======

Summary
-------
A highly available key/value store. Provides a place for various services (shopping cart, best seller lists, customer preferences, session management, product catalog, etc) to store their data in a "primary key only," non-relational way. Dynamo’s gossip-based membership algorithm helps every node maintain information about every other node. Dynamo can be defined as a structured overlay with at most one-hop request routing. Dynamo detects updated conflicts using a vector clock scheme, but prefers a client side conflict resolution mechanism.

To achieve "always on", dynamo will sacrifice consistency under certain failure scenarios (eventually consistent). It makes extensive use of object versioning and application-assisted conflict resolution in order to provide the most accurate state of the database as possible. That said, dynamo does try to give some control to services over durability and consistency, and allow them to make trade-offs between functionality, performance, and cost effectiveness.

Dynamo (at the time of the dynamo paper release) was able to support hundreds of thousands of concurrently active sessions (shopping cart service, holiday season). Keep in mind all the condensed notes in this file are from a paper written in 2007.

- Data is partitioned and replicated using consistent hashing
- Consistency is facilitated by object versioning
- Consistency among replicas during updates is maintained by a quorum-like technique and a decentralized replica synchronization protocol.
- Employs a gossip based distributed failure detection and membership protocol
- Completely decentralized: storage nodes can be added/removed without needing any manual partitioning or redistribution

.. _dynamo-general:

General
-------

Query Model
^^^^^^^^^^^
Simple read and write operations to a data item that is uniquely identified by a key. State is stored as binary objects (blobs) identified by unique keys. No operations span multiple data items and there is no need for relational schemas. Dynamo targets applications that need to store relatively small objects (sub 1MB).

ACID Properties
^^^^^^^^^^^^^^^
Atomicity, Consistency, Isolation, Durability. A set of properties that guarantee a database transaction is processed reliably. Typically, ACID model software will sacrifice availability over consistency. Dynamo targets services which operate with weaker consistency needs in any case where it is advantageous for higher availability. It does not provide any isolation guarantees, and only permits single-key updates.

Efficiency
^^^^^^^^^^
Latency is of utmost importance in Dynamo. Throughput is next. Applications must be able to configure Dynamo such that they can consistently (99.9% percentile distribution) achieve their latency and throughput requirements. Cost efficiency, availability, and durability guarantees can all be traded off in order to prioritize latency and throughput.

Other
^^^^^
Dynamo was designed to be an internal, "in a safe network" service. There are no security related requirements such as authorization/authentication. Additionally, each service is expected to have its own Dynamo cluster. As such, Dynamo is only expected to scale to hundreds of storage hosts. More on scalability limitations later.

.. image:: media/dynamo-amazonarch.jpg
   :alt: Amazon's Service Oriented Architecture
   :align: center

.. _dynamo-sla:

SLA
---
Amazon typical(2007): Provide a response within 300ms for 99.9% of its requests for a peak client load of 500 req/sec.

A typical page request may cause the page rendering engine to request data from over 150 services. These services often have multiple dependencies themselves. As such, a call graph will fan out in a tree structure. Each service in the call graph must comply with its SLA in order to get a response back to the user's browser fast enough.

Describing an average, median, and expected variance is not good enough if you want to provide a good experience to all users. For example, a heavy user may have a longer buy history, and therefore could have their page loads perform worse that a less preferred customer. To avoid this, you must calculate the costs associated with providing a higher percentile of distribution in relation to response time. Amazon found anything beyond 99.9% (and whatever their latency target is) to be cost prohibitive.

    Note: a load balancer's selection of which write coordinator to pass 
    a request to is very important when targeting performance at a 99.9% 
    percentile distribution. Pay some attention to it.

.. _dynamo-design-choices:

Design Choices
--------------
Replication
^^^^^^^^^^^
A typical replication model performs a synchronous replication across some interface, providing a highly consistent operation which is very concerned with the correctness of an operation. Due to failing networks and servers, strong consistency and high data availability cannot be achieved at the same time. Some optimistic protocols allow writes/changes to occur on replication hosts in the background while waiting to see whether the "master" will recover. This introduces the possibility of conflicting changes, and the need arises to resolve these conflicts (sometimes manually, as in the case of skipping transactions forward in a mysql replication log). When do we resolve them, and who resolves them?
- Per the When, a conflict arises upon a read or a write action. In most applications, the conflict resolution happens on write operations, at the expense of availability. Dynamo instead keeps writes available no matter what, relies on eventual consistency, and focuses on resolving conflicts when data is *read*.
- Per the Who, a conflict can be resolved by either the datastore or the application. The datastore is somewhat limited as it can really only say "the last write wins." The application on the other hand is aware of the data schema, and as such can pull in all the changes and merge them, hopefully returning a single unified view(/shopping cart). This requires more developer work.

.. image:: media/dynamo-replication-keyhashing.png
   :alt: Partitioning and replication of keys in Dynamo ring.
   :align: center

   **Figure 2: Partitioning and replication of keys in Dynamo ring.**

Other Considerations
^^^^^^^^^^^^^^^^^^^^
*Incremental Scalability:* Scale out one storage node at a time, with minimal ineraction and minimal to no performance impact.

*Symmetry:* Every node should have the same responsibilities as its peers. No unicorns. Much easier operationally

*Decentralization:* The design should favour decentralized, peer-to-peer techniques over centralized control.

*Heterogenity:* The design should be able to accommodate some nodes hardware being faster/slower than others. You should not have to upgrade all nodes to the same newer hardware spec each time you get new stuff.


As mentioned prior, Dynamo is targeted mainly at applications that need an “always writeable” data store where no updates are rejected due to failures or concurrent writes.

Second, as noted earlier, Dynamo is built for an infrastructure within a single administrative domain where all nodes are assumed to be trusted. 

Third, applications that use Dynamo do not require support for hierarchical namespaces (a norm in many file systems) or complex relational schema (supported by traditional databases). 

Fourth, Dynamo is built for latency sensitive applications that require at least 99.9% of read and write operations to be performed within a few hundred milliseconds. To meet these stringent latency requirements, it was imperative for us to avoid routing requests through multiple nodes (which is the typical design adopted by several distributed hash table systems such as Chord and Pastry). This is because multi-hop routing increases variability in response times, thereby increasing the latency at higher percentiles. Dynamo can be characterized as a zero-hop DHT, where each node maintains enough routing information locally to route a request to the appropriate node directly.

.. _dynamo-architecture:

Architecture
------------

.. csv-table:: Summary of techniques and their advantages
   :header: "Problem", "Technique", "Advantage"
   :widths: 20, 20, 30

   "Partitioning", "Consistent Hashing", "Incremental Scalability"
   "High Availability for writes", "Vector clocks with reconciliation during reads", "Version size is decoupled from update rates"
   "Handling temporary failures", "Sloppy quorum and hinted handoff", "Provides high availability and durability guarantee when some of the replicas are not available"
   "Recovering from permanent failures", "Anti-entropy using Merkle trees", "Synchronizes divergent replicas in the background"
   "Membership and failure detection", "Gossip-based membership protocol and failure detection", "Preserves symmetry and avoids having a centeralized registry for storing membership and node liveness information (ie:zookeeper)"


TODO Consistent hashing: :ref:`distributedsystems-hashing`

Vector clocks: :ref:`dynamo-vectorclocks`

TODO Sloppy quorum :ref:`distributedsystems-quorum`  Useful: http://jimdowney.net/2012/03/05/be-careful-with-sloppy-quorums/

TODO Gossip protocols: :ref:`distributedsystems-gossip`

Merkle tree: :ref:`dynamo-merkle`

Dynamo shares the same needs that any distributed architecture needs to address. Mainly, scalable and robust solitions for:
- Load balancing
- Membership and failure detection
- Failure recovery
- Replica synchronization
- Overload handling
- State transfer
- Concurrency and job scheduling
- Request marshalling and routing
- System monitoring and alarming
- Configuration management


.. _dynamo-architecture-partitioning

Partitioning
^^^^^^^^^^^^
Since Dynamo must scale incrementally, it requires a mechanism to dynamically partition data over a set of nodes. Dynamo uses consistent hashing to spread data across multiple storage hosts.

Let's reference this image again:

.. image:: media/dynamo-replication-keyhashing.png
   :alt: Figure 2: Partitioning and replication of keys in Dynamo ring.
   :align: center

   **Figure 2: Partitioning and replication of keys in Dynamo ring.**

**Consistent hashing** works by treating the output range of a hash function as a fixed circular space or "ring" (ie: the largest hash value wraps around to the smallest hash value). Each node in the system is assigned a random value within this space, which represents is "position" in the ring. When a data item represented by a key comes in, the key is hashed in order to get a value, and then the ring is walked **clockwise** until a node is found with a position larger than the data item's position. That node, the first node found with a position larger than the data item, is assigned to be the coordinator for that key+data. As such, each node becomes responsible for the region in the ring between itself and its **predecessor** node. Because of this, we can see in Figure 2 that Key K would be owned by Node B. Node B owns the whole range between itself and Node A.

The main advantage of consistent hashing is that departure or arrival of a node only affects its immediate neighbours, and other nodes remain unaffected. The output range of the hash stays the same - the new node is just plopped down between two other nodes. This seems bad because if a new node got placed **directly** counterclockwise of, say, node D, wouldn't that mean that node D would be responsible for a very tiny portion of ranges and therefore get less load and datat? Correct. The picture above shows nodes in nice and symmetrical placements, but in reality that node placement in the ring is random. This leads to non-uniform data distribution and load. And what if one of your nodes is a lot more powerful than the others, so you want it to be responsible for a larger portion of the hash?  

To get around these issues, Dynamo uses a variant of consistent hashing: instead of mapping a node to a single point, a physical node gets mapped to multiple **virtual nodes**, each of which have a "token" (ie: position) along the ring. To summarize, each physical node is now responsible for an arbitrary/configurable number of tokens along the ring. Using virtual nodes has the following advantages:

- If a node becomes unavailable, the load handled by this node is evenly dispersed across the remaining available nodes (ie: other physical nodes can take control of the dead nodes' virtual nodes)
- When a node becomes available again, or if a new node is added, the new node can accept a roughly equivalent amount of load from each of the other available nodes (Note: this means that node additions/removals can now have an impact on more than just their clockwise neighbour)
- The number of virtual nodes that a node is responsible for can be decided based on its hardware specs


Replication
^^^^^^^^^^^
Above we described how each node is responsible for a set of keys. Replication in dynamo is configured "per-instance," thus, each node is responsible for replicating its content to N other nodes based on a configuration parameter. Phrased another way, each key, k, is assigned to a coordinator node (ie: the node responsible for that range of keys), and the coordinator is in charge of replicating the key+data items to N-1 clockwise successor nodes in the ring. So with N=3, referencing figure 2 above, node B replicates the key *k* to nodes C and D in addition to storing it locally.

The list of nodes that are responsible for storing a particular key is called the *preference list*. More details about how this list is maintained are in the Membership and Failure Detection section below. Each node in the cluster is designed to be aware of which nodes should be in this list for any particular key. To account for node failures, preference list contains more than just N nodes! Note how above we talked about virtual nodes - it is possible that the first N virtual successor node positions for a particular key may be owned by less than N physical nodes. To address this, the preference list for a key is constructed by skipping positions in the ring such that the list only contains distinct physical nodes. (...unclear on how it does this...)

.. _dynamo-versioning:

Data Versioning
---------------

Dyano is eventually consistent, and as such it allows for updated to be propogated to all replicas asynchronously. A put() call may return success to its caller prior to the update actually making its way to all the replicas. As such, a subsequent get() may return "stale" data.

In Dynamo, when a client wishes to update an object, *it must specify which version it is updating.* This is done by doing a prior read operation which contains the vector clock information. More on that later.

Certain amazon applications, such as shopping cart, should never deny an "add to cart" or a "remove from cart." When an update like this ends up happening on an older version of the object while a newer version of the object has not yet propogated, both versions are kept as immutable objects of different versions. These divergent versions are reconciled later, either by the client or by dynamo.

Most of the time, new versions subsume the previous version(s), and the system itself can determine the authoritative version (syntactic reconciliation, ie: a diff or a simple rule like if obj.v2 > obj.v1: obj=v2). However, in the presense of failures and concurrent updates, version branching may happen which result in conflicting versions of an object. In this case (dynamo) the client must perform the reconcilliation in order to collapse divergent branches back into one (semantic reconciliation). A typical example of this operation is "merging" different versions of a customer's shopping cart. Using this method, updates will never be lost, but deleted items may resurface.

It is important to emphasize that if you don't want to "lose" data, the client code you're writing needs to explicitly acknowledge the possibility of multiple versions of the same data.

.. _dynamo-vectorclocks:

Vector Clocks
^^^^^^^^^^^^^
Dynamo uses vector clocks in order to "capture causality between different versions of the same object." A vector clock is just a list of (node, counter) pairs. *Each version* of an object you shove into dynamo has a vector clock associated with it. If the counters on the first object's clock are less than or equal to the counters on the second object's clock, then we can assume that the second object is newer than the first, and the first can be forgotten. If this is not the case, then the two objects are considered in conflict and will require reconciliation.

As mentioned prior, a dynamo client must specify which version of the object that it is updating by using vector clock information obtained from a read operation. Upon processing a read request, if Dynamo has access to multiple branches that cannot be syntactically reconciled, it will return *all* the objects at the leaves, with corresponding version information in the context. An update usin gthis context is considered to have reconciled the divergent versions, and the branches are collapsed into a single new version.

**Any storage node in dynamo is eligible to receive client get and put operations for any key.** In a non-failure scenario though, where the client is aware of dynamo partitioning (ie: you're not running requests through a load balancer), the operation will go to the top node in the preference list for that key region. Whichever node ends up handling the read/write is called the *coordinator*.

Remember that each node is capable of building a preference list for any particular key, so each node knows whether it is in the "top N nodes" for a particular key. If a load balancer routes a request to a node for a key in which it is not part of the top N of the key's preference list, it will route the request onwards to the first of the top N nodes in the preference list. If a node receives a request in which it is part of the top N, but is not first in the top N, it will still perform the operation. This means that in a loadbalanced setup there is a lot more reconciliation.

Example Flow
^^^^^^^^^^^^
With the above in mind, let's go through an example:

.. image:: media/dynamo-versioning.png
   :alt: Figure 3: Version evolution of an object over time
   :align: center

   **Figure 3: Version evolution of an object over time.**

Let's assume that we're using a load balancer to distribute requests. A client is doing a fresh write, a new object, and the request gets routed to Node X. Node X hashes the key and sees that it belongs in the top N nodes of that key's preference list, and as such is able to perform this operation. It writes out that new object D1 and assigns a vector clock (X, 1). Another write comes in for the same object, and it gets loadbalanced to X again, who then *creates a new object* D2 and assigns a vector clock to it of (X, 2). At this point D1 will be cleaned up on node X because D2 descends from D1, but other nodes might not have D2 yet due to replication lag or failure, so D1 might still be lingering out there.

Now let's say the same client updates D2, but the request this time is routed to Node Y. Node Y creates a new object, D3, and assigns a vector clock [(X, 2), (Y, 1)]. All good so far, as both X and Y are part of the top N preference list.

Now a new client comes along and does a read for a key associated to D. It ends up getting the old version D2(X,2) from a stale node. It does its change and then does a put(), and the request ends up hitting Node Z. Z says, ok, I am in the top of the preference list (or the top N in the preference list are down/unavailable, or Z is temporarily in a network segmentation), so it performs the operation, creating a new object D4[(X, 2),(Z, 1)].

Once everything is back to normal, nodes with D3 and/or D4 will find that they can get rid of D1 and D2, but upon receiving D3 or D4 they will find that there are changes between the two that are not reflected in each other. Both versions of the data must be kept and presented to a client (upon a read) for *semantic* reconciliation. These two versions are kept forever, until a client comes along and reconciles the data!

So, a client comes along and wants to update this key associated with D3/D4. The node it's reading from, let's say Y, knows of the two versions and as such passes back that information in the read's "context". In this context is a summary of the clocks of D3 and D4, namely [(X, 2), (Y, 1), (Z, 1)]. If the client can reconciliate the data received, it will then perform a put(). Let's say that hits Node X. X will then create a new object with its sequence number iterated, ie D5[(X, 3), (Y, 1), (Z, 1)]. D3 and D4 can then be safely garbage collected by Dynamo.

    Dynamo needs to do a read prior to every write operation in order to grab vector
    clock information. This can be very limiting in systems which have high write
    loads. Look at Cassandra for environments like this.

.. _dynamo-failure:

Failure and Membership
----------------------
Hinted Handoff
^^^^^^^^^^^^^^
If Dynamo used a traditional quorum approach, it would be unavailable during server failures and network partitions, and would have reduced durability even under the simplest of failure conditions. To remedy this, it does not enforce strict membership and instead uses a "sloppy quorum," where all read and write operations are performed on the first N *healthy* nodes, not necessarily the first N nodes encountered when walking the consistent hashing ring. Let's use Figure 2 again where N=3 as an example:

.. image:: media/dynamo-replication-keyhashing.png
   :alt: Figure 2: Partitioning and replication of keys in Dynamo ring.
   :align: center

  **Figure 2: Partitioning and replication of keys in Dynamo ring.**

If node A is temporarily down or unreachable during a write operation, then a replica that would normally have lived on A will now be send to node D. The replica sent to D will have a hint in its metadata that suggests which node was the intended recipient of the replica (in this case, A). Nodes that receive hinted replicas will keep them in a separate local database that is scanned periodically. Once A is detected as having recovered, D will attempt to deliver the replica to A. Once the transfer succeeds, D will then delete the object from its local store (but it will still have the replicated copy in its regular datastore). During this whole process, the same amount of object replicas are kept.

If node A is temporarily down or unreachable during a write operation, then a replica that would normally have lived on A will now be send to node D. The replica sent to D will have a hint in its metadata that suggests which node was the intended recipient of the replica (in this case, A). Nodes that receive hinted replicas will keep them in a separate local database that is scanned periodically. Once A is detected as having recovered, D will attempt to deliver the replica to A. Once the transfer succeeds, D will then delete the object from its local store (but it will still have the replicated copy in its regular datastore). During this whole process, the same amount of object replicas are kept.

This ensures availability even in the event of network segmentation or server failures, at the expense of resiliency. For applications requiring the highest level of availability, you can set W (minimum number of confirmed writes) to 1, which ensures that a write is accepted so long as a single node has durably written the key to its local store. In practice, most applications set W to 2 or more. More details about N, R, and W later.

.. _dynamo-merkle:

Merkle Trees
^^^^^^^^^^^^
Hinted handoff works best if system membership churn is low and server/network problems are transient. Should that not be the case and some scenario occurs where a node fails forever before it can pass back its hinted replica to the proper node, dynamo uses a replica synchronization method involving Merkle Trees.

A Merkle tree is a hash tree where each leaf is the hash of an individual key. Parent nodes higher in the tree are hashes of their respective children. So let's think of a merkle tree with 20 leaves, a parent responsible for 4 leaves each, and a grandparent (root) responsible for those 5 parents. If a node wants to know whether another node has the same keys as them, they just need to compare each others root (grandparent) hashes. If they differ, then they can check the next level down for differing hashes. They may only find 1 out of 5 different, and can then compare the hashes of each leaf in only that 1 branch to see what is different or missing. This allows an efficient comparison of data without having to scan through every value.

In Dynamo, each node maintains a separate Merkle tree for each key range that it hosts (ie: a Merkle tree for every virtual node). This allows nodes to exchange the root node of the Merkle tree with all other nodes which have the same key ranges (virtual nodes) in common. The disadvantage with this approach is that when a node joins or leaves the system/virtual nodes are redistributed, key ranges change and require the tree to be recalculated. Mitigation of this effect is discussed later on.

Membership and Failure Detection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Node A may consider Node B failed if node B does not respond to node A's messages (even if B is responsive to node C). Node A will then route requests to alternate nodes which map to B's partitions. Node A will then periodically check whether connectivity to B has recovered. Node A will not "report" to anyone that B is down, it will simply mark it as down locally. Additionally, if node A does not need to ever talk to node N, then it does not care about node N's state. As such, temporary node failure is "detected" on a per-node basis - there is no global view for node failure.

It is beneficial however to have some sort of global cluster state for when permanent node additions/removals happen. A node outage rarely signifies a permanent departure, and therefore it should not trigger a rebalancing of virtual nodes or a repair of unreachable replicas. As such, manual node addition/removal was deemed most appropriate. This is done via a command line tool or web browser interface with dynamo. The node that serves the add/remove request writes the membership change and its time of issue to persistent store, keeping a history. A gossip-based protocol propagates membership changes and maintains an eventually consistent view of membership. Each node contacts a peer chosen at random every second, and the two nodes reconcile their persisted membership change histories.

When a node starts for the first time, it chooses its set of tokens (virtual nodes) and maps nodes to their respective token sets. That mapping is then persisted on disk and initially only contains the local node and the token set. During the gossip-based random-node-every-second process, nodes will compare their mapping and token set information with each other and reconcile that information. So, a new node will very likely gossip an existing node that is already fully aware of the rest of the cluster, getting the new node up to speed right away. 

    Note:: The above method of cluster data discovery can be fouled up a bit if you're adding multiple nodes at once. 
    If an administrator added node A, then added node B, these nodes would not be immediately known to each other. 
    To mitigate this, a user can configure something called "seed" nodes, which are nodes that all other nodes can 
    eventually reconcile with (outside of the random every second process). These nodes essentially have the best 
    view of the current state of the cluster

Addition/Removal of Storage Nodes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
When a new node (say, X) is added into the system, it is assigned a random scattering of virtual nodes (tokens) around the ring. Using Figure 2, let's just use one virtual node as an example. Let's say that virtNodeX is added to the ring between virtNodeA and virtNodeB. As such, with N=3, X is now responsible for the ranges between F-G, G-A, and A-X. As a consequence, nodes B, C, and D no longer have to store the keys in their respective tail ranges. So, upon confirmation from X, they will then transfer the appropriate set of keys to X. When a node is removed from the system, this proces happens in reverse.


.. _dynamo-implementation:

Implementation
--------------
Dynamo allows a choice of persistent data store ("local persistence engine"). This is typically Berkeley Database (sub 100kB objects), MySQL (larger objects), and an in-memory buffer with persistent backing store.

As for the request coordination, each client request results in the creation of a state machine on the node that received the client request. The state machine contains all the logic for identifying the nodes responsible for a key, sending the requests, waiting for responses, protentially doing retries, processing the replies and packaging the response to the client. Each state machine instance handles exactly one client request.

An example read operation (minus failure states and retry operations):

# send read requests to the (virtual) nodes
# wait for minimum number of required responses
# if too few replies were received within a given time bound, fail the request
# otherwise, gather allt he data versions and determine the ones to be returned
# if versioning is enabled, perform syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions

After a read request is returned to a client, the state machine sticks around for a short while in order to receive any outstanding responses. If stale versions were returned by any of the virtual nodes, the coordinator updates those nodes with the latest version. This is called *read repair* and it saves the Merkle Tree based anti-entropy process described above from having to do it later.

Since each write usually follows a read operation, the coordinator for a write is chosen to be the node that replied first to that read operation. The fastest replying node is stored in the context information of the request. This optimization is useful in edge case 99.9% situations because the node that gave out the read information is going to be the one doing the write.

.. _dynamo-durability:

NRW: Performance vs Durability
------------------------------
* N=Node replicas. How many nodes you want a copy of your object stored on.
* R=Read responses. How many nodes you want an answer from in order to consider your read successful (and consistent)
* W=Write responses. How many nodes you want an answer from who have successfully written your object

The common (N,R,W) configuration used by most Dynamo instances within amazon is (3,2,2). They will run this on clusters with a couple hundred nodes, spanning multiple datacenters (over high bandwidth, low latency interconnects). Since the R or W response is exchanged between the coordinator and the nodes, intelligent selection of nodes is required such that client facing requests do not have to span across datacenters.

Above was mentioned an in-memory buffer with persistent backing store. Certain services that use dynamo require higher performance at the expense of durability. To acheive this, dynamo provides the ability for each node to maintain an object buffer in its main memory. Each write operation is stored in the buffer and gets periodically written to storage by a writer thread. In this scheme, read operations first check if the requested key is present in the buffer, and if it is, it's returned without hitting storage. They saw factor of 5 decreases in latency at the 99.9% percentile during peak traffic even for a very small object buffer. The danger here is that if a node goes down, all the contents of its buffer (ie: a bunch of writes) are lost. To mitigate this, the write operation in this scenario is refined to direct at least one N node to perform a "durable write." The coordinator still only waits for W responses, so the client latency is not affected.

Partitioning
^^^^^^^^^^^^

.. image:: media/dynamo-partitioning.png
   :alt: Figure 7: Partitioning and placement of keys in three strategies. N=3 in this example. A, B, and C represent unique nodes that form a preference list for key k1. Black arrows are tokens (ie:virtual nodes).
   :align: center

   **Figure 7: Here we have an N=3 consistent hashing ring. A, B, and C are unique nodes which are part of the preference list for key k1. The arrows represent tokens. A, B, and C happen to be responsible for tokens beside each other.**

*Strategy 1: T random tokens per node and partition by token value:* In this model, when a physical node joins the ring, a random set of tokens are **created** at "random but uniform" locations around the ring and then these tokens are assigned to the new node. The token ranges are not perfectly uniform, and get worse when plopped down between existing tokens at various distances. All other existing nodes which have a new token plopped down between a token range they control then need to redefine their own token ranges. This also necessitates that they update their Merkle trees, and they also need to start up a background thread to scan their local storage and transfer the keys that they own over to the new node. This kinda sucks and takes a long time under high load.

*Strategy 2: T random tokens per node and equal sized partitions:* in this model, the hash space is divided up evenly while the token placement remains "random but uniform." Not sure about this one, seems dumb

*Strategy 3: Equal distribution of tokens and partitions:* Hash space is divided evenly, and a token is placed at each segment. This doesn't change. A node is added, and it steals tokens from other nodes in order to take on some load. Same happens if a node leaves. This strategy still has the problem of needing to transfer replicas in a background thread, but avoids merkle tree rebuilds and uneven hash sizes.

Encryption
==========

Alice and Bob
-------------
An analogy that can be used to understand the advantages of an asymmetric system is to imagine two people, Alice and Bob, who are sending a secret message through the public mail. In this example, Alice wants to send a secret message to Bob, and expects a secret reply from Bob.

Symmetric
^^^^^^^^^
With a *symmetric* key system, Alice first puts the secret message in a box, and locks the box using a padlock to which she has a key. She then sends the box to Bob through regular mail. When Bob receives the box, he uses an identical copy of Alice's key (which he has somehow obtained previously, maybe by a face-to-face meeting) to open the box, and reads the message. Bob can then use the same padlock to send his secret reply.

Asymmetric
^^^^^^^^^^
In an *asymmetric* key system, Bob and Alice have separate padlocks. First, Alice asks Bob to send his open padlock (*public key*) to her through regular mail, keeping his key (*private key*) to himself. When Alice receives it she uses it to lock a box containing her message, and sends the locked box to Bob. Bob can then unlock the box with his key and read the message from Alice. To reply, Bob must similarly get Alice's open padlock to lock the box before sending it back to her.

The critical advantage in an asymmetric key system is that Bob and Alice never need to send a copy of their keys to each other. This prevents a third party – perhaps, in this example, a corrupt postal worker – from copying a key while it is in transit, allowing the third party to spy on all future messages sent between Alice and Bob. So, in the public key scenario, Alice and Bob need not trust the postal service as much. In addition, if Bob were careless and allowed someone else to copy his key, Alice's messages to Bob would be compromised, but Alice's messages to other people would remain secret, since the other people would be providing different padlocks for Alice to use.

Alternate Asymmetric
^^^^^^^^^^^^^^^^^^^^
**In another kind of asymmetric key system** in which neither party needs to even touch the other party's padlock (or key), Bob and Alice have separate padlocks. 

First, Alice puts the secret message in a box, and locks the box using a padlock to which *only she has a key*. She then sends the box to Bob through regular mail. When Bob receives the box, he *adds his own padlock* to the box, and sends it back to Alice. When Alice receives the box with the two padlocks, she removes her padlock and sends it back to Bob. When Bob receives the box with only his padlock on it, Bob can then unlock the box with his key and read the message from Alice. 

Note that, in this scheme, the order of decryption is the same as the order of encryption – this is only possible if commutative ciphers are used. A commutative cipher is one in which the order of encryption and decryption is interchangeable, just as the order of multiplication is interchangeable (i.e. A*B*C = A*C*B = C*B*A). 

A simple XOR with the individual keys is such a commutative cipher. For example, let E1() and E2() be two encryption functions, and let "M" be the message so that if Alice encrypts it using E1() and sends E1(M) to Bob. Bob then again encrypts the message as E2(E1(M)) and sends it to Alice. Now, Alice decrypts E2(E1(M)) using E1(). Alice will now get E2(M), meaning when she sends this again to Bob, he will be able to decrypt the message using E2() and get "M". Although none of the keys were ever exchanged, the message "M" may well be a key (e.g. Alice's Public key). **This three-pass protocol is typically used during key exchange.**
Filesystems
===========

General
-------

A filesystem is something which defines how data is stored and retrieved. 

A filesystem typically stores all the metadata associated with the file - including the file name, length, and location within the directory hierarchy - separate from the contents of the file.

Most filesystems store the names of all the files which are in one directory in one place - the directory table for that directory - which is often stored like any other file. Many filesystems put only some of the metadata for a file in the directory table, and the rest of the metadata for that file in a completely separate structure, such as the inode.

Most filesystems also store metadata not associated with any one particular file. Such metadata includes information about unused regions - free space bitmap, or a B-Tree structure - and information about bad sectors. Often such information about an allocation group is stored inside the allocation group itself. More on this information in the filesystem-specific sections.

Features
--------

Journaling
^^^^^^^^^^


Write Barriers
^^^^^^^^^^^^^^
So as mentioned above, in journaling filesystems changes to filesystem metadata are first written to a journal, and then once that journal write succeeds, a "commit record" is added to the journal to indicate that everything else there is valid. Only after the journal transaction has been comitted in this fashion can the kernel do the real metadata writes. Should the system crash in the middle, the info needed to safely finish the job can be found in the journal.

The catch here though is that the filesystem code must, before writing the commit record to the end of the journal, be absolutely sure that all of the transactions information has made it to the journal. Just doing the writes in proper order (metadata journal update then commit record) is not enough. Drives and arrays these days have large internal caches and will re-order operations for better performance. As such, the filesystem must explicitly instruct the disk to get all of the journal data onto the media before a commmit record can be written. If the commit record gets written before the metadata, your journal can become corrupt.



XFS
---

XFS is a high performance *journaling* file system. It generally excels in the execution of parallel input/output operations, due to its *allocation group* based design. XFS is an *extent-based* filesystem.

XFS is internally partitioned into **allocation groups**, which are virtual storage regions of fixed size. Any files and directories that you create can span multiple allocation groups. Each allocation group manages its own journal, its own inodes, and its free space independently of other allocation groups. This is where it gains its advantage in both scalability and parallelism. If the file system spans many physical devices, allocation groups can optimize throughput by taking advantage of the underlying separation of channels to the storage components.

XFS implements **journaling** for metadata operations. XFS records file system updates asynchronously to a circular buffer before it can commit the actual data updates to disk. The journal can either be located internally in the data section of the file system, or externally on a separate device to reduce contention for disk access. If the system crashes or loses power, the journal is read when the filesystem is remounted, and replays any pending metadata operations. The speed of this recovery is not dependent on the size of the filesystem.

To reduce fragmentation and file scattering, each file's blocks in XFS can have variable length **extents**


Whilst many other filesystems manage space allocation with one or more block oriented bitmaps (see _`NTFS`), in XFS these structures are replaced with an extent oriented structure consisting of a pair of B+ trees (see TODO for more information on B-trees) for each file system allocation group.


Ref:
http://linux-xfs.sgi.com/projects/xfs/papers/xfs_white/xfs_white_paper.html
https://en.wikipedia.org/wiki/XFS

NTFS
----

We generally don't care about NTFS, but we did mention "free space bitmap" in the general section. You may also remember having to "defrag" your hard drive and were wondering why. While ext* also use free space bitmaps, NTFS's implementation gives the clearest explanation of this phenomenon.

NTFS uses a special $BitMap file to keep track of all the used and unused "clusters" (ntfs nomenclature for logical blocks: anywhere from 512b to 64KB) on an NTFS volume. It's a single file (a type of Free Space Bitmap as mentioned above) which contains a mapping of the entire volume, where when a file takes up space on the NTFS volume, the corresponding location in the $BitMap file is marked as used (11111...).  When data is removed, the corresponding map location is marked 0 as unused. When an OS is looking to write out a file, it scans this map file to see where the next available logical block is in which it can place this data, writes out a block, then continues on looking for the next free space available such that it can finish writing the file (if said file is larger than a single block). Even if the OS manages to write out the whole file in contiguous blocks, another file will be written immediately after it. If the original file grows at all, it becomes fragmented. We can see with this approach how quickly fragmentation can occur, and how unruly a $BitMap file can get on a large filesystem (64MB on a 2TB drive).

Here we can see the bitmap from an NTFS volume with a single 1MB file added:

.. image:: media/NTFS-BitMap.jpg
   :alt: NTFS $BitMap with 1MB file added
   :align: center

Note that ext3/4 also use bitmaps, but in regards to fragmentation they are a little bit smarter. Instead of placing multiple files near each other on the hard disk, ext3/4 will scatter different files all over the disk, leaving a large amount of free space between them. They will also choose free blocks around the (growing) file when writing out new data, rather than just the first free block they find. If a file does end up fragmenting, the file system will attempt to move the files around to reduce fragmentation (in ext4 at least). It does this automatically, without the need to run any utility. It can do this so long as the filesystem is not super full (80%+).


Ref:
https://whereismydata.wordpress.com/2009/06/01/forensics-what-is-the-bitmap/

.. PantsNotes documentation master file, created by
   sphinx-quickstart on Fri Nov 27 06:30:43 2015.

Welcome to PantsNotes's documentation!
======================================

This is a spot where various sysadmin related notes can be read in one place.

Contents:

.. Add paths/to/file (.rst not needed) to the toc
   OR you can just glob all files in current dir
.. toctree::
   :maxdepth: 2
   :glob:

   *


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

.. _interview:

Interview Material
==================

TODO: 
- Fix all ref links.
- Finish all questions and coding challenges

Facebook
--------
presreen q's system, networking, posix signals (done)
1st: 45min coding (phone), practical, relavent to sysadmin, some comp sci fundamentals.
2nd: 45min systems (phone)

(onsite) deeper coding, deeper systems, networking, architecture, collab/team building scenario questions

Study more of:
- memcached!!
- kill -9 will not remove a zombie process...why?
- Signals
- core dumps
- process states (scheduled? wait? runnig? sleep? uninterruptible?)
- make a section about kexec
- python:
  - input/output practice
  - interfacing with external proceses
- design: kleppmann's book

Questions
---------
- What is an initrd and why is it needed?
initrd stands for Initial RAM Disk. PXE/some_bootloader extracts and executes the kernel, and then the kernel can extract and mount its associated initrd. initrd provides a virtual root filesystem which contains several executables and modules that permit the real root filesystem to be mounted, or to do whatever else you'd like at that point in the boot process.

Nearly all of initrd's use cases involve needing to load some needed module into the kernel in order to support a non-standard root filesystem, LVM, network drive, or RAID controller prior to full boot. You may also choose to rebuild your kernel to include necessary modules and get rid of the initrd; however, in the instances of needing to mount a network drive, run some sort of process that doesn't need to boot fully into linux, or do some other "staging" type thing, an initrd environment is useful.

- Files, inodes, filesystems...how do they work? What are file descriptors? 
**Filesystems, inodes, file descriptors**: Described in :ref:`filesystems`

- What are semaphores? What is a mutex? What's the difference between the two?
A semaphore is best described as a signaling mechanism, but could also be described as a type of lock. A semaphore is an object that contains a (natural) number, on which two modifying operations are defined. One operation, V, adds 1 to the number. The other operation, P, removes 1. Because the natural number 0 cannot be decreased, calling P on a semaphore containing 0 will block the execution of the calling process/thread until some other thread comes along and calls a V on that semaphore. You may create a semaphore with more than one "slot" available (ie: s(6)). As such, semaphores can be used to restrict acess to a certain resource to a maximum (but variable) number of processes.

A mutex is a locking mechanism which helps multiple tasks serialize their access to a shared resource. It's simply some function or object you call prior to performing a block of code on some shared resource. Your first call sets a locked flag, then you run your code, and then your second call to the mutex releases the lock.

You might think of a semaphore or a mutex as a key to a bathroom. One key, one door works well for a mutex or a semaphore(1). But above we mentioned semaphores can count higher than one...so one key for six bathrooms. A mutex will not scale in this scenario as it would block each time 1/6 resources are used. A semaphore will allow 6 people to use the bathrooms at once, but it has no idea which bathroom is free at which time. What you end up needing is a separate mutex for each resource regardless. 

Because of this, you should not be relying solely on semaphores for locking - in fact, you really should only be using semaphores for simple signaling. For example, have a semaphore for power button which your display subscribes to (semaPend(sem_pwr_button); //wait for signal), such that when the power button is hit, a post (V, increment, semaPost(sem_pwr_button); //send the signal)) is sent and your display thread then unblocks and performs some_code. Another use could be naive throttling: only allow 3 threads to access a database at once.


- How are cookies passed in the HTTP protocol?
The server sends one of these in its response header (square brackets optional):
    Set-Cookie: <em>value</em>[; expires=<em>date</em>][; domain=<em>domain</em>][; path=<em>path</em>][; secure]

Note that "value" above is a string, and is almost always in a format like this: **key=value** , and is usually enforced as such.

And if the client accepts the cookie write, it sends this in its next request header:
    Cookie: name=value

- How does traceroute work? 
ICMP packets are sent, with the initial packet having a TTL of 0 and each consecutive packet having its TTL incremented by one. This elicits a response along each hop of a network path. The TTL count exists in the IP header.

Google Glassdoor
^^^^^^^^^^^^^^^^
- Rank the following in terms of speed: access a register, access main memory, perform a context switch, hd seek time
1 Register. 1 or 2 cycles. Smallest and fastest memory on a system. A compiler will typically allocate registers to hold values retrieved from main memory.
2 Perform a context switch (which type? assuming thread switch). 30-60 cycles best case.
3 Access main memory. NUMA local: 100 cycles NUMA remote: 300 cycles for no/normal congestion
4 HD seek time. A typical hdd needs anywhere from 2.5ms to 6.5ms to seek, depending on rotational speed (2ms=15k). Arm movement (stroke/track-to-track) takes anywhere from 0.2 to 1ms. SSD seek time is around 0.08-0.16ms

**Context Switch:** The process of storing execution state of a process or thread so that execution can be resumed from the same point at a later time.

    "Context Switch" can mean several different things, including: thread switch (switching 
    between two threads within a given process), process switch (switching between two 
    processes), mode switch (domain crossing: switching between user mode and kernel mode 
    within a given thread), and more. 

Which type of context switch you're talking about can mean a very different performance costs. For example, a context switch pausing one thread and the cpu scheduling another where each thread is not sharing memory (separate working sets) could dirty the cpu cache if there is not enough space to hold both thread's memory or the new thread fills the cache with new data. The same is true for processes. Additionally, if two processes share the same working set of memory and one is context switched out and another is scheduled in on a different core, it does not have access to the same cache/working set without a NUMA hop or a trip to main memory.

http://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html



- What information is contained in a file inode?
**Filesystems, inodes, file descriptors**: Described in :ref:`filesystems`



- How is MTU size determined?
MTU is referenced by packet (and frame) based protocols like TCP and UDP in order to determine the maximum size of packet it should construct for communication over a given interface. Something called **Path MTU Discovery** (PMTUD) is used in order to discover this value.

In IPv4, this works by setting the *DF* (don't fragment) bit in the ip header of outgoing packets. Any device along the network path whose MTU is smaller than the packet will drop it and send back an ICMP *fragmentation needed* message containing its MTU. The source host reconfigures appropriately, and the process is repeated.

IPv6 works differently as it does not support fragmentation (nor the don't fragment option). Instead, the initial packet MTU is set to the same as the source interface, and if it hits a device along the path where the packet size is too large for its MTU setting, that device drops the packet and sends back an ICMPv6 *Packet Too Big* message which contains its MTU. The source then reconfigures its MTU appropriately, and the process is repeated.

If the path MTU changes lower along the path after the connection is set up, the process still does its thing. If the MTU changes to a higher value, PMTUD will eventually discover this (Linux performs another PMTD check every 10 minutes by default) and increase MTU accordingly.

Some firewall operators will blanket deny all ICMP traffic. This means that after a TCP handshake happens and the first packet is sent out with a larger MTU than something along the link can handle, the firewall blocks the ICMP reply and you end up with a "black hole" connection where the source keeps retrying to send data and some device along the path keeps dropping it, with a blocked response. Some PMTUD attempt to infer this problem and lower MTU size accordingly, but the lack of response could also just be due to congestion.

Some routers may work around this issue by changing the *maximum segment size* (MSS) of all TCP connections passing through links which have an MTU lower than the ethernet default of 1500. While an MTU is concerned with the total size of a packet, MSS only determines the TCP Segment (minus TCP header) size - typical default = 536 Bytes.

[TCP Packet[TCP Segment[IP datagram[Data link layer Frame]]]]
[UDP Datagram[UDP Segment[IP datagram[Data link layer Frame]]]]

Also reference: :ref:`networking-mtu`


- Which system call returns inode information? (study all common system calls and know them)
**Kernel - System Calls**: :ref:`kernel-systemcalls`


- What are signals? What signal does the "kill" command send by default? What happens if the signal is not caught by the target process?

Check out :ref:`kernel-signals`

Signals are software interrupts. Kill sends a SIGTERM by default. The kernel delivers signals to target processes or process groups on behalf of an originating process, or on behalf of itself. If the originating process has the permissions to send a signal to another, the kernel forwards it on.

Note that processes can ignore, block, or catch all signals except SIGSTOP and SIGKILL. If a process catches a signal, it means that *it includes code that will take appropriate action when the signal is received*. If the signal is not caught, the kernel will take the appropriate action for the signal.

* SIGHUP hangup. Send this to a terminal and it will likely log you out. Other applications may instead use this signal as an indication to reload their configuration without terminating themselves.
* SIGINT is sent when you ctrl-c something. It is intended to provide a mechanism for an orderly, graceful shutdown of the foreground process. Interactive shells (mysql, other) may take it to mean "terminate current query" rather than the whole process.
* SIGQUIT signals a process to terminate and do a core dump
* SIGSTOP suspends a processes execution. If you are experiencing some sort of intermittent socket/buffer full or backflow buildup related bug, SIGSTOP is a good way to reproduce the issue. File handles will be kept open.
* SIGKILL is the ol' kill -9


- Describe a TCP connection setup
Look here: :ref:`networking-tcp`


- What happens when you type 'ps' (shell word splitting, searching PATH, loading dynamic libs, argument parsing, syscalls, /proc, etc. expand)
A variant of "the rabbit hole" question. :ref:`rabbithole`

- what is the worst case time for a quicksort?
O(n^2) for already-sorted lists if your pivot is the final element in the array you're sorting, or if all elements in an array are the same. Quicksort performance generally depends on your pivot. Look here: :ref:`algorithms`

- What is the maximum length(depth) of a binary tree?
http://codercareer.blogspot.com/2013/01/no-35-depth-of-binary-trees.html

Max depth is *n*, ie: unlimited. The maximum depth of a binary tree is the length of the longest path.

.. image:: media/compsci-binarytreedepth.png
   :alt: A binary tree with depth 4
   :align: center
 
In this image, we can see that the left subtree has a depth of 3 while the right subtree has a depth of 1. So long as the difference in depth between two branches is no greater than 1, it is considered *balanced*. Therefore, the binary tree depicted in this image is balanced.


- What is the theoretical best trans-continental round-trip ping time?
Light travels at just below 300,000KM/sec. Light travels through fiber around 30% slower, so 210,000KM/sec. London to NYC is about 5500KM. So, 5500/210000 = 0.026, or 26ms. Routers/switches only add microseconds of delay, so being generous, add 1ms total for both sides. So RTT = around 53ms. Verizon consistently sees 72ms between london and nyc in the real world.


- How do you solve a deadlock?
A deadlock occurs when multiple processes/threads must acquire *more than one shared resource*, or in the case of *recursive/self deadlock* where a thread tries to acquire a lock that it is already holding. Recursive deadlocks are the most common as they are easy to program by mistake. For example, if some function calls code to some other outside module which over some path ends up calling a function in the original module which is protected by the same mutex lock, then it will deadlock. The solution for this type of deadlock is to know your code path. Avoid calling functions outside the module when you don't know whether they will call back into the module without reestablishing invariants and dropping all module locks before making the call.

In the case where multiple shared resources are needing to be locked prior to performing an operation, if two or more concurrent process obtain multiple shared resources indescriminately a situation can occur where each process has a resource needed by another process. As a result, none of the processes can obtain all the resources it needs and as such all are blocked from further execution. Within a single application, deadlocks most often occur when two concurrently running threads need to lock the same two or more mutexes. The common advice for avoiding this type of deadlock is to always lock the two mutexes/resources in the same order: if you always lock mutex A before mutex B, then you'll never deadlock.


- Difference between processes and threads
*Processes are the abstraction of running programs*: A binary image, virtualized memory, various kernel resources, an associated security context, etc. A process *contains* one or more threads.

*Threads are the unit of execution in a process*: A virtualized processor, a stack, and program state. Threads share one memory address space, and each thread is an independent schedulable entity.

Put another way, processes are running binaries and threads are the smallest unit of execution schedulable by an operating system's process scheduler.


- What is a socket?
A socket is a way to speak to other programs using standard Unix file descriptors. When Unix programs do any sort of I/O, they do it by reading or writing to a file descriptor. A file descriptor is simply an integer associated with an open file. This file can be a network connection, a FIFO, a pipe, a terminal, a real on-disk file, or just about anything else! Read more about sockets here: :ref:`linux-kernel-sockets`


- What is a transaction (db)?
A transaction is simply a completed operation. Read more about RDBMS+ACID or NoSQL+CAP/Other here: :ref:`rdbms`

- What algorithm does python's .sort() use?
Timsort! Read more here: :ref:`algorithms-sorting` 

Facebook Glassdoor
^^^^^^^^^^^^^^^^^^
- What is a filesystem, how does it work?
A filesystem is a method of organizing data on some form of media. Read about specific filesystems here: :ref:`filesystems`
 
- What is a socket file? What is a named pipe?  
Read more about sockets here: :ref:`linux-kernel-sockets`. A named pipe is just a | that exists on a filesystem rather than only in your command line. Here are some cool things you can do with named pipes:

Create a pipe that gzips things piped to it and then outputs to a file:

  mkfifo my_pipe
  gzip -9 -c < my_pipe > out.gz &
  # Now you can send some stuff to it
  cat file > my_pipe

An example that is perhaps more useful is this:

  mkfifo /tmp/my_pipe
  gzip --stdout -d file.gz > /tmp/my_pipe # Decompress file.gz, send to my_pipe
  # Now load the uncompressed data into a MySQL table:
  LOAD DATA INFILE '/tmp/my_pipe' INTO TABLE tableName;

So what we did here was use a named pipe in order to transfer data from one program (gzip decompressing stuff) to another (mySQL). This allowed us to write out the entire uncompressed version of file.gz before loading it into MySQL, rather than having to decompress the whole thing first.

(RobertL: Data written to a pipe is buffered by the kernel until it is read from the pipe. That buffer has a fixed size. Portable applications should not assume any particular size and instead be designed so as to read from the pipe as soon as data becomes available. The size on many Unix systems is a page, or as little as 4K. On recent versions of Linux, the size is 64K. What happens when the limit is reached depends on the O_NONBLOCK flag. By default (no flag), a write to a full pipe will block until sufficient space becomes available to satisfy the write. In non-blocking mode (flag provided), a write to a full pipe will fail and return EAGAIN.)
 
- What is a zombie process? How and when can they happen?
When a process ends via exit, all of the memory and resources associated with it are deallocated; however, the process's entry in the process table remains. Its process status becomes EXIT_ZOMBIE and the process's parent is sent a SIGCHLD signal by the kernel letting it know that its child process has exited. At this point, the parent process is supposed to call a wait() in order to read the dead process's exit status and such. After the wait() is called, the process's entry in the process table is removed.

If a parent process doesn't handle the SIGCHLD and call wait(), you end up with a zombie process. Let run wild under high load, you may run out of PIDs. To get rid of zombie processes, you may try using kill to send SIGCHLD to the parent, and if that doesn't work, kill the parent. The zombie will become an orphan which is then picked up by init (1), who calls wait() periodically.

- What does user vs system cpu load mean?
Read more about the user and system separation in :ref:`linux-internals`.

- How can disk performance be improved?
Caching, sequential reads/writes, block/stripe alignment.

- Explain in every single step about what will happen after you type "ls (asterisk-symbol-redacted)" or "ps" in your terminal, down to machine language
Variant of :ref:`rabbithole`

- Suppose there is a server with high CPU load but there is no process with high CPU time. What could be the reason for that? How do you debug this problem?
Might be due to something causing high IOWait and not having associated higher cpu usage. If everything else is basically idle, this is usually an indicator that the disk/controller you're writing to is about to die. If there's a process at something really low, like 7% or something, then it could just be pushing a lot of data to slow media and not requiring much cpu time to do it. iostat will tell you what disk is being written to, and iotop will tell youwhich process it is. If you don't have these tools installed, look for processes that are in uninterruptible sleep:

  while true ; do ps -eo state,pid,cmd|grep "^D" ; sleep 1 ; done

Anything marked with a "D" at the start are in uninterruptible sleep, or a wait state. If you see a suspicious process, cat /proc/<pid>/io a couple times to see its io activity. You can also check lsof to see what file handles it has open.

  cat /proc/12345/io
  lsof -p 12345

If you're not seeing high IOWait, the high cpu is likely due to many very short lived processes stuck in a crash loop or doing some other thing it's not supposed to. atop shows you all processes which have lived and died over a polling period. Alternately, Brendan Gregg has a tool called execsnoop which he built for exactly this problem. If your kernel is new enough, you can use systemtap as well.

- What happens when a float is cast to/from a boolean in python?
If float == 0: bool = False ; else: bool = True  ? Not sure what more to say here
  
- Given a database with slow I/O, how can we improve it?
  - If relational, check out :ref:`rdbms` notes.
  - Profile the thing to see where it's slow (expand)
  - indexing (expand)
  - disk optimisations (expand)

- What options do you have, nefarious or otherwise, to stop people on a wireless network you are also on (but have no admin rights to) from hogging bandwidth by streaming videos?
  - discover their mac address (iwconfig wlan0 mode monitor;tcpdump), create another interface and assign their mac address as your own, make script to forever perform gratuitous ARP until offender gets annoyed at poor performance and stops using internet. (might also just be able to do arping -U ip.addre.s.s & echo 1 > /proc/sys/net/ipv4/ip_nonlocal_bind http://serverfault.com/questions/175803/how-to-broadcast-arp-update-to-all-neighbors-in-linux) 
  - If you can gain access to wifi router, ban their mac or set QoS if available
  - (expand)

- How exactly does the OS transfer information across a pipe?
:ref:`linux-internals-systemcalls`
Linux has an in-memory VFS called pipefs that gets mounted in kernel space at boot. The entry point to pipefs is the pipe(2) syscall. This system call creates a pipe in pipefs and then returns two file descriptors (one for the read end, opened using O_RDONLY, and one for the write end, opened using O_WRONLY).

In the case of unnamed pipes using bash, bash will call pipe(2) and get its file descriptors back, dup2 the pipe's stdin and first program's stdout, start up the second program and dup2 its stdin with the pipe's stdout.

The "left" program writes out, and that data ends up in a 64KB (usually) sized buffer, which is immediately read by the "right" program. Once the "left" program is done writing, it closes its fd, and the listening end then gets an EOF (read(2) will return 0). 

If the pipe is full, the write will block. If the pipe is empty, the read will block. It's possible to create FIFOs or socket files non-blocking, which will return an error if full.

- What problems are you going to run into when doing IPC (pipes, shared memory structures)?
In computer science, "Classic" IPC problems all refer to resource contention, or synchronization and deadlock problems. These are generally solved by using semaphores and mutexes, and ensuring access to multiple mutexes is done in order.
  https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem
  https://en.wikipedia.org/wiki/Readers%E2%80%93writers_problem
  https://en.wikipedia.org/wiki/Dining_philosophers_problem
  https://en.wikipedia.org/wiki/Sleeping_barber_problem

- What is "file descriptor 2"
STDERR. fd0 is STDIN, and fd1 is STDOUT. Read more in :ref:`linux-file-descriptors`

- What's the difference between modprobe and insmod?
Modprobe is more intelligent than insmod. It refers to a /lib/modules/$(uname -r)/modules.dep.bin (or other depending on OS) dependency file to read available modules and their dependencies. It'll accept the name of .ko files in /lib/modules/$(uname -r) as well. Insmod on the other hand will not do dependency resolution, and accepts paths to module files. Modprobe uses insmod to do its work once it does its more intelligent work.



Study Topics
------------
- Brush up on RAID
The RAID write hole problem can crop up when a power loss or some other event causes a disk write to be interrupted such that raid parity bits are not consistent with the data. Prevent this by using Battery Backed caches or PDUs, and/or consider storage systems that use transactional-style writes.

- learn the particulars of ssh
- core system functionality such as I/O buffering

- (googs)Prepare for Hashmap/hashtable questions

- (googs)Understand how job scheduling is handled in the most recent iterations of the kernel
:ref:`linux-internals-scheduling`

- (googs)Know your signals
See above, "What are signals?"

- (googs)Study up on algorithms and data structs
:ref:`algorithms`

- (googs)Study the book "Cracking the Coding Interview" for several weeks prior to interviewing. practice "whiteboarding" your code

- (fb)Review DNS, TCP, HTTP, system calls, signals, semaphores, complete paths (ie: telnet blah.com 80), boot process (incl UEFI)

- (fb)Refresh CCNA related knowledge, TCPDump commands (memorize syntax, memorize basic "listen"), ipv6 notes, load balancing types, load balancer failover modes & how VIP mac addr changes (gratuitous/unsolicited ARP), direct routing vs NAT, jumbo frames, MTU size, fragmentation and when it can occur, what a packet looks like

- (fb)Review systemtap, perftools, sar(sysstat), and other options

- (fb)Write about shared file systems which are read/written to from many servers.

- (fb)Write about distributed systems and different types of consistency models and where they are used


Design
------
Reference stackshare.io for ideas.

* (googs)How would you design Gmail?
* (googs)How do you best deal with processing huge amounts of data? (if you say map reduce, learn a ton about it)
* (fb)Outline a generic performant, scalable system. From frontend (lb's? or cluster-aware metadata like kafka) to backend (db's, storage, nosql options, etc). Remember networking as well: what features does a high performance network card supply - what can it offload? What should you tweak network wise for high bandwidth connections
* (fb)How would you design a cache API?

* (fb)How would you design facebook?
1) Define desired features, and split them into their own design. ie: photos, video, news feed, messenger, events

Video: Use GPU's to transcode streams to different quality levels

* (fb)How would you design a system that manipulates content sent from a client (eg: clean bad words in a comment post)?
For the clean bad words from a comment post, I would consider splitting the words in a comment post to a list (order matters here if we are to reconstruct the sentence, so can't sort or use a dict) and then iterating over them against a badwords hash/dict where the dict key is the badword. This would be O(n)+O(1), or an O(n) operation (this assumes that the hash function for the dict keys is sufficiently robust to make collisions uncommon, giving linear time O(1)).

If badwords is massive, you could consider keeping it sorted and then doing a binary search (O(log n)) against it when comparing words.

If the comment is huge or has many duplicate words and we want to prevent comparing them against the badwords more than once, you could consider splitting each word to a dict where the key is the word, and then doing a str.replace() on the original comment text if a badword match is found.

* Design the SQL database tables for a car rental database.
* How would you design a real-time sports data collection app?
* design a highly-available production service from bare metal all the way to algorithms and data structures. (eg: gmail, hangouts, google maps, etc.)

Coding Questions
----------------

Google Glassdoor
^^^^^^^^^^^^^^^^
- Implement a hash table
dicky = {'herp': 1, 'derp': 2, 'potato': 3}

- Remove all characters from string1 that are contained in string2
This is O(n), but inefficient as strings are immutable in python, so string1 keeps getting recreated.
  string1 = "can you still read this?"
  string2 = "aeiou"
  for char in string2:
    string1 = string1.replace(char, "")
  return string1

str.translate() in python uses C string magic, ends up being fastest at *removing* chars

Python 2 only:
  string1 = "can you still read this?"
  string2 = "aeiou"
  string1.translate(None, string2)

Python 3's translate method expects a translation table (ie: dict) passed to it which contains the unicode ordinal value of a character as the key, and an associated ord, str, or None as the value. maketrans() will create this table for us. Any characters entered in the *third* argument will be mapped to None, which is what we want in this case
  string1 = "can you still read this?"
  string2 = "aeiou"
  charTransTable = str.maketrans('', '', 'aeiou')
  # or
  charTransTable = str.maketrans('', '', string2)
  string1.translate(charTransTable)

- implement quicksort. Determine its running time.


- Given a numerym (first letter + length of omitted characters + last letter), how would you return all possible original words? E.G. i18n the numeronym of internationalization
- Find the shortest path between two words (like "cat" and "dog), changing only one letter at a time.
- Reverse a linked list
- Write a function that returns the most frequently occurring number in a list
- Do a regex to get phone numbers out of a contacts.txt file

Facebook Glassdoor
^^^^^^^^^^^^^^^^^^
- re-implement 'tail' in a scripting language
  - From lern/tail.py:

  import time
  import sys
  def main(f):
    # Go to end of file
    f.seek(0, 2)
    # Start printing from EOF minus 100 chars. If file not 100chars, start printing from beginning
    pos = f.tell() - 100
    if pos < 0:
      f.seek(0)
    else:
      f.seek(pos)
      # Using readline() and silencing output will cheaply get our file position pointer to the next
      # line without printing a truncated line to the user (as a result of seeking back an arbitrary
      # 100chars)
      silence = f.readline()
    while True:
      # The method tell() returns the current position of the file read/write pointer within the file.
      position = f.tell()
      # readline() reads a line and then advances the position read/write pointer on the file object
      line = f.readline()
      # If readline doesn't return anything, sleep for a second
      if not line:
        time.sleep(1)
      else:
        print line,
  if __name__ == '__main__':
    main(open(sys.argv[1]))


- Battleship game: write a function that finds a ship and return its coordinates.

- Write a script to ssh to 100 hosts, find a process, and email the result to someone
  for s in $(cat hosts) ; do ssh user@${s} "ps -ef|grep blah|grep -v grep|mail -s "This is the subject" user@myemail.com" ; done
  - For anything more complicated, or sent out to thousands of nodes instead of hundreds, I would consider trying out Rundeck instead of an ssh loop. Could also use salt, ansible, or mcollective if these are already in use in the org

- Write a function to sort a list of integers like this [5,2,0,3,0,1,6,0] in the most efficient way
  sorted(listy)
  - :ref:`algorithms-sorting`

- Given a sentence, convert the sentence to the modified pig-latin language: Words beginning with a vowel, remove the vowel letter and append the letter to the end. All words append the letters 'ni' to the end.

  sentence = "The origin of species is a detailed book requiring many hours of study"
  listsentence = sentence.split()
  pigsentence = []
  vowels = "aeiou"
  for w in listsentence:
    for v in vowels:
      if w[0] == v:
        w = w[1:]
        w += v
        break
    w += "ni"
    pigsentence.append(w)
  " ".join(pigsentence)


- take input text and identify the unique words in the text and how many times each word occurred. Edge cases as well as performance is important. How do you identify run time and memory usage?
  - Here's a rough script. Needs syntax removal and lower-casing everything:
  import sys
  def freqgen(wlist):
      dicky = {}
      for w in wlist:
          try:
              dicky[w] = dicky[w] + 1
          except KeyError:
              dicky[w] = 1
      # Let's convert the dict to a list of tuples (dicky.items()) 
      # for easier sorting. Key off the second value in each tuple
      for k, v in sorted(dicky.items(), key=lambda x: x[1]):
          print(k + " " + str(v))
  if __name__ == '__main__': 
      with open(sys.argv[1], "r") as f:
          bigstring = f.read().replace("\n", " ")
          wlist = bigstring.split()
      freqgen(wlist)
  - Runtime:
  python3 -m cProfile wordfreq.py short-story.txt
  - Check out runsnakerun for visualization of cProfile output, pretty cool. www.vrplumber.com/programming/runsnakerun/
  python3 -m cProfile -o out.profile wordfreq.py short-story.txt ; python runsnake.py out.profile
  - Memory. There is a module called memory_profiler that will output, line by line, how much memory your script uses:
  pip install -U memory_profiler
  pip instlal psutil #this is for better memory_profiler module performance
  vim freqgen #add @profile decorator above the function you're interested in
  python -m memory_profiler freqgen.py short-story.txt


- build a performance monitoring script, adding more features and improving efficiency as you go

- For a given set of software checkins, write a program that will determine which part along the branch where the fault lies. 
 - So we assume we already have a list of git revisions, and once a certain revision gets hit everything after it fails
 - Do a binary search in order to determine where the build starts breaking. Ie: pick the middle number, do a checkout, build, if fail then do another binary search in the middle of startrevision and failedrevision-1. If success, then do another binary search between successrevision+1 and finalrevision..etc etc. Do this until you find that failedrevision-1=a successful revision

- Given a list of integers which are sorted, but rotated   ([4, 5, 6, 1, 2, 3]), search for a given integer in the list. 
 - Think of the array as two separate lists. If number we're searching for is less than or equal to the last number in the array (3 in this case), then cut array in half and do a binary search on just that half until number is found

    For above questions, elaborate on theoretical best performance. Talk about 
    memory vs CPU usage. Talk about whether certain system calls take more 
    resources than others. How long it takes to: access a register, access main 
    memory, perform a context switch, hd seek time

General
^^^^^^^
- re-implement nc in python
- re-implement grep in python
- make a url shortener in python

Quickies
--------
Make immutable, can't delete this file:
    chattr +i filename

Special file being a douche to rm? eg: $!filename, -filename, 'filename-
    ls -i    #list by inode
    find . -inum 1234 -exec rm {} \;
IPv6
====

THE BITS
--------
- ipv6 is 128 bits long

  1011010010100110100100110010010001010001001000011110100101000101110100110100100011010010100011111010100101101011010010

- This is split into 8 groups of 16 bits, with a : every 16 bits

  1011010010100110:1001001100100100:....

- Now convert to hex. One hex value is 4 bits

  1011 0100 1010 0110 : 1001 0011 0010 0100 : ...
   B    4    A    6   :  9    3    2    4

- So that's the first 32 bits of an ipv6 address
- Most of the time the network is a /64

STRIP ZEROS
-----------
- Let's say we have leading 0's in our hex address. These get stripped off. You can include them, but other stuff will strip them off:

  003C:B47C:028D... --> 3C:B47C:28D....

- Here's another example. A host address, and what it gets shortened to:

  2001:DB8:21:111:0:0:0:1/64 --> 2001:DB8:21:111::1/64

- Any time we have consecutive zeros eg: 0:0:0:0, these can get shrunk to just ::.
- The router/other sees just 5 groups of 16 bits defined, and assumes there are 3 groups of 0's in place of "::"
- **You can only do this once per address**. For example 2001:DB8:0:0:15:0:0:2 --> 2001:DB8::15::2 is invalid. You can only use a single set of :: here, does not matter which group


LINK LOCAL
----------
- Link local addresses are used to do ipv6 related orchestration/control messages. These **will not route past directly connected devices**, and cannot be used by nginx/mysql/whatever else.
- The link local address is required for IPv6 sublayer operations of the Neighbour Discovery Protocol, as well as for some other ipv6-based protocols including dhcpv6
- If it begins with **"FE80::"** then it is a link local address. Almost always subnetted /64.
- Simply doing an "int fa0/0 ; ipv6 enable" on an interface will cause the interface to automatically generate a link local address. SLAAC is used to generate the address (more on that later).
- Link local addresses can talk within the interface's broadcast domain

EUI64 ADDRESSING
----------------
- Here's my linux ipv6 eth0 link local address:

  fe80::f6ce:46ff:fe2d:38e6/64

- The host portion of that is f6ce:46ff:fe2d:38e6. So how is that generated? You may notice that it's pretty close to your MAC address, but not quite. 
- Your mac is only 48 bits, but we need 64 to fill up the hostID portion of the address. So, we inject FFEE into the middle of the 48 bits. So 24bits of mac addr, FFEE, then 24bits of mac addr

  f4:ce:46:2d:38:e6  --> f4:ce:46:FF:EE:2d:38:e6

- Additionally, we come along and flip the 7th bit. If it was a 0, it flips it to 1, and vice versa 0 to 1. This is to conform with some MAC address standard
  - The 7th bit in a mac address is the "universal" bit. If 0, it means that the address was IANA assigned. It's 1 if the mac address was locally changed by an administrator. Assigning your own link local address to like, F80::1, breaks this RFC rule. Who cares? Nobody. It's a stupid rule that overcomplicates things, and continues to generate arguments

  f4:ce:46:FF:EE:2d:38:e6  -->  f6:ce:46:FF:EE:2d:38:e6  -->  f6ce:46ff:fe2d:38e6

- This process is the same should you choose to IP your hosts using EUI64, does not just apply to link local addrs. SLAAC does this automagically for us
- EUI64 only works with /64's
- So, you can manipulate what your link local address is going to be by changing your mac address. Additionally, cisco routers allow you to specify what you want your ipv6 local address to be:

  int fa0/0
  ipv6 address fe80::1

- This is perfectly fine and you might want to use it for small network segments, like the points between routers, just so it's much more readable
- But if you do screw up, how are duplicate IPv6 addresses discovered? Covered in Neighbour Discovery section below.


GLOBAL ADDRESSING
-----------------
- Global as in publicly routable
- Currently, global addressing assignments are 2xxx::/7 and 3xxx::/7. You should be able to hit up your ISP for a global block in one of these ranges.
- Once you have your global network block, IP'ing your hosts works just the same as described in the LINK LOCAL section. You can rely on EUI-64 and SLAAC to generate the HostID portion of your device's ipv6 IP, or you can just specify it yourself.
- Your host can generate its HostID, but how does it know what network it is a part of? Read the SLAAC section to see how this is figured out. In short, upon network boot, it queries a multicast group that your router is a part of and your router passes back a router advertisement containing network information.


NEIGHBOUR DISCOVERY
-------------------
https://www.youtube.com/watch?v=O1JMdjnn0ao

- **ARP and Broadcasts were REMOVED in v6!** The replacement is "Neighbour Discovery Protocol" (NDP), which communicates via Multicast.
- NDP just wants to translate ipv6 addresses into MAC addresses, and stick those translations into its neighbors cache (like arp cache)
- So, since there is no arp broadcast packet that hits everyone, we are using Multicast to discover our neighbors instead. Multicast still hits everyone, but a device only has to fully de-encapsulate the multicast message if the frame is destined for a multicast group it cares about
- If a server or something wants to contact a remote ipv6 address does not exist in its NDP cache, a multicast ICMP packet of type "Neighbor Solicitation" (NS) is sent to a multicast address group IP. This solicitation contains your link local source address, your mac address, and the remote target ipv6 address.
  - Wait, what multicast address group? It uses something called "Solicited Node Multicast Address Group"
  - Whenever you assign an ipv6 address to a host (either link local or global), that host is going to join a special multicast group based on the last 24 bits of its HostID
  - The multicast group IP always starts with FF02::1:FF. So, if you ping a host with link local address FE80::200:AAFF:FEAA:AAA, it actually sends a multicast message to FF02::1:FFAA:AAAA (where AA:AAAA is the last 24 bits of the hostID). Only hosts which share the last 24 bits of hostID end up processing the request and answering
  - Additionally, every host joins multicast group address FF02::1. This is essentially your multicast "broadcast" address that everyone listens on
- Once the solicitation hits the remote device, it responds (Unicast, directly now, to the sender's link local address) with a "Neighbor Advertisement" (NA) saying hey that's me, here's my mac addr etc
- Entry is saved into neighbor cache with link local address mapped to L2 mac address of remote device
- Entry changes to status "STALE" in cache after 30 seconds if no further traffic happens


Duplicate Address Detection
^^^^^^^^^^^^^^^^^^^^^^^^^^^
- Let's say we are trying to assign address 3333::3 to an interface.
- The process above, where a neighbor solicitation is sent out to the solicited node multicast address group, occurs automatically prior to binding a new ipv6 address to an interface. So, send out a neighbour solicitation to multicast address group ff02::1:ff00:3 , which is where any device that might already have 3333::3 as its address would be listening. This solicitation has source address :: because we don't want to use a source address that might already be in use
- If no neighbor advertisement is received from a device that already has that address, the device will send out its own neighbor advertisement to FF02::1 (ie: everyone) that it now has that ipv6 address. It also sends a message to ff02:16 stating that it's joining the ff02::1:ff00:3 group.
- If trying to assign a duplicate address, the neighbor solicitation happens (source address ::, destination group ff02::1:ff00:3 for example, and local source address is the IP you're trying to set), and then a device that already has the address responds with a neighbor advertisement to ff02::1 with its source address 3333::3 and its info, etc. Your device then should fail Duplicate Address Detection (DAD) and not assign the address.


Summary So Far
--------------
- FE80: Link Local
- 2xxx: Global Unicast (on interwebs today)
- 3xxx: Global Unicast (on interwebs today)
- FFxx: Multicast


ROUTING and STATELESS ADDRESS AUTOCONFIGURATION (SLAAC)
-------------------------------------------------------
- When you enable ipv6 routing on a cisco (conf t; ipv6 unicast-routing), it will automatically join the multicast broadcast group FF02::2 ("all routers" multicast group). When a host doesn't know where to send its packets, it will query the FF02::2 group to look for routers, and your router should respond.
- Once you enable routing, your router (cisco anyways) will send out a **"router advertisement"** (RAs) to the all nodes multicast group FF02::1 saying "hey i'm a router! I have network 2001:DB8:21:111::/64 on this interface". It does this every 200 seconds
- So when a client boots up and starts up an ipv6 interface, it sends out a **"router solicitation"** to FF02::2. This triggers an immediate router advertisement, which the router sends out to FF02::1. The client receives the router advertisement and **reads in the ICMPv6 Option Prefix Information section of the packet** that its network prefix is 2001:DB8:21:111::/64. It then takes this prefix, **generates its own ipv6 EUI-64 address** (based on its mac), and then **sets its gateway for that interface to the source address of the router solicitation.**

- Let's say that the router's IP is fa80::1 and the host's MAC address is 88:88:88:88:88:88. The host would therefore generate the following IPv6 interfaces and default gateway:

  fe80::288:88ff:fe88:8888  Its link local address
  2001:db8:21:111::288:88ff:fe88:8888  Its EUI-64 generated global address
  fe80::1  The router solicitation source address as its default gateway

- Some OS's might even create an additional ipv6 address which is totally random, like 2001:db8:21:111:149f:30ab:5e81:6166 or something, and then use that to communicate to the internet with. This is so that remote servers on the internet can't track you by your specific EUI-64 ID. Each time you restart the network, a new random one is generated.


STATELESS DHCP
--------------
- So our clients can come up, generate their own IP, and get a default gateway. What about knowing which DNS servers it should be pointed at? Welp, turns out there's an "Option" flag in your router advertisement telling the client that there is more information available. The client does its config from the router advertisement and then queries the router for extra options. 
- On the router (cisco anyways):

  conf t ; ipv6 dhcp pool MYPOOL ; dns-server 2001:DB8:21:5555::5
  int fa 0/0 ; ipv6 dhcp server MYPOOL ; ipv6 nd other-config-flag

- In addition to the Option flag, there is also a "managed" flag available in the router advertisement. With this toggled, it tells the client not to generate its own EUI-64 address, and to instead get its IP from your DHCPv6 server. In the above example, all we're using dhcp for is to get a dns server


"PRIVATE" ADDRESSING
--------------------
- Non-routable addressing exists in IPv6. These addresses are called "Unique Local Addresses (ULA)" and exist in the range fc00::/7. 
- Since you're not using NAT (just don't. Read on, and also Google why NAT+ipv6 is a shitty idea), any host which has these addresses will not be able to communicate to the internet. They'll hit their internet gateway and on a properly configured router, get dropped. So, the only place you'd use these is perhaps as secondary IP's which are used internally between devices you own. This would simplify firewall rules.
- The caveat here is that **you're making the rest of your network more complex by using private, non-routable addressing**. Web/other proxies to the internet? Port forwarding/DNAT? Double NAT situations? UPnP, strict NAT modes screwing with gamers and other p2p protocols? "But I need the source IP for business intelligence" forcing direct routing traffic shenanigans back through a load balancer rather than your actual router/firewall/whatever? It's all awful. **Get rid of it.** Private addressing and NAT are a HACK. They're patchwork workarounds and they have abused us long enough that a lot of people have obvious signs of stockholm syndrome convincing themselves that they should continue with this broken, less secure, exception laiden model of networking. A lot of people are recommending not to use ULA's at all, and instead giving everything a global addr and then relying on iptables/ipfw/other and an upstream router or firewall to filter or vlan traffic. The filter rules would be very simple, and make a lot more sense than securing your network via an overly complicated private addressing design.
- Though ULA's are supposed to be non-internet-routable, the RFC states that they should actually be globally unique. Not sure why this is, as anything with this IP cannot communicate to the internet anyways due to a non-routable source address.
- Anyways, if you want to make a private network, you could start with the following:

  fc00:0000:0000:0001/64  -(shortened)->  fc00::1/64  #network address
  fc00::1:288:88ff:fe88:8888/64  #EUI-64 hostID generated from MAC 88.88.88.88.88.88


ANYCAST
-------
- Let's say you have two DNS servers hanging off of different routers, perhaps in different geographical locations.
- We want both DNS servers to have address 2345::9/64, so we assign that IP to both dns servers. 
- Now upstream of each of these, we give each of our upstream routers an ipv6 address if they don't have one already. Additionally, you enable some sort of routing protocol on that interface (eg: ospf) which is connected to your dns server. After that, you assign 2345::/64 to the interface as well ('ipv6 address 2345::/64 anycast' works on cisco routers. The 'anycast' token disables DAD). This causes ospf on each router to also advertize that it has the 2344::/64 network
- Once this is complete, any router listening for routing advertisements will see an advertised route for 2345::/64 and choose (based on routing protocol) which path is best (shortest, AS, ospf metric, etc)


BEST PRACTICES
--------------
- Give all your devices a globally addressable address and rely on firewalls for security/blocking outbound where needed
- Avoid ULA's (private addressing) as they will introduce unneeded network complexity, if not in your network today, then in the future
- You're generally going to get a /48 from your uplink provider. Your "leaves" or final network segments should be /64's. This allows EUI-64 to work. Pay attention to whatever subnetting you do in between those, you only have 16 bits left for VLAN'ing
- Use a /127 for p2p, and a /128 for loopback
- Manually configuring much simpler addresses (.1 - .f) for core routing gear/load balancers is a good idea for simplicities sake
- **Avoid "encoding" schemes**, where you use vlan numbers or former ipv4 addresses or building numbers/geo information in your address bits. These things change and will eventually screw up your structure. You don't need to avoid like the plague, but just be aware that these schemes tend to be wasteful and break, and they cause people to make sub-optimal future design decisions such that these schemes don't break.
- Rely on DNS and SLAAC. IP memorization sucks. Predictable IP's are awesome. Stop wasting time with this "find the next available IP from an inventory that might or might not be up to date, then ping/check router/switch arp cache to ensure the IP isn't already in use" and "make sure the inventory is updated after every single IP change" BS.


GOTCHAS
-------
- Tools like ip need the -6 argument to do shit. eg: ip -6 r
- With ping6 on linux you need to specify the interface you'd like to ping from (when pinging link-local addresses anyways). Eg: ping6 -I 172-br0 fe80::1
- No cisco/have a linux or unix based router? "radvd" is the thing to use. Check it out
- On your linux/unix router, after you change your link local address (to something easier, like fe80::1) and remove your old one, restart radvd or it won't hand out adverts/gateway properly.
.. _kafka:

Kafka
=====

http://kafka.apache.org/documentation.html

If you're new to kafka, skip to :ref:`kafka-general`. If you just want to see a quick list of reference commands with regards to topic modification, performance testing, mirrormaker, monitoring setup, and maintenance, skip to :ref:`kafka-quicknotes`.

.. _kafka-design:

DESIGN NOTES
------------

OVERVIEW
^^^^^^^^
- Do edge processing. Cut down on inter-DC traffic
- Increase resiliency of data transport layer
- 150TB/day minimum
- Aiming for repl factor 2 with 24hrs retention


PARTITIONING
^^^^^^^^^^^^
- 3 node cluster on 1gig uplinks, no expansion expected, 6 partitions and repl factor 2 is a good default. Double or triple if your consumers are slow.
- 3 node cluster on 10gig uplinks, no expansion expected, 30 partitions and repl factor 2 is a good default. Double or triple if your consumers are slow or you have more consumers per group than num of partitions. 60+ partitions is better for (redacted) topics as they have a high rate of data.
- Linkedin:
  - Makes sure each individual partition is under 50GB over a 4 day retention period as they see spikey lag problems in mirrormaker when exceeding this limit. They sometimes see this problem even with 30GB partition sizes. We push more data than this (though probably less msgs/sec), and might have to consider using more than 60 partitions per BE topic.

  "As a note, we have up to 5000 partitions per broker right now on current
  hardware, and we're moving to new hardware (more disk, 256 GB of memory,
  10gig interfaces) where we're going to have up to 12,000. Our default
  partition count for most clusters is 8, and we've got topics up to 512
  partitions in some places just taking into account the produce rate alone
  (not counting those 720-partition topics that aren't that busy). Many of
  our brokers run with over 10k open file handles for regular files alone,
  and over 50k open when you include network."

- When choosing number of partitions, producer and consumer count and individual consumer bandwidth are of note. A minimum of ten 1gig consumers are needed to saturate a single topic from a single 10gig broker, therefore at least 10 partitions per broker would be needed in this scenario. This assumes your consumers can process at gigabit speeds!
- Upon broker failure / unclean shutdown, partition leader election takes around 5ms per partition. Multiply this by 1000 partitions and you've got 5 seconds of downtime + however much time the broker failure took to detect
  - Suppose also that the controller broker fails. A new broker needs to be elected, which will then look in zookeeper to read the metadata of every partition. This may take around 2ms per partition, so with 1000 partitions factor in another 2 seconds downtime + broker controller election
- Replicating 1000 partitions from one broker to another adds about 20ms of latency. If this is too much for your real-time application, use less partitions or share topic between a larger number of brokers
- Giant partitions are not really wanted as they take longer to migrate and work with administratively. If a topic is expected to grow to a very large size, increase partition count a bit (with respect to num of brokers)
- It's a good idea to keep partitions per broker below 4000. In general, the less partitions the better so long as you can keep your performance and per-partition size goals
- LinkedIn runs 31k+ topics with 350k+ partitions on 1100+ brokers (http://events.linuxfoundation.org/sites/events/files/slides/Kafka%20At%20Scale.pdf). This averages ~11 partitions per topic (!!unknown average cluster size!!)
- Ideally, in our publisher we want to use a partition key such that we can get common data onto the same partition. What can we key off of? This choice is determined by how we want to process the data on the other side. For example, if we partition by DSP and we have a samza/other job that wants to grab and calculate all metrics from a specific DSP, having that DSP exist on a single partition allows us to avoid a more costly multi-partition-consume operation.
  - It is possible to calculate partition key several times. ie: hash once for userID, hash a second time for adID, use result to choose partition
  - Key/hash is going to change if partition count for a topic changes. Keep this in mind when initially creating a topic, such that you don't have to increase partition count whenever you expand your broker count


TOPIC NAMING
^^^^^^^^^^^^
Ultimately, our topic naming should be something that our devs across teams can expect and understand, and something that makes sense when considering an aggregation of topics being sent from multiple sites down to a larger data processing site.

(redacted)

DELINEATING MESSAGES BY TIME
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(redacted)

3)
- Don't put time logic into kafka. Instead, Use 'timestamp' field in (redacted) message:
  - This makes it the consumers / data processing application's responsibility to deserialize and parse the protobuf message in order to see which day the data is part of
  - This is the most accurate timestamp because it is generated by the AE/BE/Other
  - This keeps our messages clean and consistent per topic. Beneficial for simpler monitoring and malformed message / message structure exceptions
  - Disadvantage is that you have to deserialize the message in order to know whether you would like to continue processing further messages

Option 3 suits Kafka's design philosophy.


COMPRESSION
^^^^^^^^^^^
- The java producer has a built in compression feature, and the java consumer has a complimentary built in decompression feature.
  - Compression is performed by the producer, who takes a handful of messages, compresses them, wraps them in a "compressed message set" and then publishes that message set as a single message to kafka. This message is differentiated by a magic byte set in its header, letting downstream consumers know that it is a compressed set
  - When the provided java consumer sees this magic byte as it's consuming, it will grab the message and transparently unpack it, and then output one message at a time from that message set
  - The compressed message set is its own message, it has one offset. Therefore, when the consumer consumes the message set, it will only advance its consumer offset position by one, even though the message set has several messages in it. This is expected; however, in the event of consumer failure and message set re-transmission, you may end up with more duplicate messages than you may expect.
- Most alternate clients i've seen (eg: kafkacat) offer compression/decompression per spec as well
- Java snappy has been extremely unstable in our usage. lz4 is faster and much more stable, but is currently unsupported by librdkafka (C) clients.


PRODUCING
^^^^^^^^^
0.8.2 Java producer (old)

- Publish new messages to a specific topic *and an optional partition* (use partitioner.class to define part. scheme)
- Threadsafe. You may share the same producer among your threads.
- The producer manages a single background thread that does I/O, as well as a TCP connection to each broker it needs to communicate with.
- Failure to close the producer after use will leak I/O resources
- When writing to Kafka, producers can choose whether they wait for the message to be acknowledged by 0,1 or all (-1) replicas.
  - Note that -1 means all current in-sync replicas...if an ISR is currently out of sync, publishes with acks=-1 will still succeed. If this is not wanted, you may specify the number of ISR's you'd like a response from (eg:acks=2) before the write is successful. Keep in mind that this will halt all writes if your acks number is higher than the available brokers.


CONSUMING
^^^^^^^^^
Deprecated section. 0.9 consumer has no separation of consumer. This section needs updating.

- SimpleConsumer: Allows you to read a message multiple times
- SimpleConsumer: Allows you to consume only a subset of partitions in a topic in a process
- SimpleConsumer: Allows you fine grained transaction control, allows making sure a message is processed 'just once'
- SimpleConsumer: You must keep track of your own offset to know where you left off consuming
- SimpleConsumer: You must figure out which broker is lead broker for a topic and partition
- SimpleConsumer: You must handle broker leader changes
- HighLevelConsumer: Don't care about handling message offsets. Stores last offset read from a specific partition in Zookeeper, stored under a Consumer Group name
- HighLevelConsumer: Kafka doles out partition assignments per thread connected to it under a certain consumer group. Your High Level Consumer should ideally have as many threads as there are partitions+replications
- HighLevelConsumer: If you have less threads than there are partitions, there will be no guarantee of ordering aside from sequential offset number


MIRRORMAKER
^^^^^^^^^^^
- Best practice is to keep mirrormaker local to the target cluster. This makes sense as consuming can be controlled by offsets, so upon network interruption it's not a big deal. Network interruption when trying to produce, however, results in message timeouts and loss.
- Target kafka clusters are termed "aggregate clusters". These clusters host an aggregate of topics from multiple other clusters in various DC's
- LinkedIn triple bolds and shouts that you should never ever ever produce to an aggregate cluster. This means that your mirrormaker processes should be the only ones ever producing to your aggregate cluster.
  - This means if your aggregation/main datacenter also provides the same services as your edge cluster (which all mirrormaker data back to your main DC for aggregation), you should also set up a separate "edge" cluster at your main datacenter and then mirrormaker that to your "aggregated results" kafka cluster even though it's local (ie: don't produce to your aggregate cluster)
  - This makes sense from an ETL perspective and just as an orderly segregation of services perspective. Not really a super show stopper but would be best practice.
- Mirrormaker usually needs to be more resilient than normal...might be able to afford smaller batches and acks=-1
- Messages are always going to be decompressed by mirrormaker and then recompressed upon publishing to the destination cluster. This is necessary for KeyedMessage and many other publishing features to work. There is ongoing work to perhaps provide the option to simply publish the compressed MessageSet onwards to the destination DC without decompressing.
- Partitions are not preserved! screws up your key based partitioning. You must use the same partitioner class at the mirrormaker publisher end to get accurate message placement
- If your aggregate clusters are where you really need the data, then your retention period on your remote clusters should only be long enough to cover mirrormaker (read: network) problems
- ***!!!Run a separate mirrormaker process for important topics!!!** If your main process fucks up, you don't want it affecting the high priority topics (ie: topics that are used for hourly reporting, or topics that change "live" search results etc)
  - You may also consider running a separate mirrormaker process for bloated topics, such that they won't affect the others
  - Less time to catch up if you fall behind


MONITORING
^^^^^^^^^^
- Ensure JMX options are set in kafka-run-class.sh included with package, or call those out in puppet. This is the line you need:
  bin/kafka-run-class.sh:  KAFKA_JMX_OPTS="$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=9999 "

- In addition to your standard process, disk, other server monitoring, you should be watching at a bare minimum your consumer offset lag time, as well as your broker cluster health (number of in-sync-replicas, replica lag time). It's preferrable to use a tool like Burrow in order to monitor your consumer lag, but you can also do it from included scripts.
  LAG=$(/opt/kafka/kafka_install/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group "${GROUP}" --zkconnect "${ZK}" --topic "${TOPIC}" | grep $TOPIC | awk -F' ' '{ SUM += $6 } END { printf "%d", SUM/1024/1024 }')

- [Running Kafka at Scale - Linkedin](https://engineering.linkedin.com/kafka/running-kafka-scale)
- Linkedin uses an internal "Kafka Audit" tool to ensure a message got from point A to point Z. So, every producer keeps track of how many messages it has produced over a certain time period. It then periodically send this count to an auditing topic. Additionally, each consumer they use also has a counter for how many messages it has consumed over a certain time period. It also periodically sends this count to an auditing topic. A separate consumer then consumes this audit topic, pushes the numbers to a DB, and there is a UI in front of the DB. They then can compare the #produced and #consumed messages, spitting an alert if there's a mismatch (duplicates, missing, etc).
  - Audit message content includes a timestamp and service+hostname header, start and end timestamp (for the produced messages it counted), topic name, tier (tier is on what tier the audit message was generated, eg: 0=remote_source_application, 3=mirrormaker_aggregate), and message count itself.
  - Concerns here are that since we're only counting messages, duplicate messages can cover message loss. It's also difficult to keep track of complex message flows


OPEN QUESTIONS
^^^^^^^^^^^^^^
- How much do we care about ordering?
- What is going to be pulling data from kafka? 
  - (edge processing, send aggregate results to LAS? What do we want to process at the edge?)
- Is all of our processing done in 24hr chunks?


BUGS & GOTCHAS
^^^^^^^^^^^^^^
- This one can sometimes happen if your consumer breaks from zookeeper at the right time. To fix it, reset the offset for your consumer group in zookeeper and delete anything spurious/temporary. Alternately, restart your consumers with a different consumer group ID.
  [2015-11-13 11:36:40,299] FATAL [mirrormaker-thread-1] Mirror maker thread failure due to  (kafka.tools.MirrorMaker$MirrorMakerThread)
  kafka.common.ConsumerRebalanceFailedException:
- You should avoid using zookeeper for your offset commits. Use 0.8.3 or newer and commit them to kafka instead.
- Snappy java sucks. If you're getting corruption errors or any type of failed produce errors, especially when restarting a broker, try using lz4 or no compression instead. Keep in mind librdkafka (ie: C-based) clients don't currently support lz4


NOTABLES
^^^^^^^^
- Kafka encourages large topics rather than many small topics
- A producer can choose a random partition to write to, but in a production system, you probably want to choose which partition to write to.
- Each partition must fit on one machine. Each partition is ordered. Each partition is made up of several log files..
- Each partition is consumed by only one consumer in a consumer group. Many partitions can be consumed by a single process though. You could have 1000 partitions consumed by a single process
- So, the partition count is a bound on the maximum consumer parallelism. More partitions means you can have more consumers, which means potentially faster consuming.
- Taken to an extreme, too many partitions means many files. This can lead to smaller writes if you don't have enough memory to properly buffer a batch of writes. If you have enough brokers, this shouldn't be a problem.
- Each partition corresponds to several znodes in zookeeper. Zookeeper keeps everything in memory, so beware
- More partitions means longer leader failover time. Each partition can be handled in milliseconds, but with thousands of partitions, this can add up
- The broker checkpoints the consumer position (as of 0.8.2) via an API call. It's stored at one offset per partition, so the more partitions, the more expensive the position checkpoint is
- It is possible to later expand the number of partitions; however, the broker will not attempt to reorganize data in the topic. If you are depending on this key-based semantic and numpartitions/numkeys changes and your data isn't in the expected place, you have to manually copy messages over to where you expect them to be
- Use a separate consumer connector per topic if feasible (https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Myconsumerseemstohavestopped,why?)

- TIME: Kafka allows getting the latest or earliest message offset by unix timestamp, but it does so at log segment granularity. So to get more accurate results you should use log.roll.ms rather than log.segment.bytes to roll your log segments. Be careful of how many files you create here.


FROM THE INTERNETS
^^^^^^^^^^^^^^^^^^
  ~~
  A few things I've learned:
  
  -Don't break things up into separate topics unless the data in them is truly independent.
  -Consumer behavior can be extremely variable, don't assume you will always be consuming as fast as you are producing. Don't assume the lag on all your partitions will be similar.
  -Keep time related messages in the same partition.
  -Design a partitioning scheme, so that the owner of one partition can stop consuming for a long period of time and your application will be minimally impacted. (for example, partitioning by transaction id). Knowing what data will end up on what partitions will allow you to differentiate between data you have and data you don't.
  ~~  

  ~~
  When the partitioning key is not specified or null, a producer will pick a random partition and stick to it for some time (default is 10 mins) before switching to another one. So, if there are fewer producers than partitions, at a given point of time, some partitions may not receive any data. Topic partition sizes may be lopsided for a time due to this as well.
  ~~


.. _kafka-general:

GENERAL
-------
- Kafka maintains feeds of messages in categories called Topics
- Processes that publish messages to a kafka topic are called Producers
- Processes that subscribe to topics are called Consumers

So,

  Producer ---> [kafka cluster(topic)] ---> Consumer

A "client" refers to either a producer or a consumer. There's lots of
clients out there, using various languages https://cwiki.apache.org/confluence/display/KAFKA/Clients

BROKERS
^^^^^^^
- A broker is an instance of kafka. It uses zookeeper to know about other brokers in its cluster.
- Each broker can be queried to find out information about the cluster, such as describing a topic, finding out your replication factor, which nodes are leaders for which partitions, etc.
- There is a "Controller" broker in each cluster which handles partition leader elections, updating zookeeper, etc
- Read more in the "CLUSTERING" section

TOPICS
^^^^^^
- A topic contains one or more partitions. Partitioning data allows for dataset sizes larger than a single server.
- Ordering guarantees are provided on a per-partition basis. When a topic is split into partitions, the only guarantee on ordering is within the partitions themselves.
- Total ordering guarantees (within kafka, without requiring external client logic) can only be accomplished by using topics with a single partition, and a transactional publishing model
- You can either create a topic (and define the number of partitions you want) with kafka-topics.sh
or some other script, or alternatively you can configure your brokers to auto-create topics when a non-existent topic is published to
- Each published message is added to the end of a partition and assigned a sequential id called the offset (unique)
- The kafka cluster retains all published messages for a configurable period of time (or size limit). Consuming a message does not remove it
- A Consumer can consume any (offset)message it likes, in any order

Topic Creation Example: 

  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 1 --topic test
  bin/kafka-topics.sh --list --zookeeper localhost:2181

PARTITIONS
^^^^^^^^^^
- Number of partitions is defined upon topic creation, as is number of replicas
- partitions are distributed across servers in a kafka cluster. Kafka does this itself upon topic creation
- You can run bin/kafka-create-topic.sh on one of your brokers, specifying number of replicas and number of partitions. Alternatively, kafka can auto-create topics when a non-existant topic is published to. In this case, it uses defaults for numreplicas and numpartitions.
- Partitions are represented as a set of log files on disk. You can configure how big you want your log files to be before new writes go to a new log.
- When you define a maximum topic size or maximum age, these values apply to your partitions, not the topic as a whole. Once a partition log file hits a certain age, or the maximum partition size is reached (size of all log files added together), the oldest log is deleted from disk.
- Each partition can be replicated across a configurable number of servers. You end up with one "leader" for that partition, and zero or more "followers." If a leader dies, a follower picks up.
- Each server acts as a leader for some of its partitions and a follower for others
- The controller detects broker failures and elects a new leader for each affected partition. The detection is done via an internal broker RPC, not via zookeeper. The controller will however update zookeeper after it has chosen a new leader for an affected partition.
- Read rollout-notes for an idea of how many partitions you should choose for your topics

PRODUCERS
^^^^^^^^^^
- Producers publish data to the topics of their choice. They query kafka brokers to get metadata about what topics, partitions, and replicas are available. (future producers: just point them at two or three kafka's to grab initial bootstrap data, they'll use that data to get the rest of the brokers in a cluster)
- Kafka comes with an example producer, kafka-console-producer.sh, which accepts stdin(per-line), or raw input (run it, then type stuff in)
- The producer is responsible for choosing which message to assign to which partition within the topic. This can be done round robin, or according to some function (like a hash on a key in the msg)
- The producer can query kafka to see if it has an existing topic, or it can choose to create its own topic. It can pass how many partitions, replicas etc it wants for this new topic, or let kafka use its defaults.
- When writing to Kafka, producers can choose whether they wait for the message to be acknowledged by 0,1 “all” (-1) *in-sync* replicas, or x number of replicas. Note that -1 means all *current in-sync replicas*...if a replica is currently out of sync, publishes with acks=-1 will still succeed. If this is not wanted, you may specify the minimum number of ISR's you'd like a response from (eg:acks=2) before the write is successful. Keep in mind that this will halt all writes if your acks number is higher than the available brokers.

CONSUMERS
^^^^^^^^^^
- kafka-console-consumer.sh comes with kafka. It'll dump messages to stdout
  bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
- Consumers label themselves with a consumer group name
- Consumers read data from topics. Kafka maintains a consumer group list, and assigns a partition in a topic per consumer in a consumer group.
- Each partition in a topic is delivered to *one* consumer instance (thread) within each subscribing consumer group. 
  - This allows for multiple consumers in a consumer group to process a topic in parallel, because only one of them is consuming each partition.
  -This means there should not be more consumer instances (in the same consumer group) than there are partitions, extras are left idle
  - Multiple consumer groups can subscribe to the same topic. You can put every consumer into its own consumer group if you want.
- One consumer per partition model allows kafka to ensure the ordered delivery of a partition.
  - If you need total ordering guarantees (ie: the entire topic, not just a partition in the topic), you have to make a topic with a single partition (and thereby a single consumer)...OR some sort of ordering inside your data itself, which something can re-order once all the consumers pull it together.
- As of 0.8.2, kafka provides an API call that allows consumers to publish their checkpoint offset. 
  - Internally, the implementation of offset storage is just a compacted kafka topic (__consumer_offsets) keyed on the consumer's group, topic, and partition
  - The offset commit provides acks=-1 durability
  - Kafka maintains an in-mem view of this tuple: <consumer group, topic, partition>, so offset fetch reqs can happen very quickly without the need for scanning the compacted __consumer_offsets topic
  - Previously, it was the consumers responsibility to push its current offset location to zookeeper

GUARANTEES
^^^^^^^^^^
- Messages sent to kafka by one producer to a particular topic partition will be appended in the order they are sent, ie: the first sent message will have a lower offset in the log than the second sent message. This is a guarantee built into your producer.
- A consumer sees messages in the order they are stored in the log
- For a topic set to replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log. Eg: replication factor 3 can withstand 2 server failures.

CLUSTERING
^^^^^^^^^^
- kafka nodes find out about each other via zookeeper (zookeeper.connect= in your server.properties). By default, all kafka nodes who are connected to the same zookeeper cluster will be part of the same cluster.
  - To set up multiple kafka clusters connecting to the same zookeeper cluster, you must define a 'chroot' address on your zookeeper.connect line. eg:
zookeeper.connect=hostname1:port1,hostname2:port2,hostname3:port3/chroot/path
  - Note that you must create this chroot path yourself prior to starting up your kafka cluster
  - this will put that kafka's nodes/leaves under its own path, thereby separating your cluster out from the main path.
  - If you are running chroots as above (*and a kafka version less than 0.8.2*), you also have to configure your consumers to use the appropriate chroot path. In consumer.properties it's the same zookeeper.connect line.
- After zookeeper discovery, each kafka cluster designates a "controller". It will be responsible for:
  - Leadership change of a partition (though each leader can still independently update the ISR list)
  - New topics; deleted topics
  - Replica re-assignment
  - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Internals
- Broker failure detection is done via direct RPC, internal kafka functions - not zookeeper
- Controller death could be detected by controller.socket.timeout.ms configured on each broker
- Communication between the controller and the rest of the brokers is done through RPC
- The controller is the one that commits changes (new topics, leadership changes etc.) to zookeeper

COMPACTION
^^^^^^^^^^
http://kafka.apache.org/documentation.html#compaction
- You have the option to either compact or delete log segments once you hit either a certain time limit, or size limit. The idea is to selectively remove records where we have a more recent update with the same primary key.
- can be set per topic at creation, or using alter topic. Default log.cleanup.policy is delete.
- Let's say you're pushing messages to kafka that look like this "uid123 herp=derp", "uid123 herp=lerp", "uid123..." etc etc. You're updating "the same value" over time, perhaps it's a user updating his/her new email address or info. Log compaction will look at a unique key in your logs(messages) and then remove all but the latest update related to that key.
- Compaction allows consumers who are unable to keep up with the log (or crashed) to see the last value a key was set to
- You can set your key's value to 0 to mark that key for deletion (happens 24hrs later by default)
- compaction is not compatable with compressed topics
- your message offset never changes. Some get deleted, the newest stays, and the offset stays the same
- Take note of log.cleaner. options such that performance is not an issue
- Be aware that your topic is going to continue growing until you intervene, so be aware of how many primary keys you have and your primary key growth

API DEETS
^^^^^^^^^
**DEPRECATED NOTES AS OF 0.9**. The following info is mostly deprecated as of 0.8.3 (0.9), but still gives an idea of how things work.

**Producer-API**

The Producer API wraps two low level producers. These are kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer. The goal is to expose all the producer functionality through a single API to the client.
The API provides the ability to batch multiple produce requests (when procer.type=async). As events enter a queue, they are buffered until queue.time or batch.size is reached. Then a background thread kafka.producer.async.ProducerSendThread dequeues the batch of data and lets kafka.producer.EventHandler serialize and send the data to the appropriate broker partition. Monitoring/stats can be done by implementing kafka.producer.async.CallbackHandler to inject callbacks at various stages.
The API also provides software load-balancing between partitions via an optionally user-specified kafka.producer.Partitioner. Otherwise, you can provide a key. If you do not include a key, kafka will assign the message to a random partition. You may also define your own partition.scheme to key off and direct certain data to certain partitions.

**Consumer-API**


There's two levels. The "simple" API maintains a connection to a single broker and has a close correspondance with the network requests sent to the server. It's stateless and has the offset passed in every request, which allows the user to maintain metadata however they choose.
The "high-level" API hides the details of brokers from the consumer. It allows a consumer to consume off a cluster of machines without concern of the underlying topology. It also maintains state of what has been consumed. It also allows you to subscribe to (or ignore) topics based on a regex.
- SimpleConsumer: Allows you to read a message multiple times
- SimpleConsumer: Allows you to consume only a subset of partitions in a topic in a process
- SimpleConsumer: Allows you fine grained transaction control, allows making sure a message is processed 'just once'
- SimpleConsumer: You must keep track of your own offset to know where you left off consuming
- SimpleConsumer: You must figure out which broker is lead broker for a topic and partition
- SimpleConsumer: You must handle broker leader changes
- HighLevelConsumer: Don't care about handling message offsets. Stores last offset read from a specific partition in Zookeeper, stored under a Consumer Group name
- HighLevelConsumer: Kafka doles out partition assignments per thread connected to it under a certain consumer group. Your High Level Consumer should ideally have as many threads as there are partitions+replications
- HighLevelConsumer: If you have less threads than there are partitions, there will be no guarantee of ordering aside from sequential offset number

CONSUMER OFFSETS
^^^^^^^^^^^^^^^^
- With 0.8.2 (0.8.3 really) comes the ability to do broker-committed offsets. This means that the consumer api has the option to commit your offsets to a topic called __consumer_offsets on your brokers, rather than having to use zookeeper. There's a bunch of new config options for this - check the docs for offsets.*
- The topic is created as soon as the first consumer using the new API commits an offset to kafka. This topic is created with 50 partitions, repl factor 3, and log compaction on by default
- Commits to __consumer_offsets by the consumer API use groupID, Topic, and PartitionID to key off of in order to determine which partition of __consumer_offsets to save the commit message to.
- The default offsets.topic.num.partitions=50 seems quite high, and 100-200 partitions are "recommended" for production. 
  - Keying offset commits across many partitions may mildly improve performance of consumers looking up offset values due to smaller partition sizes, but I'm not sure this justifies the extra 5ms-per-partition, 2ms-per-partition leader initialization time, and replica thread / other used resources. 
- This feature may be buggy. We're currently seeing a random, some brokers but not others, 3300% idle increase in CPU utilization with __consumer_offsets @ 50 partitions and repl 3 on a 3 node cluster. Not sure yet how it's related

MESSAGES
^^^^^^^^
- Messages consist of a fixed-size header and a variable-length opaque byte array payload. The header contains a format version, and a CRC32 checksum to detect corruption or truncation. Put whatever you want in the payload, kafka doesn't care.
- Header has:
  1 byte magic identifier to allow format changes
  {{ONLY if magic byte set}} 1 byte "attributes" identifier (eg: compression enabled, type of codec used)
  4 byte CRC32
  N byte payload

LOG
^^^
- A topic like 3p2r (3 partitions, repl factor 2) is going to have three directories on disk - 3p2r_0 3p2r_1 3p2r_2. If you've got three brokers, one partition leader may have 3p2r_0 while another will have _1. A replication partner may also have a 3p2r_0 directory even though it's not the leader for it, because it's a replication partner.
- Two files are saved. An .index file, and a .log file. The .index file contains 64-bit integer offsets giving the byte position of the start of each message in a .log file. It's a mapping of offset to physical location. The .log file holds the actual data.
- The format of the log files saved to disk is a sequence of log entries. The format for a log entry is the same as above, but has a message length integer:
  4 byte message length integer (1+4+n)
  1 byte "magic" value
  4 byte crc
  n byte payload
**::Writes**
- A log file will grow to a certain size (default 1GB), and then begin writing to separate .index/.log files. Each log file is named with the offset counter of the first message it contains. So, if you're saving 64KB messages and rotating at 1GB, that's around 15625 messages per. It takes a bit for kafka to notice and rotate, so typically you're going to have log files a bit larger than 1GB and having more than your expected # of messages per log file. You may end up with 00000000000000000000.index and then 00000000000000016767.index with 64KB files @ 1GB rotation. So, your first .log file has 16766 messages in it.
**::Reads**
- Give the offset (ie:counter) and the max chunk size (intended to be larger than any single message could be, but in the event of an abnormally large message, retry the read doubling the buffer size each try until the message is read successfully).

BIG SLOW DISK IS GOOD
^^^^^^^^^^^^^^^^^^^^^
http://kafka.apache.org/documentation.html#persistence
- It's common perception that disks are slow and should be avoided. However, because of kafka's sequential nature, it can take great advantage of the operating systems disk read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes.
- Additionally, building on top of JVM means having to use java's ... memory management. The memory overhead of objects is very high, often doubling the size of the data stored (or worse). Additionally, Java garbage collection becomes increasingly fiddly and slow as the in-heap data increases.
- So what to do? Write the stuff out to a file on disk immediately. It just gets written to your pagecache, which is going to take up way less space than in mem java. Data is also stored as a compact bytes structure rather than individual objects. Also, the cache stays warm across process reboots rather than being cleared. And when data needs to be retrieved, you get a direct pointer to the data in memory. You also end up not having to worry about "in memory" vs "on-disk" information, now it's the OS's job.

NOTEABLE CONFIGURATION NOTES
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
http://kafka.apache.org/documentation.html#configuration
- message.max.bytes (1000000) : max size of a message the broker can receive from a producer. Synch this with the max fetch size your consumers use. You don't want messages too large for your consumers. Also keep in mind possible batched+compressed MessageSets, which may be many times the size of a single normal message!
- num.io.threads (8) : set this to as many threads as you have disks
- queued.max.requests (500) : number of requests that can pile up waiting for disk I/O before the network threads stop reading in new requests
- socket.send.buffer.bytes : socket.receive.buffer.byes (100*1024) : SO_SNDBUFF and SO_RCVBUFF respectively
- log.retention.{minutes,hours} (7d) : retention period. If both this and log.retention.bytes are set, it will do both. Whichever is hit first.
- log.retention.bytes (-1) : how big a single log (ie: partition) can grow before a log segment is deleted/compacted (topics are split into partitions (aka:logs), and logs are split into segments (default 1gig segments). Deletions happen to entire segments at once.
- log.flush.interval.messages (None) : sets your fsync time before messages in memory are sync'd to disk. It is strongly recommended to rely on replication rather than setting an fsync time, as this has a huge performance impact.
- default.replication.factor (1)
- replica.lag.time.max.ms (10000) : If a follower hasn't sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.
- replica.lag.max.messages (4000) : If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.
replica.fetch.max.bytes (1024*1024) : how many bytes (per partition) to try to grab in one fetch to the leader
- num.replica.fetchers (1) : number of replication threads to the leader. Increasing this can increase the degree of I/O parallelism in the follower broker
- controlled.shutdown.enable (false) : This makes the broker attempt to transfer partition leadership to other brokers prior to shutting down. Speeds up unavailability window during shutdown. *WARN* Ensure your init script properly waits and terminates all child java processes prior to returning success, else restart will fail */WARN*
- auto.leader.rebalance.enable (true) : If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the "preferred" replica for each partition if it is available.
  - leader.imbalance.per.broker.percentage (10) : The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.


DEPLOYMENT NOTES
^^^^^^^^^^^^^^^^
- Upon node failure, kafka doesn't know whether it'll be permanent or not. Maybe you want to replace it with a new broker, or make another existing broker part of the replication pool for that topic. The preferred method upon broker failure is to bring up a new broker (if it's a total replacement) with the same broker.id as your failed box. This way when you start it up it'll join, notice that it's way off, and start automatically replicating/recovering.
  - Other cluster resizing or rebalancing operations must be done manually, using bin/kafka-preferred-replica-election.sh or kafcat (not kafkacat, kafkat), or yahoo's kafka-manager
- kafka includes a bin/kafka-{producer,consumer}-perf-test.sh. This client utilizes batch processing, and can run multiple threads. It automatically counts for you, can push a specified # of messages, etc. Using this, we can easily max a gigabit link on each node of a 3-node kafka cluster using 64KB message sizes. We can also get around 1.2million 100byte publishes/sec out of this hardware from 3 2008 crappy blades.
- The java producer that comes with kafka has a compression.codec option. Enabling compression will cause the producer to buffer up a set of messages, wrap them in an eventMsg, compress that whole eventMsg, then send that message as a whole to your broker. This message's header will have its magic byte set along with a flag saying that this message is compressed. On the other side, the java kafka consumer has an iterator that can automatically detect these compressed eventMsgs, decompress the eventMsg, and then spit out each individual message.
- There's a nice little statically linked C client called kafkacat that is extremely useful for quick producing/consuming/troubleshooting. It accepts input from stdin if acting as a producer, or outputs to stdout if acting as a consumer. It also supports compression, and will automatically decompress MessageSets when consuming


STUFF TO WATCH
--------------
This section describes a list of kafka related tools and such to keep an eye on.


PRODUCERS/CONSUMERS
^^^^^^^^^^^^^^^^^^^
  [Kafkacat](https://github.com/edenhill/kafkacat) :: 'netcat' for kafka. Lightweight statically-linked producer/consumer client. Takes stdin and consumes to stdout. Extremely simple, versatile, I use it almost exclusively. librdkafka does not support lz4 at the moment.
  [Flasfka](https://github.com/travel-intelligence/flasfka) :: Expose kafka over python's simple web framework flask(http). Pretty neat-o. Limitations: utf-8 input only. Encode to base64 if you need to push arbitrary data to it. We use this for twittershitter
  [Bruce](https://github.com/tagged/bruce) :: C client that sits on a host and listens to a unix dgram socket. Your app then only has to send info in a simple binary format to a /dev/kafka_bruce. Saves devs from having to implement kafka API client code in their codebase.
  [Stealthly Syslog to Kafka](https://github.com/stealthly/go_kafka_client/tree/master/syslog) :: syslog->kafka producer, per SRD-29. Works fine
  [omkafka for rsyslog](http://www.rsyslog.com/doc/master/configuration/modules/omkafka.html) :: omkafka, builtin module for rsyslog to publish to kafka. Likely our best bet for syslog->kafka
  
  Choice of a client will ultimately be up to our devs:
  [Kafka Clients](https://cwiki.apache.org/confluence/display/KAFKA/Clients)


MONITORING RELATED
^^^^^^^^^^^^^^^^^^
  [Burrow](https://github.com/linkedin/Burrow) :: Consumer lag checker from linkedin. If you use nothing else, use this.
  [jmxtrans](https://github.com/jmxtrans/jmxtrans) :: Crappy but fast JMX monitoring tool, but has built in writer for openTSDB and many others. Will periodically poll for stats from mbeans exposed via JMX
  [collectd](https://collectd.org) :: Use collectd with jmxtrans or whatever else
  [tjconsole](https://github.com/m-szalik/tjconsole) :: Text jconsole! Discover what metrics are available to you over your JMX interface.
  [Dropwizard](https://github.com/dropwizard/metrics) :: The big main java metrics monitoring suite. Kafka uses this package to expose its stats
  [Logstash JMX Poller](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jmx.html) :: Logstash has a jmx poller. This is much preferable to jmxtrans so long as it is able to scale
  [kafka-web-console](https://github.com/claudemamo/kafka-web-console) :: Web interface for monitoring kafka. Might be worth a look.
  [DCMonitor](https://github.com/shunfei/DCMonitor) :: Web interface for monitoring kafka. Might be worth a look.

ADMIN TOOLS
^^^^^^^^^^^
  [kafkat](https://github.com/airbnb/kafkat) :: Not to be confused with kafkacat. Simplified command line administration for kafka. Anything is better than those shit awful json-script-using kafka builtin tools...so check this out
  [kafka-manager](https://github.com/yahoo/kafka-manager) :: an admin tool from yahoo. Can handle partition migration and other stuff. Worth a look


.. _kafka-quicknotes:

QUICKNOTES
----------
These were written for kafka 0.8.2 and as such may not work with 0.9. Reference kafka documentation for updated versions of these commands if they do not work.

TOPIC CREATION, METADATA, TOPIC DELETION, ETC
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Keep in mind all of these settings have defaults in server.properties. You only need to define these topic creation settings if you want to stray from the defaults.

  # Create some topics. 86400000s is 24hrs. USE THE .ms CONFIG OPTION!! .m and .h are always broken/breaking/not honoured (even in 0.9)
  ./kafka-topics.sh --zookeeper srv1003:2181 --create --topic 3p1r --partitions 3 --replication-factor 1
  ./kafka-topics.sh --zookeeper srv1003:2181 --create --topic 3p2r --partitions 3 --replication-factor 2 --config max.message.bytes=6553600 --config segment.bytes=8589934592 --config delete.retention.ms=86400000
  # Check for your newly created topics
  ./kafka-topics.sh --describe --zookeeper srv1002:2181
  # Alternately, you can use kafkacat 
  kafkacat -L -b srv1005
  
  # You need this enabled to delete topics. Enable it and restart all your brokers:
  #   server.properties:delete.topic.enable=true
  ./kafka-run-class.sh kafka.admin.TopicCommand --zookeeper 192.168.100.102:2181 --delete --topic 3p1r
  # Alter an existing topic.
  ./kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config max.message.bytes=128000 --config retention.bytes=6000000000000
  ./kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --deleteConfig max.message.bytes

PRODUCER/CONSUMER RUNNING + PERFORMANCE TESTING
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  #Test multithreaded, 100byte msg default. Sync by default (acks=-1).
  ./kafka-producer-perf-test.sh --broker-list srv1005:9092,srv1007:9092,srv1008:9092 --messages 50000000 --topics 3p1r --threads 16
  #64KB message size
  #REQUIRES: Modify perf-test.sh KAFKA_HEAP_OPTS="-Xmx24576M" , or whatever you RAM situation permits
  ./kafka-producer-perf-test.sh --broker-list srv1005:9092,srv1007:9092,srv1008:9092 --messages 100000 --topics 3p1r --threads 16 --message-size 64000 --batch-size 50
  
  ./durr.sh | kafkacat -b srv1005 -t omgwtfbbq & CHILD_PID=$! ; sleep 5s ; kill -HUP $CHILD_PID
  cat 50mlinesofstuff | kafkacat -b srv1005,srv1007,srv1008 -t testymctesttopic
  #Can kick out ~184k msgs/sec single threaded, 100byte messages on crappy blade. Very fast single threaded performance, twice that of perf-test single threaded.

MIRRORMAKER
^^^^^^^^^^^
  # You don't need two consumer configs. Just shown here to show that a single mirrormaker process can read from multiple clusters
  bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config sourceCluster1Consumer.config --consumer.config sourceCluster2Consumer.config --num.streams 2 --producer.config targetClusterProducer.config --whitelist=".*"

  #Producer/Consumer config options are in documentation under Configuration. Here are some minimal examples:
  $ cat sourceCluster1Consumer.config
  group.id=mirrormaker.dc1
  zookeeper.connect=zoo0000:2181
  offsets.storage=kafka
  dual.commit.enabled=true
  
  #Note these are OLD producer config options. Use doc section "3.4 New Producer Configs" for 0.8.3+
  $ cat targetClusterProducer.config
  metadata.broker.list=kaf0000:9092
  #-1=wait for all ISRs. 0=dont wait. 1=wait for leader response
  request.required.acks=-1
  producer.type=sync
  serializer.class=kafka.serializer.DefaultEncoder
  compression.codec=snappy
  batch.num.messages=50

TEST IT
^^^^^^^
  bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group mirrormaker.dc1 --zookeeper dc1-zookeeper:2181 --topic test-topic

EXAMPLE
^^^^^^^
./bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config config/mm.dc2.consumer.config --num.streams 2 --producer.config config/mm.dc2.producer.config --whitelist=".*"
./bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group mirrormaker.dc1 --zookeeper zoo0000:2181


OPS MONITORING HOWTO
^^^^^^^^^^^^^^^^^^^^
  # Enable JMX upon java start
  kafka-server-start.sh:export JMX_PORT=9999
  kafka-run-class.sh:KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote=true
  kafka-run-class.sh:KAFKA_JMX_OPTS="$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=9999 "
  service kafka restart ; netstat -pantu|grep 9999

  # Use jconsole(with GUI) or tjconsole(from console) to look at your options
  https://github.com/m-szalik/tjconsole
  [broker-srv1005 #] java -jar tjconsole-1.5-all.jar
  TJConsole> ? 
  TJConsole>connect 127.0.0.1:9999 
  TJConsole>use kafka<tab> ... kafka.server<tab> ...kafka.server:type=BrokerTopicMetrics,name= <tab> <etc etc etc etc> 
  TJConsole>use kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec 
  TJConsole>get 

https://github.com/jmxtrans/jmxtrans
- Make your .json. This is ugly as hell. Use git:llaursen/kafka/host.json.j2 as example, or google for stackdriver's jmxtrans kafka.json (incompatible with 0.8.2, incorrect mbean names and quotes usage. Only use as an example.). Also use TJConsole above in order to find interesting metrics and proper mbean metric names.
  [srv1005 /usr/share/jmxtrans]# ./jmxtrans.sh start kafka.json 
- tail -f /tmp/JMXTrans/kafkaStats/BytesInPerSec.txt or whatever 


MAINTENANCE
^^^^^^^^^^^
Normally if a kafka broker dies a horrible death, you would bring up a new one to replace it which **has the same broker.id as the failed box.** This way, once the new broker is back, the other brokers in the cluster (and zookeeper) will treat it like the old one and fully restore its state.

In the instance you would like to permanently remove a broker, you are going to have to run the kafka-reassign-partitions script in order to get all partitions off the thing prior to removal. kafka-manager is a good tool to use for this. You may also use airbnb's kafkat, or if you're a narcisist, you can use the command line json based kafka-reassign-partitions.sh tool.

In the case of adding a new broker, 0.8.2+ will automatically rebalance partitions amongst all brokers in a cluster if you have the config option enabled. I'm including an example here in the case that you would like to do it manually for some reason. In our example, p3r3 is a topic with...3 partitions and 3 replications:

  #If you don't have / haven't built your --reassignment-json-file, do below to generate one. Otherwise, skip to last command.
  #Create a json file which includes topics you want to rebalance
  > cat topics-to-move.json
  {"topics":
         [{"topic": "p3r3"},{"topic": "list_example"}],
  "version":1
  }
  #Run the tool to --generate a json line you need to use later
  #--broker-list includes your new broker, and excludes your broken one.
  ./kafka-reassign-partitions.sh --generate --broker-list 1005,1007,1008 --topics-to-move-json-file topics-to-move.json --zookeeper srv1002:2181 
  #Look at the output. If it's doing what you want, put the final line into a file
  !! | tail -1 > my-generated.json
  #Now run the tool with --execute and --reassignment-json-file in place of generate and topics-to-move
  ./kafka-reassign-partitions.sh --execute --broker-list 1005,1007,1008 --reassignment-json-file my-generated.json --zookeeper srv1002:2181
  #All this thing does is modify zookeeper, then kafka picks up the changes.
  #You may want to run kafka-preferred-replica-election.sh after this if your LEADER is not your PREFERRED replica.


COMMON ERRORS
^^^^^^^^^^^^^
**"failed due to Leader not local for partition|NotLeaderForPartitionException"** :: This usually just means that a producer/consumer tried to produce/consume to a broker which was not leader for that partition. These errors will occur when brokers get restarted, ISR's get shifted, partition reassignment happens, etc. etc. and can usually be ignored unless they come in high volume or repeat for longer than 5 seconds. Clients should use this error as a queue to re-request updated metadata to find a new leader. Note: You may also get this error if a metadata update request fails...


PERFORMANCE
^^^^^^^^^^^
**::Memory Related::**

kafkaServer-gc.log will let you know when you're hitting garbage collection:
  2015-04-29T13:48:02.610-0700: 89.811: [GC (Allocation Failure) 89.812: [ParNew: 279664K->48K(314560K), 0.0037317 secs] 284469K->4856K(1013632K), 0.0038937 secs] [Times: user=0.04 sys=0.00, real=0.00 secs]

Generally we want to **reserve a ton of our RAM for filesystem caching** as that is how kafka works, rather than giving a bunch of it to JVM. Still, lets try to give JVM an efficient amount such that garbage collection doesn't occur too often.

Connect to your kafka broker under load using jconsole and then click the Memory tab and wait a few seconds. The bars marked "Heap" in the bottom right are what you're wanting to pay attention to - CMS, Eden, and Survivor. 
- You generally want CMS to be low to nothing. This gets cleared out when ConcurrentMarkSweep runs.
- It's ok for Eden space to bounce around a lot
- If survivor space is often filling up, you need to allocate more memory to your heap. This is where you run into java.outofMemory errors.

You also want to pay attention to the GC time: text to see how long garbage collection is taking. ConcurrentMarkSweep is a very expensive operation, so it's important that it runs fast.

With kafka 0.8.2 and producer-perf-test running pushing around 400-500mbit (its advertised maximum on a 1gig connection), you'll generally fill 1gig of CMS every ~45mins (**no consumers**).

Note consumers have a large impact on memory usage. Do your memory tweaking both with and without consumers running. Note that if you're seeing a lot of rebalances in your consumer log, you probably need to increase the amount of memory you've given your consumer process.


**::Partition Rebalancing::**

Your producers might spew a bunch of these after adding/removing a producer or experiencing some sort of temporary outage on one/all of your brokers:
  [2015-04-29 14:18:44,139] WARN Produce request with correlation id 10556 failed due to [perf_test6p2r,0]: kafka.common.NotLeaderForPartitionException (kafka.producer.async.DefaultEventHandler)

You might see this in your controller.log:
  [2015-04-29 14:18:38,826] TRACE [Controller 1]: checking need to trigger partition rebalance (kafka.controller.KafkaController)

This is normal if you have recently had a broker shut down for a length of time, or have added a new broker, experienced a disruption, etc etc. Kafka is rebalancing partitions by possibly saying "hey (new|formerlyfailed) broker, you handle these partitions", setting as an ISR, waiting until it is in sync, and then promoting that broker to be leader for a certain partition. During this failover, if you want guaranteed delivery of messages, your producer must catch the non-fatal kafka.common.NotLeaderForPartitionException and pause publishing or consuming for several hundred ms. Kafka clients will auto retry to send 3 times with 100ms delay between retries before deciding to drop the message (default).

**::SO_SIZES::**

You may see socket.send.buffer.bytes & socket.receive.buffer.bytes in kafka's server.properties. I didn't see any performance increase/decrease moving these from 1MB -> 10MB when pushing 500Mbit x 3 producers -> 3 node cluster, no matter the buffer size. The limitation in this case appears to be the producer-perf-test suite.


KAFKA RELATED TOOLS/CLIENTS SETUP DEETS
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
**::flasfka::**

https://github.com/travel-intelligence/flasfka :: Expose kafka over python's simple web framework flask(http)

  yum install python-devel
  git clone https://github.com/travel-intelligence/flasfka
  python setup.py #DEPS WARN: This will grab both kafka and flask python modules

/etc/flasfka/conf.py:
  HOSTS=["srv1005:9092","srv1007:9092","srv1008:9092"]
  DEFAULT_GROUP="flasfka"
  CONSUMER_TIMEOUT=0.1
  CONSUMER_LIMIT=100

Don't forget to set your environment variable:
  export FLASFKA_CONFIG=/etc/flasfka/conf.py

/usr/lib/python2.7/site-packages/flasfka-0.0.1-py2.7.egg/flasfka/__init__.py:
  import logging
  logging.basicConfig()
  app.run(host='0.0.0.0') #Add to BOTTOM

Check your shit:
  /usr/bin/flasfka-serve & ; netstat -pantu|grep 5000
  curl http://localhost:5000/topic-name/


Linux Internals
===============

TODO:

- user vs system
- context switching
- CPU and IO schedulers, use tag ".. _linux-internals-scheduling:"

.. _linux-internals-systemcalls:

System Calls
------------
The kernel is the only program running in a special CPU mode called kernel mode, allowing full access to devices and the execution of privileged instructions. 

User processes run in user mode. These processes have a separate stack and registers (memory space) from kernel mode processes. These processes must request privileged operations from the kernel via system calls.

System calls are the primary mechanism by which user-space programs interact with the Linux kernel. The execution of privileged instructions in user mode causes exceptions, which are then handled by the kernel.

Popular system calls are open, read, write, close, wait, execve, fork, exit, and kill

An example with sys_read()
^^^^^^^^^^^^^^^^^^^^^^^^^^
my_app wants to access a storage device to read some info and then transfer that info to its memory space:

- main process passes its arguments to libc_read
- libc_read loads the specific system call number (in this case __NR_read) into the cpu's EAX register, as well as loads whatever other arguments (ie:what to read, the file descriptor of the file to read, etc) into other processor registers.
- libc_read then throws a special software interrupt which triggers an exception, which the kernel picks up and handles
- kernel code function system_call() is called, which has a look at the eax register to find out what system call to load
- sys_read() is called, which then uses the file descriptor it was passed to check for locks, and looks up a file struct which has a pointer to the function to call in order to handle that file type.
- sys_read() then does a read request to the device node driver function, which passes back information, and the process back up to userland is reversed

.. _linux-internals-pipes:

pipe(2)
^^^^^^^
https://brandonwamboldt.ca/how-linux-pipes-work-under-the-hood-1518/

Linux has an in-memory VFS called pipefs that gets mounted in kernel space at boot. The entry point to pipefs is the pipe(2) syscall. This system call creates a file in pipefs and then returns two file descriptors (one for the read end, opened using O_RDONLY, and one for the write end, opened using O_WRONLY).

Each unix process (except perhaps a daemon) has at least three standard POSIX file descriptors - STDIN, STDOUT, STDERR. In the case of unnamed pipes (eg: ls -la | grep blah), bash will:

* clone/exec grep
* call pipe(2) and get STDIN/STDOUT file descriptors back from the new pipe 
* call dup2() on the STDIN (fd[0]) of grep and the STDOUT (fd[1]) of the pipe
  * dup2() is a system call which duplicates two file descriptors, which will result in the two descriptors being able to be used interchangably (they refer to the same open file description, and thus share the file offset and file status flags. This means that, for example, if the file offset is modified by using lseek(2) on one of the descriptors, the offset is also changed for the other
* clone/exec ls -la
* call dup2() on the stdout (fd[1]) of ls and stdin (fd[0]) of the pipe

Named pipes, also known as FIFOs, work pretty much the same as unnamed pipes, except they stick around when programs are done using them.

Pipe size default is typically 64KB. When a pipe's buffer is full, a write(2) will block. If all file descriptors pointing to the read end of the pipe have been closed, writing to the pipe willr aise the SIGPIPE signal. If this signal is ignored, write(2) fails with error EPIPE.

When a pipe is empty, a read(2) will block. If there is nobody listening on the other end (all the file descriptors pointing to the write end of the pipe have been closed), then the pipe will return an EOF (ie: read(2) will return 0).


.. _linux-internals-signals:


Linux Signals
-------------
Signals are software interrupts. Kill sends a SIGTERM by default. The kernel delivers signals to target processes or process groups on behalf of an originating process, or on behalf of itself. If the originating process has the permissions to send a signal to another, the kernel forwards it on.

SIGHUP(1) - hangup - users terminal is disconnected somehow. Some daemons are programmed to reload their config and log files rather than close when they get this signal. If a process does not catch this signal, the default action of the parent is to close the process.
SIGINT(2) - interrupt - ctrl+c sends a sigint. SIGINT is nearly identical to SIGTERM, but "nicer." Interactive shells such as mysql may take it to mean "terminate current query" rather than terminate itself.
SIGKILL(9) - kill - forceful termination. Memory stripped. Cannot be ignored
SIGTERM(15) - Terminate. This asks the process to quit, go through its normal cleanup procedure
SIGSTOP(17,19,23) - Suspends a processes execution. Cannot be ignored. If you are experiencing some sort of intermittent socket/buffer full or backflow buildup related bug, SIGSTOP is a good way to reproduce the issue. File handles will be kept open.

Note that processes can ignore, block, or catch all signals except SIGSTOP and SIGKILL. If a process catches a signal, it means that *it includes code that will take appropriate action when the signal is received*. If the signal is not caught, the kernel will take the appropriate action for the signal.



/proc and /sys
--------------

procfs exposes runtime information & statistics of devices and processes, as well as allows you to change runtime variables on those devices and processes. Over the years, /proc has become a popular dumping grounds for random information and features. It's unruly, mostly because it doesn't follow any standard of structure.

Sysfs does the same thing, but provides a structure for this information. This structure is created by the kernel. Sysfs is intended as a replacement for procfs. All new stuff is expected to use sysfs rather than the unstructured dumping grounds of proc.


sysfs
^^^^^
  https://www.kernel.org/doc/pending/hotplug.txt
  http://devicetree.org/Device_Tree_Usage

sysfs is a virtual file system provided by the linux kernel that exports information about various kernel subsystems, hardware devices, and associated device drivers from the kernel's device model to user space through virtual files.

Basically, the kernel reads a device tree that is passed to it by a device's firmware, then takes information in that device tree along with whatever BUS path it got the message from and uses it to populate under /sys a file/directory structure for that device. A UEVENT is then generated and shot out through a 'netlink' socket, which udev receives.

Eg: Check number of caches available to cpu0 and the size of those caches:

  # grep . is same as cat /path/to/files*
  grep . /sys/devices/system/cpu/cpu0/cache/index*/size 
  # typical results: two 32k level 1 cache, 256k lvl2 cache, and 3MB lvl3 cache)

 

/dev and Device Nodes
---------------------

- Firmware passes (ePAPR & IEEE stanard) device tree to linux kernel listening on some bus
  - Device tree has "bindings", which are descriptions of a device, requested major number (optional)
  - In almost all cases, keyword "compatible" is used to assign bindings
  - Compatible=mymodule tells the kernel what module should be loaded to interact with this device
- kernel determines a standard major number (either based on a map of major numbers and device types), or allocates a new major number (16 bits, 4096)


Process Management
------------------

Processes vs Threads
^^^^^^^^^^^^^^^^^^^^
Separate processes can not see each others memory. They have to communicate with each other via system calls (IPC). Threads share the same memory, so you lose the overhead. Unfortunately this also makes it easy for threads to step all over each other, with one thread perhaps changing a variable value without telling another thread. These are called Concurrency Problems.

It's fully possible for a process to create a bunch of threads to do stuff, and the kernel won't know about it. Its schedulers will keep treating the process as having one thread. This is bad for performance reasons. As such, there is a clone() system call (also used for process cloning) which allows registration and resource consideration within the kernel for a thread.


task_struct
^^^^^^^^^^^

Each process has a task_struct. This is a large structure which holds process data such as the state of execution, a stack, a set of flags, the parent process, the thread of execution (of which there can be many), and open files. The state variable is a set of bits that indicate the state of the task. The most common states indicate that the process is running or in a run queue about to be running (TASK_RUNNING), sleeping (TASK_INTERRUPTIBLE), sleeping but unable to be woken up (TASK_UNINTERRUPTIBLE), stopped (TASK_STOPPED), or a few others. The flags word defines a large number of indicators, indicating everything from whether the process is being created (PF_STARTING) or exiting (PF_EXITING), or even if the process is currently allocating memory (PF_MEMALLOC). The name of the executable (excluding the path) occupies the comm (command) field. The mm field represents the process's memory descriptors.

So, each userspace process gets its own task_struct, except init which has a statically defined struct called init_task. *These are collected into either a hash table (hashed by PID) or a circular doubly linked list*. The circular list is **ideal for iterating** through, such as a process scheduler would do. There is no head or tail to this list, so you can use the init_task struct as a reference point to iterate further.


Process Creation
^^^^^^^^^^^^^^^^

- Program calls fork() (actually clone() these days, but using fork() in this description)
- fork() system calls to sys_fork()
- sys_fork() calls do_fork()
- do_fork() does an alloc_pidmap to get a new PID
- do_fork() then calls copy_process and passes the flags, stack, and registers used by the parent process, the parent process PID, and the newly allocated PID
- copy_process consults with Linux Security Module (LSM) to see whether the current task is allowed to create a new task
- copy_process then calls dup_task_struct, which creates a new kernel stack, thread_info structure, and task_struct for the new process. The new values are identical to those of the current task. At this point, the child and parent process descriptors are identical.
- Now the child needs to differentiate itself from its parent. Various members of the process descriptor are cleared or set to initial values. Members of the process descriptor that are not inherited are primarily statistic information. The bulk of the data in the process descriptor is shared.
- Next, the child's state is set to TASK_UNINTERRUPTIBLE, to ensure that it does not yet run.
- Now, copy_process() calls copy_flags() to update the flags member of the task_struct. The PF_FORKNOEXEC flag, which denotes a process that has not called exec(), is set.
- Depending on the flags passed to clone(), copy_process() then either duplicates or shares open files, filesystem information, signal handlers, process address space, and namespace.
- The new task is then assigned to a processor, and control is passed back to do_fork() with a pointer to the new child
- The process isn't actually running at this point, so do_fork() calls the function wake_up_new_task on it. This places the new process in a run queue and wakes it up for execution
- do_fork() then returns the new PID value back on up through fork() to the caller
- **The parent process and the child process resume execution at the exact same spot.** fork() returns a PID > 0 to the parent process, such that it knows when it resumes execution that it is the parent. It will then likely call wait() in order to wait for the child to finish executing (or at least close all its related file descriptors, off-handedly letting the parent process know that the child ran successfully)
- The child process gets woken up and continues executing at the same spot as its parent, just after the fork() call. In contrast to the parent process, it gets a return PID of 0 from the fork() call, and hits an if pid == 0 block (true) which will then call execve() in order to replace the executable image of this child process

**tldr;** clone() is called, a new PID is generated as well as a new task_struct and other process-related info, flags are copied over to the new process's task_struct, the new task is assigned to a processor and then woken up and its PID is passed back to the parent process.

Example (NOTE: asterisks escaped (\*) due to markup formatting. Remove before running code):
  #include <unistd.h>
  #include <stdio.h>
  #include <fcntl.h>
  
  int main(void)
  {
    int pid = fork();
    // Child and Parent resume execution here
  
    if (pid == -1) {
      // fork threw an error
      fprintf(stderr, "Could not fork process\n");
      return -1;
    } else if (pid == 0) {  
      // retcode 0 means this is a child process
      fprintf(stdout, "Child will now replace itself with ls\n");
  
      // Setup the arguments/environment to call
      char \*argv[] = { "/bin/ls", "-la", 0 };
      char \*envp[] = { "HOME=/", "PATH=/bin:/usr/bin", "USER=derp", 0 };
  
      // Call execve(2) which will replace the executable image of this
      // process
      execve(argv[0], &argv[0], envp);
  
      // Execution will never continue in this process unless execve returns
      // because of an error
      fprintf(stderr, "Oops!\n");
      return -1;
    } else if (pid > 0) {
      // retval greater than 0, we are the parent process
      int status;
  
      fprintf(stdout, "Parent will now wait for child to finish execution\n");
      wait(&status);
      fprintf(stdout, "Child has finished execution (returned %i), parent is done\n", status);
    }
  
    return 0;
  }


Process Scheduling
^^^^^^^^^^^^^^^^^^
The scheduler maintains lists of task_struct's. Each list has a different priority number. task_struct's are placed in each list based on loading and prior process execution history, along with other factors depending on which process scheduler you're using.


Process Destruction
^^^^^^^^^^^^^^^^^^^

- User space calls exit(), which makes a sys_exit() system call, which calls do_exit()
- do_exit() sets the PF_EXITING flag in the processes task_struct, which tells the kernel to avoid manipulating this process while it's being removed
- do_exit() makes a series of calls. exit_mm to remove memory pages, exit_notify to notify the parent process and other things, and more?
- Finally, the process state is changed to PF_DEAD in its task_struct and the schedule function is called to select a new process to execute
- release_task is called which will reclaim memory that the process was using


File Descriptors
----------------

- To the kernel, all open files are referred to by File Descriptors. A file descriptor is a non-negative number. 
- When we open an existing file or create a new file, the kernel returns a file descriptor to the process. 
- The kernel maintains a table of all open file descriptors which are in use. The allotment of file descriptors is generally sequential and they are alloted to the file as the next free file descriptor from the pool of free file descriptors. When we closes the file, the file descriptor gets freed and is available for further allotment.
- When we want to read or write a file, we **identify the file with the file descriptor that was returned by the open() or create() system calls**, and **use it as an argument to either read() or write().**
- It is by convention that, UNIX System shells associates the file descriptor 0 with Standard Input of a process, file descriptor 1 with Standard Output, and file desciptor 2 with Standard Error. File descriptor ranges from 0 to OPEN_MAX.

.. image:: media/linux-twoprocs_same_fd.jpg
   :alt: Two independent processes with the same file open
   :align: center


Udev, Modprobe, Hardware Discovery
----------------------------------
The kernel is constantly monitoring various system buses. When a piece of hardware is initialized (on kernel exec) or plugged in, the kernel generates a "UEVENT" on a netlink socket which UDEV is listening to. This event includes the hardware's *Product/Vendor IDs* (PD/VD)

Memory
------

dentry/inode caches
^^^^^^^^^^^^^^^^^^^
Each time you do an 'ls' (or any other open(), stat(), whatever operation) on a filesystem, the kernel needs to get information about the filesystem which resides on the disk. The kernel parses this data and puts it into some *filesystem independent structures* so that access to files can be handled in the same way across all different filesystems.

The kernel has the option of throwing away these data structures, but it bets you are going to need the info again and as such keep these structures around in several caches called the dentry and inode caches.

dentries are common across all filesystems, but each filesystem has its own cache for inodes. This ram is a component of "Slab:" in meminfo. View the different caches and their sizes by doing:

  cat /proc/slabinfo
  head -2 /proc/slabinfo #get column names ; cat /proc/slabinfo|egrep "dentry|inode"

Note that slabinfo contains various other caches.

Process Memory
^^^^^^^^^^^^^^
Per-process memory details:

  cat /proc/<pid>/maps
  cat /proc/<pid>/smaps  #lots more detail

Check out the [heap] entry to see how much memory the kernel allocated for the process's heap. It may or may not be what was requested!

- **VIRT** stands for the virtual size of a process, which is the sum of memory it is actually using, memory it has mapped into itself (for instance the video cards’s RAM for the X server), files on disk that have been mapped into it (most notably shared libraries), and memory shared with other processes. VIRT represents how much memory the program is able to access at the present moment.
- **RES** stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds directly to the %MEM column.) 
- **SHR** indicates how much of the VIRT size is actually sharable (memory or libraries). In the case of libraries, it does not necessarily mean that the entire library is resident. For example, if a program only uses a few functions in a library, the whole library is mapped and will be counted in VIRT and SHR, but *only the parts of the library file containing the functions being used will actually be loaded in and be counted under RES.*


VFS
---
VFS works as an abstraction layer sitting between filesystems and system calls. By having this layer, a system call doesn't need to know how to communicate with all these different filesystems (ext3, ufs, zfs, nfs, /proc, /dev), and instead only communicates to VFS. VFS then communicates to the file system.

inode vs vnode
^^^^^^^^^^^^^^
get pic from http://stackoverflow.com/questions/5256599/what-are-file-descriptors-explained-in-simple-terms


I/O Stack
---------
Application -> System Calls -> VFS -> File System -> Volume Manager -> Block Device Interface -> Target I/O Driver -> Host Bus Adapter Driver -> Disk Devices
It's also possible for the system call to skip straight to block device interface.


Character vs Block Devices
--------------------------
Character (aka raw) devices provide unbuffered, sequential access of any I/O size down to a single character, depending on the device. An example of this would be a keyboard  or a serial port.

Block devices perform I/O in units of blocks, which are typically 512bytes. Blocks can be accessed randomly based on their block offset (location), which begins at 0 at the start of the block device.
.. _networking:

Networking
==========

.. _networking-tcp:

TCP
---
*IP* works by exchanging pieces of information called packets. A packet is a sequence of octets (bytes) and consists of a header followed by a body. The header describes the packet's source, destination and control information. The body contains the data IP is transmitting.

Due to network congestion, traffic load balancing, or other unpredictable network behavior, IP packets can be lost, duplicated, or delivered out of order. TCP detects these problems, requests retransmission of lost data, rearranges out-of-order data, and helps minimize network congestion to reduce the occurrence of the other problems. Once the TCP receiver has reassembled the sequence of octets originally transmitted, it passes them to the receiving application.

- Ordered & error checked delivery of data
- TCP waits for out-of-orders or retransmissions. This makes it unsuitable for live data (skype etc)
- Transmission Control Protocol accepts data from a data stream, divides it into chunks, and adds a TCP header creating a TCP segment. The TCP segment is then encapsulated into an Internet Protocol (IP) datagram, and exchanged with peers.
- The term TCP packet appears in both informal and formal usage, whereas in more precise terminology segment refers to the TCP Protocol Data Unit (PDU), datagram[4] to the IP PDU, and frame to the data link layer PDU

- A TCP segment has a header and a data section
- TCP header has: sport, dport, seq #, ack #, offset, reserv, flags (9 control bits), wsize, chksum, urg pointer, options, padding
- flags contains stuff like SYN, ACK(1=enable ack), RST(reset conn), FIN(no more data from sender)...etc
- The Seq number field has dual purpose. If SYN=1, then Seq# is set to the initial sequence number. The sequence number of the actual first data byte and the acknowledged number in the corresponding ACK are then this sequence number plus 1. In other words, Seq# doesn't increment until the data starts


TCP Connection Establishment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(server has a port binded and is listening. passive open.)
1. SYN: The active open is performed by the client sending a SYN to the server. The client sets the segment's sequence number to a random value (eg:222).
2. SYN-ACK: In response, the server replies with a SYN-ACK. The acknowledgment number is set to one more than the received sequence number i.e. 223, and the sequence number that the server chooses for the packet is another random number, 333.
3. ACK: Finally, the client sends an ACK back to the server. The sequence number is set to the received acknowledgement value (223 - yes, same as above), and the acknowledgement number is set to one more than the received sequence number i.e. 334

**Seq# Note:**
Seq#'s are used to identify each BYTE of DATA, not each tcp segment. So, if a sending computer sends a packet containing four payload bytes with a sequence number field of 100, then the sequence numbers of the four payload bytes are 100, 101, 102 and 103. When this packet arrives at the receiving computer, it would send back an acknowledgment number of 104 since that is the sequence number of the next byte it expects to receive in the next packet. This is called Cumulative Acknowlegement. 

.. _networking-mtu:

Maximum Transmission Unit (MTU)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
MTU is referenced by packet (and frame) based protocols like TCP and UDP in order to determine the maximum size of packet it should construct for communication over a given interface. Something called **Path MTU Discovery** (PMTUD) is used in order to discover this value.

In IPv4, this works by setting the *DF* (don't fragment) bit in the ip header of outgoing packets. Any device along the network path whose MTU is smaller than the packet will drop it and send back an ICMP *fragmentation needed* message containing its MTU. The source host reconfigures appropriately, and the process is repeated.

IPv6 works differently as it does not support fragmentation (nor the don't fragment option). Instead, the initial packet MTU is set to the same as the source interface, and if it hits a device along the path where the packet size is too large for its MTU setting, that device drops the packet and sends back an ICMPv6 *Packet Too Big* message which contains its MTU. The source then reconfigures its MTU appropriately, and the process is repeated.

If the path MTU changes lower along the path after the connection is set up, the process still does its thing. If the MTU changes to a higher value, PMTUD will eventually discover this (Linux performs another PMTD check every 10 minutes by default) and increase MTU accordingly.

Some firewall operators will blanket deny all ICMP traffic. This means that after a TCP handshake happens and the first packet is sent out with a larger MTU than something along the link can handle, the firewall blocks the ICMP reply and you end up with a "black hole" connection where the source keeps retrying to send data and some device along the path keeps dropping it, with a blocked response. Some PMTUD attempt to infer this problem and lower MTU size accordingly, but the lack of response could also just be due to congestion.

Some routers may work around this issue by changing the *maximum segment size* (MSS) of all TCP connections passing through links which have an MTU lower than the ethernet default of 1500. While an MTU is concerned with the total size of a packet, MSS only determines the TCP Segment (minus TCP header) size - typical default = 536 Bytes.

[TCP Packet[TCP Segment[IP datagram[Data link layer Frame]]]]
[UDP Datagram[UDP Segment[IP datagram[Data link layer Frame]]]]

Maximum Segment Size (MSS)
^^^^^^^^^^^^^^^^^^^^^^^^^
- Typically derived from getting MTU from data link layer
- Watch for specialized network hardware along your data path screwing with headers, adding shit and making the packet sizes weird. 
- Troubleshoot: you might want to decrease MTU size on the sender. Also wireshark along the data path if possible (sometimes not due to hardware owned by upstream)


TCP Windows
^^^^^^^^^^^
- TCP Receive Window is the amount of data that a computer can accept without acknowledging the sender. Its original maximum was 64KB, and that's what the field can still hold. Now there is an option called TCP Window Scale which specifies a byte shift on the original field in order to determine how many orders of magnitude higher than the original 64KB that a window size should be set to.
- Window size is determined during the 3 way handshake
- The throughput of a communication is limited by two windows: the congestion window and the receive window. The former tries not to exceed the capacity of the network (congestion control) and the latter tries not to exceed the capacity of the receiver to process data (flow control).
- "Bandwidth Delay Product" :: (bits/sec) * RTTms = BDP. If more than 64KB of data is "in flight", then a bit shift is in order to raise window size
- Some routers and packet firewalls rewrite the window scaling factor during a transmission. This causes sending and receiving sides to assume different TCP window sizes. The result is non-stable traffic that may be very slow.

TCP Timestamps
^^^^^^^^^^^^^^
- Same as seq# basically. Not normally based on system clock, just a random value.
- In the case that the tcp window size exceeds the number of possible sequence numbers (remember, each seq# is assigned to 4bytes of info), the tcp timestamp is used to determine whether a retransmitted packet is part of this 4GB segment, or the other.

TCP Flags
^^^^^^^^^
**URG**: Urgent flag says "process me immediately, before finishing the stream". An example is when TCP is used for a remote login session, the user can send a keyboard sequence that interrupts or aborts the program at the other end. These signals are most often needed when a program on the remote machine fails to operate correctly.

TCP Problems
^^^^^^^^^^^^
- TCP sucks At Wireless. Wireless links are known to experience sporadic and usually temporary losses due to fading, shadowing, hand off, and other radio effects. This causes incorrect congestion prediction, window scaling, etc. A congestion avoidance phase occurs where speed is compromised. There are new congestion control algorithms out there that attempt to perform better (vegas, westwood, veno, santa cruz etc)
- The application cannot access the packets coming after a lost packet until the retransmitted copy of the lost packet is received. This sucks for stuff that is live


UDP
---
- Lower overhead & reduced latency vs TCP
- Less complexity. Useful where no response is not a big deal
- 64KB max message size

Networking General
------------------

Read me: https://dougvitale.wordpress.com/2011/12/21/deprecated-linux-networking-commands-and-their-replacements/

OSI Model
  application							data
  presentation							data
  session							data
  transport     [end-to-end connections and reliability]	segments
  network       [path determination & logical addressing]	packets
  data link     [physical addressing (MAC & LLC)]		frames
  physical      [media, signal, binary transmission]		bits

PDNTSPA! or "All People Seem To Need Data Processing"

TODO
.. image media/networking-tcppacket.png

ARP
^^^
A protocol used to translate network-layer addresses (ie: ip addresses) to link-layer addresses (mac addrs).

Start using "ip n" (ip neighbour) instead of arp -a.

Devices which "share" a virtual IP may use gratuitous arp upon virtual IP migration in order to spam update switches and other connected devices with the new associated mac address.


Switching and Routing
---------------------
STP
^^^

- STP (spanning tree protocol) analyzes a network to ensure no looping can occur on networks with shitty design. It does this by designating a root bridge, finding root "ports" which are just paths, and then disabling all ports aside from the least cost path. Updates and such on link down, etc. Disable stp on host ports for faster no shutdown (dont have to wait for convergence)
- A broadcast storm can occur when switches are in a loop. Switch A is connected to B and C, B connected to A and C, etc etc. Host A on switch A makes a broadcast request. Switch A broadcasts this to B and C. B broadcasts this to C. C broadcasts this to A, and A thinks that this is a new broadcast request and so sends out another broadcast to B. Repeat

Private VLANs
^^^^^^^^^^^^^
PVLANs, also known as port isolation, are vlans that contain switchports which are restricted such that they can *only communicate with a given "uplink".* This means that in contrast to using regular VLANs, you can have two servers connected to the same switch who are on the same pvlan, unable to talk to each other without routing through the designated uplink first. This means that direct peer-to-peer traffic between peers on the same switch is blocked - any such communication must go through the uplink.

A PVLAN acheives this by dividing a VLAN (*Primary*) into sub-VLANs (*Secondary*). A regular VLAN has a single broadcast domain, while private VLAN partitions one broadcast domain into multiple smaller broadcast domains. Here's the breakdown:

- *Primary VLAN*: The original VLAN. Used to forward frames downstream to all Secondary VLANs.
- *Secondary VLAN*: These are configured with one of the following types:
  - *Isolated*: Any switchports associated with an Isolated VLAN can reac the primary VLAN, but not any other Secondary VLAN. Additionally, hosts associated witht he same isolated VLAN cannot even reach each other. There can be multiple isolated VLANs in one private VLAN domain.
  - *Community*: Any switchports associated with a common community VLAN can communicate with each other and with the primary VLAN, but not with any other secondary VLAN. There can be multiple different community VLANs within one private VLAN domain.

There are mainly two types of ports in a Private VLAN: *Promiscuous* and *Host*. A Host port further divides into Isolated port, and Community port. From the above description, we can derive that an uplink or cross-connect to a router/firewall/other switch would be configured as Promiscuous, while ports going to physical servers would be configured as host ports in either isolated or community.

Performance and Troubleshooting
===============================
TODO:
- perftools, systemtap
- jconsole, java heap info
- bgregg stuff in general. Also ref http://techblog.netflix.com/2015/11/linux-performance-analysis-in-60s.html

General
-------
.. image:: media/performance-linux_observability_tools.png
   :alt: Linux Performance Observability Tools, bgregg
   :align: center

Python
------
Excellent: https://www.huyng.com/posts/python-performance-analysis

Runtime
^^^^^^^
  python3 -m cProfile wordfreq.py short-story.txt

Check out runsnakerun for visualization of cProfile output, pretty cool. www.vrplumber.com/programming/runsnakerun/
  python3 -m cProfile -o out.profile wordfreq.py short-story.txt ; python runsnake.py out.profile

Can also check out python visualization libraries that use graphvis.

Memory
^^^^^^
There is a module called memory_profiler that will output, line by line, how much memory your script uses:
  pip install -U memory_profiler
  pip install psutil #this is for better memory_profiler module performance
  vim freqgen.py #add @profile decorator above the function you're interested in
  python -m memory_profiler freqgen.py short-story.txt

Performance and Troubleshooting
===============================
TODO:
- perftools, systemtap, sysdig
- jconsole, java heap info
- bgregg stuff in general. Also ref http://techblog.netflix.com/2015/11/linux-performance-analysis-in-60s.html

General
-------
.. image:: media/performance-linux_observability_tools.png
   :alt: Linux Performance Observability Tools, bgregg
   :align: center

Python
------
Excellent: https://www.huyng.com/posts/python-performance-analysis

Runtime
^^^^^^^
  python3 -m cProfile wordfreq.py short-story.txt

Check out runsnakerun for visualization of cProfile output, pretty cool. www.vrplumber.com/programming/runsnakerun/
  python3 -m cProfile -o out.profile wordfreq.py short-story.txt ; python runsnake.py out.profile

Can also check out python visualization libraries that use graphvis.

Memory
^^^^^^
There is a module called memory_profiler that will output, line by line, how much memory your script uses:
  pip install -U memory_profiler
  pip install psutil #this is for better memory_profiler module performance
  vim freqgen.py #add @profile decorator above the function you're interested in
  python -m memory_profiler freqgen.py short-story.txt

Programming
===========

Python
------
*break* terminates the entire loop. *continue* moves on to the next loop item.

Objects and Mutability
^^^^^^^^^^^^^^^^^^^^^^

In languages like C, a variable represents storage for "stuff". If we wrote:

  int foo = 42;

it would be correct to say that the integer variable *foo* contained the value *42*. That is, variables are a sort of container for values.

In Python, this is not the case. When we say:

  foo = Foo()

it would be wrong to say that *foo* "contained" a *Foo* object. Rather, foo is a *name* with a *binding* to *the object created by Foo()*. The portion to the right side of the equals sign creates an object. Assigning *foo* to that object merely says "I want to be able to refer to this object as foo." **Instead of variables (in the classic sense), Python has names and bindings.**

So when we do this:

  >>> class Foo(): pass
  >>> foo = Foo()
  >>> print(foo)
  <__main__.Foo object at 0xd3adb33f>

What we're seeing is a pointer to the memory address of the object created by Foo(). This can allow us to quickly and easily see whether two *names* are *bound* to the same *object*. In Python we can use the *is* operator for this.

So a name is just a name, and in Python, **everything** is an object:

  >>> foo = 10
  >>> print(foo.__add__)
  <method-wrapper '__add__' of int object at 0x8503c0>
  >>> dir(10)
  [whole shitload of member functions]

Even numbers are objects.

So, it turns out that python has two types of objects: **mutable** and **immutable**. The value of mutable objects can be changed after they are created, The value of immutable objects cannot be. **Lists and dictionaries are mutable object.** You can create a list, append some values, and the list is updated in place. **Strings and Tuples are immutable objects.** Once you create as string, you can't change its value.

When you think you're changing a string, *you're actually rebinding the name to a newly created string object.* The original object remains unchanged, even though it's possible that nothing refers to it anymore.

  >>> a = 'foo'
  >>> a
  'foo'
  >>> b = a
  >>> a += 'bar'
  >>> a
  'foobar'
  >>> b
  'foo'

See that? The a+='bar' created a bran new object, and the 'a' *name* was re-bound to point to the new object. 'b' on the other hand is left pointing to the original object.


Unit Testing
^^^^^^^^^^^^

pytest is the one you want. Minimal boilerplate, maximum features. unittest uses some awful classy crap.

#) Install:

  pip install pytest ; pip3 install pytest

#) Write your unit testing code by importing some definition from some file/module you made:

  from your_module import my_fibonacci
  def test_number_6():
    assert my_fibonacci(6) == 8
  def test_number_0():
    assert my_fibonacci(0) == 1

#) Now test:

  py.test my_fib_unit_test.py

List Comprehension
^^^^^^^^^^^^^^^^^^

A list comprehension is just a shorthand way of defining a function.

  [do stuff to x    for x in list      if x>0]       ##The if x>0 is optional

Is the same as this:

  def omg(list):
    for x in list:
      if x>0:
        do stuff to x

Eg:
  poweroftwo = [x**2 for x in range(10)]
  ifdivbytwo = [x for x in poweroftwo if x % 2 == 0]

You can perform more than one operation on each item in your list too:
  wordlist = 'The quick brown fox jumps over the lazy dog'.split()
  derp = [[w.upper(), w.lower(), len(w)] for w in wordlist]


What's a lambda?
^^^^^^^^^^^^^^^^

lambda is just an in-place, anonymous function. Eg:
  addTwo = lambda x: x+2
  addTwo(2)  #4

It's the same thing as defining this:
  def addTwo(x):
    return x+2

You can even throw them into dictionaries/hash trees:
  mapTree = {
      'number': lambda x: x**x,
      'string': lambda x: x[1:]
  }
  otype = 'number'
  mapTree[otype](3)  #27
  otype = 'string'
  mapTree[otype]('foo')  #'oo'

It's really just a syntactical thing. It's good if you know that your "function" is only going to be used once, by one thing. Otherwise just create a def(). Note above that ** means to the power of.

One good use for it is with the key= value in your sort() and sorted(). eg:
  pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]
  pairs.sort(key=lambda english: english[1])
This will sort by the second value of each tuple, giving this:
  [(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]

Why __main__?
^^^^^^^^^^^^^
Why do some python files have this? Why should you use it in your python scripts?
  if __name__ == '__main__':
    main()

The python interpreter, when it reads a source (.py) file, will execute everything in it. If, for example, "python myfirstscript.py" is ran, then the interpreter prior to running the source will set the special variable "__name__" to equal "__main__". If, inside of myfirstscript.py, you have "import random_module", then the interpreter will set the __name__ variable for that module to its name, in this case "random_module".

So what's the point? Well, let's say random_module.py could, if you wanted, be ran by itself. myfirstscript.py is just importing it because it has some functions in there that are useful. Well, the interpreter just runs whatever code it opens up. If inside the random_module script you don't have the if __name__ == '__main__': main_function() clause, then the interpreter is just going to run the file. You probably don't want this...you just want the functions out of it, you are running myfirstscript.py not random_module.py. So, if you make sure the if __name__ clause is the only thing that starts the actual work of the script, then you can avoid this.

Doing it this way, you can still run random_module from inside myfirstscript.py if you want. Just do this:
  import random_module
  random_module.main()

Decorators
^^^^^^^^^^
http://simeonfranklin.com/blog/2012/jul/1/python-decorators-in-12-steps/

A decorator is just a callable that takes a function as an argument, and returns a replacement function. Let's create a simple decorator:

  def outer(some_func):
    def inner():
      print "i'm the inner function"
      ret = some_func()
      return ret + 1
    return inner
  def foo():
    return 1
  
  decorated = outer(foo)
  decorated()
  ..i'm the inner function
  ..2

outer() **returns a function**, inner(), which applies some operation on whatever function is passed to outer(). In this example, outer() is our "decorator" and it is "decorating" the foo() function (we passed it foo, and it's adding 1 to whatever foo returned).

In fact, we can completely replace our foo object with its decorated version by re-assigning it:

  foo = outer(foo)

This works since outer(foo) is ran first, uses the original foo() definition, and then assigns the result to the foo object, overwriting the old foo definition. From now on, any calls to foo() won't get the original foo, they'll get our decorated version.

We can use the myfunc = wrapper(myfunc) syntax as mentioned above, but python provides support to do this simply by using @wrapper above some function:

  @wrapper
  def myfunc:
    return blahblahblah

So what's the point? Well, slapping a memory/cpu/other performance profiling wrapper on some function could let you see how many calls it's making, how much memory it is allocating, and whatever else. There also exists situations where you have a class or function in which you cannot change the source code of, but need to extend its functionality. You may also want to write a wrapper that logs all arguments passed to a certain function, or some wrapper that does some bounds checking/filtering on function output, or any use case where you only want to temporarily apply some decorator to some function, where adding a simple @decorator above a function is much easier than changing the function itself.

Quickies
^^^^^^^^
Both list.sort() and sorted() have a **key** parameter which allows you to specify *a function to be called on each list element*. The results of this will determine how elements in a list are sorted.

  students = [ ('john','A','23'), ('jamal','B','32'), ('jerry','C','42') ]
  sorted(students, key=lambda stu: stu[2])  #sort by age, the 3rd element in each tuple

Stacks
------
Stacks are useful (and one of the original) data structures which are well suited to expression evaluation and variable storage (in particular, holding variables outside of a subroutine). 

FIFO  (first in first out) stacks are useful as they naturally work with the structure of code. The deeper you nest into if/for/whatever, each level has variables. As you nest back up to the top, these variables are popped off in order.

Another reason stacks are useful is if a subroutine is called by multiple threads at the same time, or are recursively called. In this instance, a variable could be set to one value by one thread, and then changed to another value by another thread, thereby invalidating the result. To prevent this, a stack can be allocated in memory which essentially gives the subroutine a working memory it can use. Each call of the subroutine pushes and pulls more stuff onto and off the stack.


Registers
---------
A register is a small bit of information that lives in the register file, which resides in a small bit of memory on the CPU.

Usually the EAX register holds a return value. EBP is the stack pointer, pointing to the beginning of your stack. Then you've got the program counter, which points to the current instruction, EIP. The other registers you just use however you want. 

In assembly, these registers are referred to through names like %eax, %esp, %rdi, %edi, etc.


Classes
-------
You can think of a class as a template, it's a struct basically. It holds variables with default values, functions(/methods, described below). 

Let's say you've got a class defined like this:
  class Door:
    scopeExample1 = 'inside the class'
    def open(self, arrrg):
      print 'hello stranger'
      scopeExample2 = 'inside the method inside the class'
      self.scopeExample3 = 'using self. inside the method inside the class'
      if arrrg:
        print arrrg

You can instantiate a class (create a class object) like this (mfi means my_first_instantiation):
  mfi = Door()

Now you have an object that contains all the properties inside the class. Test some stuff:
  mfi.open() ## hello stranger
  mfi.open('blahhh') ## hello stranger \n blahhh
  scopeExample1  ##NameError. Not defined.
  mfi.scopeExample1  ## 'inside the class'
  mfi.scopeExample2  ##Door instance has no attribute scopeExample2
  mfi.scopeExample3  ##Door instance has no attribute scopeExample3
  mfi.self.scopeExample3


  class Door:
    def open(self):
      print 'hello stranger'
  
  def knock_door:
    a_door = Door()
    Door.open(a_door)
  
  knock_door()
Protocols
=========
This is a general catch-all. Topic-specific protocols (eg: TCP) may exist under other pages.

DNS
---
port:53

DNS Resolution process
^^^^^^^^^^^^^^^^^^^^^^
# Host looks in its cache for www.wiki.org (if it has a dns cache)
# Host asks its configured nameserver(s) where www.wiki.org is
# nameserver checks its cache
# nameserver either asks a configured recursive nameserver, or has recursive lookup itself
# recursive nameserver has "hints" of the known address of root name servers, or is configured to have some
# recursive nameserver asks root nameserver for www.wiki.org, root nameserver passes back auth nameserver for .org
# recursive nameserver asks authoritative nameserver for .org for www.wiki.org, .org nameserver passes back auth nameserver for wiki.org
# recursive nameserver asks authoritative nameserver for wiki.org for www.wiki.org
# authoritative nameserver for wiki.org answers www.wiki.org = 147.0.2.3
# recursive nameserver passes back info to previous host or nameserver

General
^^^^^^^
Every domain has at least one authoritative DNS server (eg: ns1.wiki.org). These authoritative servers publish info about that domain and the name servers of any domains subordinate to it. These servers pass back their answers with the Authoritave Answer (AA) flag set.

These ns[1,2].wiki.org records are configured at the .org nameserver as well as the authoritative wiki.org server. In the example above, the recursive server is querying the .org server for www.wiki.org. In that response, the .org nameserver is telling it to contact ns1.wiki.org. The recursive nameserver doesn't know the IP of ns1.wiki.org and it's already querying about finding wiki.org. In this case, the .org nameserver must pass back a **"glue"** record containing the IP's of ns1&2. This is contained in the "additional" section of the DNS response.

An *authoritative-only* nameserver is configured to only return answers to queries about domain names that have been specifically configured by the admin. So, an authoritative server lets recursive nameservers know address info for the domains it has configured.

Query and Reply Structure
^^^^^^^^^^^^^^^^^^^^^^^^^
Each message consists of a header and four sections: question, answer, authority, and additional. Queries and replies have the same structure. The header section contains the fields: Identification, Flags, Number of questions, Number of answers, Number of authority resource records (RRs), and Number of additional RRs. The flags field has bits that are flipped to say whether it's a reply or a query, whether the response (only on replies) is authoritative, whether a recursive query is requested, and whether a dns server supports recursion (reply only).

A RR (Resource Record) is just a line in a DNS file
  NAME	TYPE(# in numberic form, eg:15 for MX)	CLASS(Almost always "IN" for internet)	TTL	RDLENGTH(length of RDATA field)	RDATA(ie: an ip address)


HTTP
----
TODO: EXPAND

Stateless?
^^^^^^^^^^
Most HTTP conversation does not drop the connection on every request, but rather multiple HTTP requests are pipelined in the same connection. In HTTP 1.0, client can indicate that it wants to send multiple requests in the same connection by using Connection: Keep-Alive header, in HTTP 1.1, keep alive is the default, and client and server only drops connection when explicitly requested and on timeout.

Upon hearing that HTTP is stateless, you might then assume that HTTP uses a new connection for every request. This is incorrect. HTTP is stateless because every request is independent of any other request whether they are transported through the same connection or a different connection. In other words **every request contains all information (even if just a token that represents the full context) that is needed to process that request.**

As a real life analogy, a stateless communication is like radio conversation between pilot and ground controllers, where you're expected to always state your callsign and usually the callsign of the message destination with every spurt of messages. On the other hand, a stateful protocol is like a telephone conversation, where you rely that the person on the other side to remember who you are.

Rabbit Hole
===========

**"Describe exactly what happens when you type 'telnet google.com 80' at a bash prompt in as much detail as possible"** 

The above question (or a variant of it) is an extremely common interview question, with a nearly bottomless answer to it. This page attempts a reasonable answer.

Shell Interpretation
--------------------
- Shell interpreter takes in each item, separated by space, and saves it. In this case nothing fancy is going on, and bash might set ARG1 to google.com and ARG2 to 80
- Shell looks in its $PATH for an executable named telnet

Process Creation
----------------
- Shell calls fork(), passes procname and port arguments to it
- fork() system calls to sys_fork(), sys_fork() calls do_fork(), do_fork() does an alloc_pidmap to get a new PID
- do_fork() then calls copy_process() and passes the flags, stack, and registers used by the parent process, the parent process PID, and the newly allocated PID. do_fork() now waits.
- copy_process() does its thing by creating a duplicate set of stack and task_struct for the new process, and all pointers and whatnot to file descriptors, memory space, etc. used by the parent. Memory is not actually copied, a copy-on-write system is used
- Now, copy_process() calls copy_flags(), with a bit set to run exec() likely being in there
- The new task is then assigned to a processor, and control is passed back to do_fork() with a pointer to the new child
- The process isn't actually running at this point, so do_fork() calls the function wake_up_new_task on it. This places the new process in a run queue and wakes it up for execution
- do_fork() then returns the new PID value to bash
- The child in the meantime has been woken up and has ran exec() on telnet, passing it google.com and port 80, and replaced its memory space with whatever telnet wants. It might also have closed a pipe file descriptor that it inherited from the parent, off-handedly letting the parent know that exec has ran (parent watches the pipe for an EOF)

Telnet
------
- Now Telnet needs to do a DNS lookup.
- Telnet (or perhaps an underlying library that telnet calls to handle connections. Let's assume telnet does the work itself for this example) initializes an ‘addrinfo’ type struct called hints, which you can use to fill in some bits that getaddrinfo()'s resulting struct may not fill out (socktype, protocol etc) 
- Telnet then does a system call, getaddrinfo(), passing it a DNS name, and a protocol(eg:http) or a port number, and your hints struct that you just initialized
- getaddrinfo()'s main job is DNS resolution. It returns a struct with network info that telnet can use for resultant calls to socket() and connect() (more on this after dns resolution)


DNS Resolution
--------------
- Host looks in its cache for www.google.com (if it has a dns cache)
- Host looks in /etc/hosts (normally this order. Depends on your /etc/nsswitch.conf)
- Host asks its configured nameserver(s) where www.google.com is (AFAIK it's getaddrinfo() that does a connect() in the background to connect to this configured nameserver to query)

Nameserver
^^^^^^^^^^
- nameserver checks its cache
- (if nameserver not recursive) nameserver sends request to configured recursive nameserver 
- recursive nameserver has "hints" of the known address of root (top level domain) name servers, or explicit config entries for some
- recursive nameserver asks root nameserver for www.google.com, root nameserver passes back authoritative nameserver for .com
- recursive nameserver asks authoritative nameserver for .com for www.google.com, .com nameserver passes back auth nameserver for google.com (perhaps ns[1,2].google.com , along with a "glue" record containing IP's for ns#.google.com)
- recursive nameserver asks authoritative nameserver for google.com for www.google.com
- authoritative nameserver for google.com answers www.google.com = 147.0.2.3
- recursive nameserver passes back info to previous host or nameserver
- IP information passed back to getaddrinfo()

Socket Stuff
------------
- getaddrinfo() finished running, and passed back a struct with stuff like the resolved IP address, socktype, protocol, etc.
- Telnet now calls the socket() system call in order to get a socket file descriptor. Telnet passes socket() domaintype(ipv4 or 6), socket type (eg: sock_stream), and protocol (tcp, udp etc). You get a file descriptor back. sock_stream is the right choice in this case, as telnet uses TCP because telnet is designed to be a character type device.
- Telnet then calls connect(sockfd, dest.ip). The kernel will choose a local port and bind() for us

Destination Server
------------------
- Destination server has done the socket(); bind(); listen() dance when it started up, and is waiting for traffic.
- listen(socketfd, backlog) has a backlog...this is the number of connections allowed on the incoming queue. Connections will wait until the server accept()'s them.
- Destination httpd server hits accept(). Upon accepting a connection, a struct (type: sockaddr) is created with connection info (port, srchost, etc), a new socket is created, and a NEW file descriptor # is passed back to httpd. The original socket goes back to listening, while the new one is used for the new connection

TCP Handshake
-------------
1. SYN: Client sends a SYN packet to the server, which has its SEQ number set to random value A
2. SYN-ACK: In response, the server replies with a SYN-ACK. The acknowledgment number is set to one more than the received sequence number i.e. A+1, and the sequence number that the server chooses for the response packet is another random number, B.
3. ACK: Finally, the client sends an ACK back to the server. The sequence number is set to the received acknowledgement value i.e. A+1, and the acknowledgement number is set to one more than the received sequence number i.e. B+1

-Connection established. Now Telnet can do a send() and recv()

    char ASTERISKmsg = "Beej was here!";
    int len, bytes_sent;
    len = strlen(msg);
    bytes_sent = send(sockfd, msg, len, 0);



Further Work
------------
- Describe the path that a packet takes through the kernel and out the wire. How does the kernel know which device to use? How does that mapping work?
- Expand on accept() in destination server section
Systems Administration
======================

Memory
------

  free -m

See the -/+ buffers/cache value under the free column? That's how much RAM you have available. The value under "used" is how much is being currently used by the kernel for various things - mostly filesystem caching.

  top ; ps auxww
  cat /proc/<pid>/maps ; cat /proc/<pid>/smaps
  cat /proc/meminfo

top and ps have VIRT/VSZ for virtual mem, RSS/RES for resident, and (top) SHR for shared.

- **VIRT** stands for the virtual size of a process, which is the sum of memory it is actually using, memory it has mapped into itself (for instance the video cards’s RAM for the X server), files on disk that have been mapped into it (most notably shared libraries), and memory shared with other processes. VIRT represents how much memory the program is able to access at the present moment.
- **RES** stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds directly to the %MEM column.)
- **SHR** indicates how much of the VIRT size is actually sharable (memory or libraries). In the case of libraries, it does not necessarily mean that the entire library is resident. For example, if a program only uses a few functions in a library, the whole library is mapped and will be counted in VIRT and SHR, but *only the parts of the library file containing the functions being used will actually be loaded in and be counted under RES.*
- cat /proc/<pid>/maps and smaps as stated above for detailed per-process memory usage

Diagnostics
-----------
This records some lesser known or less fully understood utilities which should be used more often when diagnosing problems with a server.





Quickies
--------

Print the last column in each line of output:

  cat something | awk '{print $NF}'


Relational Databases
====================
.. _rdbms:

# TODO: In addition to the ACID section below, review BASE(basically available, soft state, eventually consistent). BASE is typically used where ACID does not scale

Indexes
-------

Clustered Index
^^^^^^^^^^^^^^^
- This is a sorted index. An example would be a phone book. A clustered index is an index which *physically* orders the data (the actual bits on disk) in a certain way, and when new data arrives it is saved in the same order.
- A caveat with clustered indexes is that *only one can be created on a given database table*. This is because clustered indexes enforce data order - you can't enforce data order on two rows of the same table as the orders of each row would inevitably conflict.
- Additionally, clustered indexes increase write time because when new data arrives, all of the data has to be rearranged. On the bright side though, they can greatly increase the read performance from the table.

Non-Clustered Index
^^^^^^^^^^^^^^^^^^^
- These are the type that we use the most. These indexes *keep a separate ordered list* that has *pointers to the physical rows*.
- Unlike the clustered index, a table can have many non-clustered indexes. But the caveat with these indexes is that each new index will increase the time it takes to write new records.
- To summarize, non-clustered indexes *do not order* the data physically, they just keep a list of the data order.






~~Database Shit~~
-Research shows that the four common sources of overhead in database management systems are: logging (19%), latching (19%), locking (17%), B-tree and buffer management operations (35%)
((Locks protect data during transactions. Another process, latching, controls access to physical pages. Latches are very lightweight, short-term synchronization objects protecting actions that do not need to be locked for the life of a transaction.))


::ACID::
ACID (Atomicity, Consistency, Isolation, Durability) is a set of properties that guarantee that database transactions are processed reliably. 
-Atomicity requires that each transaction is "all or nothing": if one part of the transaction fails, the entire transaction fails, and the database state is left unchanged
-The consistency property ensures that any transaction will bring the database from one valid state to another.
-The isolation property ensures that the concurrent execution of transactions results in a system state that would be obtained if transactions were executed serially, i.e. one after the other. Providing isolation is the main goal of concurrency control. Depending on concurrency control method, the effects of an incomplete transaction might not even be visible to another transaction
-Durability means that once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors. Transactions/other shit are usually kept in some sort of non volatile memory (eg: mysql-log on disk)


::MapReduce::
So, MapReduce does what it says in its name. First, it Maps data. What this means is that the master node takes input, divides it into smaller sub problems, and then distributes them to worker nodes. The node may reduce the information in those files to key/value pairs, or just into common queues (such as sorting students by first name into queues, one queue for each name) (filtering and sorting). It may also remove duplicates or only keep the highest value for keys that are the same, or divide the problem into smaller sub problems, or whatever else. Many Map jobs are run at the same time on different files. After the Map jobs are done, the results are all passed to a Reduce job. This reduce job combines all the results together and performs some sort of user-inputted operation/summary on the information (such as counting the number of students in each queue, yielding name frequencies, or combining all unique keys and displaying the highest value)

This is faster than just having a single task go through all the data serially, and also requires a ton less memory and compute resources assigned to just a single process. In essence, it is a method to allow huge data sets to be processed in a distributed fashion.

::NoSQL::

-Good for people who require a distributed system that can span datacenters while handling failure scenarios, who are not worried about the extreme consistency rules a relational DB may implement. NoSQL systems, because they have focussed on scale, tend to exploit partitions, tend not use heavy strict consistency protocols, and so are well positioned to operate in distributed scenarios.

-Write performance :: At 80 MB/s it takes a day to store 7TB so writes need to be distributed over a cluster, which implies key-value access, MapReduce, replication, fault tolerance, consistency issues, and all the rest. For faster writes in-memory systems can be used

-Fast key/value store access :: Why is key/value store fast? Pass a key to a hashing algorithm, and you get the same "location" output every time of where the value is stored. You end up not having to search for the value. Basically an index per data item, rather than choosing an index per column as you would in an RDBMS

-flexible schema/datatypes(eg: JSON), no SPOF

-Programmer ease of use :: Programmers want to work with keys, values, JSON, Javascript stored procedures, HTTP, and so on. End users typically want to work on data using SQL, but this preference should not permeate throughout all datastore decisions.

-Availability vs Consistency vs Failure Handling :: Relational databases pick strong consistency which means they can't tolerate a partition failure. In the end this is a business decision and should be decided on a case by case basis. Does your app even care about consistency? Are a few drops OK? Does your app need strong or weak consistency? Is availability more important or is consistency? Will being down be more costly than being wrong?


Architecture
============

TODO:
- Note summary kleppmann's designing data intensive applications book
- Put "how would you design x" thoughts here where x is some big thing like facebook

Broad/Architectural
Concerns with distributed architectures - moving to stateful microservices, or microservices that talk to other microservices in order to do their tasks. Start worrying about network latency, fault tolerance, message serialization, unreliable networks, asynchronicity, versioning, varying loads within the application tiers etc. etc. Moving to a cloud service shares some of these problems too.

http://blog.xebialabs.com/2014/12/31/8-questions-need-ask-microservices-containers-docker-2015/

Message queue's are useful because:
-They decouple a process, act as an intermediary. The advantages of this include scenarios such as traffic spikes, or when one side of the queue is offline for some reason. During a traffic spike, perhaps one process will get totally overwhelmed. This process can be decoupled so as not to take the entire system down. The MQ still exists, and allows the other side to pull off messages at its leisure without getting overwhelmed as well.
-A message queue can provide built-in message delivery guarantees, and also ordering guarantees. This saves you from having to build the logic into your application.
-A message queue acts as a buffer between two sides that may be operating at different speeds. This helps normalize speed. Additionally, analyzing the MQ's stats will give you an idea if one side is performing poorly.
-Finally, MQ's allow for asynchronous communication. This means that one side can push on a bunch of data and forget about it, the other side doesn't have to process it right away either.
